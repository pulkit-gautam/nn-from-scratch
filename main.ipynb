{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torchvision import transforms\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = torchvision.datasets.FashionMNIST(\"./data\", download=True, transform=transforms.Compose([transforms.ToTensor()]))\n",
    "test_set = torchvision.datasets.FashionMNIST(\"./data\", download=True, train=False, transform=transforms.Compose([transforms.ToTensor()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_set.data\n",
    "y_test = test_set.targets\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_set.data, train_set.targets, test_size=0.1)\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], 784)\n",
    "X_test = X_test.reshape(X_test.shape[0], 784)\n",
    "X_val = X_val.reshape(X_val.shape[0], 784)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "X_val = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_images = 10\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 6))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(num_images):\n",
    "    img = X_train[i].reshape(28, 28)\n",
    "    label = y_train[i]\n",
    "    axes[i].imshow(img, cmap='gray')\n",
    "    axes[i].axis('off')\n",
    "    axes[i].set_title(f\"Label: {label}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = X_train.shape[0]\n",
    "num_test = X_test.shape[0]\n",
    "num_val = X_val.shape[0]\n",
    "num_features = X_train.shape[1]\n",
    "num_classes = len(np.unique(y_train))\n",
    "\n",
    "\n",
    "def one_hot_encode(y, num):\n",
    "    y_one_hot = np.zeros((10, num))\n",
    "    y_one_hot[y, np.arange(num)] = 1\n",
    "    return y_one_hot\n",
    "\n",
    "y_train_one_hot = one_hot_encode(y_train, num_train)\n",
    "y_test_one_hot = one_hot_encode(y_test, num_test)\n",
    "y_val_one_hot = one_hot_encode(y_val, num_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54000, 784)\n",
      "(6000, 784)\n",
      "(10000, 784)\n",
      "torch.Size([54000])\n",
      "torch.Size([6000])\n",
      "torch.Size([10000])\n",
      "(10, 54000)\n",
      "(10, 6000)\n",
      "(10, 10000)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_val.shape)\n",
    "print(y_test.shape)\n",
    "print(y_train_one_hot.shape)\n",
    "print(y_val_one_hot.shape)\n",
    "print(y_test_one_hot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.T\n",
    "X_test = X_test.T\n",
    "X_val = X_val.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_network(num_hidden, sizes, inputsize=784, outputsize=10, initialization='random'):\n",
    "    sizes = [inputsize] + sizes\n",
    "    sizes = sizes + [outputsize]\n",
    "    \n",
    "    parameters = {}\n",
    "    momentum = {}\n",
    "    \n",
    "    if initialization == 'xavier':\n",
    "        for i in range(1, num_hidden + 2):\n",
    "            parameters[\"w\" + str(i)] = np.random.randn(sizes[i], sizes[i-1]) * np.sqrt(2 / (sizes[i] + sizes[i - 1])) * 0.01\n",
    "            parameters[\"b\" + str(i)] = np.zeros((sizes[i], 1))\n",
    "            momentum[\"w\" + str(i)] = np.zeros((sizes[i], sizes[i-1]))\n",
    "            momentum[\"b\" + str(i)] = np.zeros((sizes[i], 1))\n",
    "    else:\n",
    "        for i in range(1, num_hidden + 2):\n",
    "            parameters[\"w\" + str(i)] = np.random.randn(sizes[i], sizes[i-1]) * 0.01\n",
    "            parameters[\"b\" + str(i)] = np.zeros((sizes[i], 1))\n",
    "            momentum[\"w\" + str(i)] = np.zeros((sizes[i], sizes[i-1]))\n",
    "            momentum[\"b\" + str(i)] = np.zeros((sizes[i], 1))\n",
    "    \n",
    "    return parameters, momentum\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1/(1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_derivative(z):\n",
    "    return sigmoid(z) * (1 - sigmoid(z))\n",
    "\n",
    "def tanh(z):\n",
    "    return np.tanh(z)\n",
    "\n",
    "def tanh_derivative(z):\n",
    "    return (1 - np.tanh(z)**2)\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def relu_derivative(z):\n",
    "    return 1 * (z > 0)\n",
    "    \n",
    "def softmax(z):\n",
    "    z = z - np.max(z)\n",
    "    return np.exp(z)/ np.sum(np.exp(z), axis=0)\n",
    "\n",
    "def softmax_derivative(z):\n",
    "    return softmax(z) * (1 - softmax(z))\n",
    "\n",
    "def cross_entropy_loss(y, y_pred, batch_size, weight_decay, parameters):\n",
    "    \n",
    "    y_pred_clipped = np.clip(y_pred, 1e-7,1 - 1e-7)\n",
    "    \n",
    "    loss = (-1.0 * np.sum(np.multiply(y, np.log(y_pred_clipped + np.exp(-8)))))/batch_size\n",
    "    \n",
    "    regularization_loss = 0\n",
    "    for i in range(1, len(parameters)//2 + 1):\n",
    "        regularization_loss += np.sum(parameters[\"w\" + str(i)]**2)\n",
    "    \n",
    "    loss += (weight_decay/(2 * batch_size)) * regularization_loss \n",
    "    \n",
    "    return loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters, activation):\n",
    "    num_layers = len(parameters)//2\n",
    "    layer_activations = {\n",
    "        \"sigmoid\": sigmoid,\n",
    "        \"relu\": relu,\n",
    "        \"tanh\": tanh\n",
    "    }\n",
    "    if activation not in layer_activations:\n",
    "        raise ValueError(\"Invalid Activation Function\")\n",
    "    \n",
    "    A = [None for _ in range(num_layers + 1)]\n",
    "    Z = [None for _ in range(num_layers + 1)]\n",
    "    \n",
    "    A[0] = X\n",
    "    for layer in range(1, num_layers + 1):\n",
    "        Z[layer] = np.dot(parameters[\"w\" + str(layer)], A[layer-1]) + parameters[\"b\" + str(layer)]\n",
    "        A[layer] = layer_activations[activation](Z[layer])\n",
    "    output = A[num_layers]\n",
    "    \n",
    "    return output, A, Z\n",
    "\n",
    "def back_propagation(y, A, Z, parameters, activation, batch_size, weight_decay, clip_threshold=1e7):\n",
    "    \n",
    "    gradients = {}\n",
    "    num_layers = len(parameters)//2\n",
    "    \n",
    "    gradients[\"dZ\" + str(num_layers)] = A[num_layers] - y\n",
    "    for layer in range(num_layers, 0, -1):\n",
    "        dW = np.dot(gradients[\"dZ\" + str(layer)], A[layer - 1].T)\n",
    "        dW += weight_decay * parameters[\"w\" + str(layer)] / batch_size\n",
    "        gradients[\"dW\" + str(layer)] = dW\n",
    "        gradients[\"db\" + str(layer)] = np.sum(gradients[\"dZ\" + str(layer)], axis=1, keepdims=True) / batch_size\n",
    "        \n",
    "        if layer > 1:\n",
    "            dZ = np.matmul(parameters[\"w\" + str(layer)].T, gradients[\"dZ\" + str(layer)])\n",
    "            if activation == 'relu':\n",
    "                dZ *= relu_derivative(Z[layer - 1])\n",
    "            elif activation == 'sigmoid':\n",
    "                dZ *= sigmoid_derivative(Z[layer - 1])\n",
    "            elif activation == 'tanh':\n",
    "                dZ *= tanh_derivative(Z[layer - 1])\n",
    "            \n",
    "            dZ = np.clip(dZ, -clip_threshold, clip_threshold)\n",
    "            \n",
    "            gradients[\"dZ\" + str(layer - 1)] = dZ\n",
    "    \n",
    "    return gradients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD(parameters, grads, learning_rate):\n",
    "    num_layers = len(parameters) // 2\n",
    "    \n",
    "    for layer in range(1, num_layers + 1):\n",
    "        parameters[\"w\" + str(layer)] -= learning_rate * grads[\"dW\" + str(layer)]\n",
    "        parameters[\"b\" + str(layer)] -= learning_rate * grads[\"db\" + str(layer)]\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "def momentum(parameters, momentums, grads, beta, learning_rate):\n",
    "    num_layers = len(parameters) // 2\n",
    "    \n",
    "    for layer in range(1, num_layers + 1):\n",
    "        momentums[\"w\" + str(layer)] = beta * momentums[\"w\" + str(layer)] + learning_rate * grads[\"dW\" + str(layer)]\n",
    "        parameters[\"w\" + str(layer)] -= momentums[\"w\" + str(layer)]\n",
    "        \n",
    "        momentums[\"b\" + str(layer)] = beta * momentums[\"b\" + str(layer)] + learning_rate * grads[\"db\" + str(layer)]\n",
    "        parameters[\"b\" + str(layer)] -= momentums[\"b\" + str(layer)]\n",
    "    \n",
    "    return parameters, momentums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X_test, parameters, activation):\n",
    "    y_pred, _, _ = forward_propagation(X_test, parameters, activation=activation)\n",
    "    predictions = np.argmax(y_pred, axis= 0)\n",
    "    return predictions\n",
    "\n",
    "def accuracy(X_train, y_train, X_test, y_test, parameters, activation):\n",
    "    train_pred = predict(X_train, parameters, activation)\n",
    "    test_pred = predict(X_test, parameters, activation)\n",
    "    \n",
    "    train_accuracy = accuracy_score(y_train, train_pred) * 100\n",
    "    test_accuracy = accuracy_score(y_test, test_pred) * 100\n",
    "    return train_accuracy, test_accuracy\n",
    "\n",
    "def fit(X_train, y_train_one_hot, learning_rate=0.001, activation=\"relu\", initialization=\"xavier\", optimizer=\"momentum\", batch_size=64, epochs=40,\n",
    "           weight_decay=0.0005, num_neurons=128, num_hidden = 3, beta = 0.9):\n",
    "    \n",
    "    sizes = [num_neurons] * num_hidden \n",
    "    parameters, momentums = neural_network(num_hidden, sizes, num_features, num_classes, initialization)\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        for i in range(0, X_train.shape[1], batch_size):\n",
    "            batch_count = min(batch_size, X_train.shape[1] - i + 1)\n",
    "            \n",
    "            if optimizer == \"nestrov\":\n",
    "                parameters_lookahead = parameters.copy()\n",
    "                num_layers = len(parameters) // 2\n",
    "                \n",
    "                for layer in range(1, num_layers + 1):\n",
    "                    parameters_lookahead[\"w\" + str(layer)] = parameters[\"w\" + str(layer)] - beta * momentums[\"w\" + str(layer)]\n",
    "                    parameters_lookahead[\"b\" + str(layer)] = parameters[\"b\" + str(layer)] - beta * momentums[\"b\" + str(layer)]\n",
    "                \n",
    "                _, A, Z = forward_propagation(X_train[:, i: i + batch_count], parameters_lookahead, activation)\n",
    "                gradients_lookahead = back_propagation(y_train_one_hot[:, i : i + batch_count], A, Z, parameters_lookahead, activation, batch_count, weight_decay)\n",
    "                \n",
    "                parameters, momentums = momentum(parameters, momentums, gradients_lookahead, beta, learning_rate)\n",
    "            else:\n",
    "                _, A, Z = forward_propagation(X_train[:, i: i + batch_count], parameters, activation)\n",
    "                gradients = back_propagation(y_train_one_hot[:, i : i + batch_count], A, Z, parameters, activation, batch_count, weight_decay)\n",
    "                \n",
    "                if optimizer == \"sgd\":\n",
    "                    parameters = SGD(parameters, gradients, learning_rate)\n",
    "                elif optimizer == \"momentum\":\n",
    "                    parameters, momentums = momentum(parameters, momentums, gradients, beta, learning_rate)\n",
    "        \n",
    "        y_pred, _, _ = forward_propagation(X_train, parameters, activation)\n",
    "        cost = cross_entropy_loss(y_train_one_hot, y_pred, num_train, weight_decay, parameters)\n",
    "        \n",
    "        if not epoch % 2:\n",
    "            print(\"Epoch number: \", epoch, \"\\tLoss:\", cost)\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number:  2 \tLoss: 2.3723057347540633\n",
      "Epoch number:  4 \tLoss: 2.2995612126809033\n",
      "Epoch number:  6 \tLoss: 2.299378559047712\n",
      "Epoch number:  8 \tLoss: 2.2942970598317216\n",
      "Epoch number:  10 \tLoss: 0.6716336657342534\n",
      "Epoch number:  12 \tLoss: 0.4080396296572764\n",
      "Epoch number:  14 \tLoss: 0.3610182925637946\n",
      "Epoch number:  16 \tLoss: 0.3438511513025415\n",
      "Epoch number:  18 \tLoss: 0.30386588631723116\n",
      "Epoch number:  20 \tLoss: 0.3097801183591696\n",
      "Epoch number:  22 \tLoss: 0.29735915965850096\n",
      "Epoch number:  24 \tLoss: 0.29006304330737975\n",
      "Epoch number:  26 \tLoss: 0.2616138083704006\n",
      "Epoch number:  28 \tLoss: 0.26801825069422147\n",
      "Epoch number:  30 \tLoss: 0.2757885940837521\n",
      "Epoch number:  32 \tLoss: 0.279034289972609\n",
      "Epoch number:  34 \tLoss: 0.3129625132861698\n",
      "Epoch number:  36 \tLoss: 0.2227070912533561\n",
      "Epoch number:  38 \tLoss: 0.29746737732542516\n",
      "Epoch number:  40 \tLoss: 0.27054866230454955\n",
      "Epoch number:  42 \tLoss: 0.214502944350299\n",
      "Epoch number:  44 \tLoss: 0.22756596167855486\n",
      "Epoch number:  46 \tLoss: 0.18135561599721173\n",
      "Epoch number:  48 \tLoss: 0.2046448211684929\n",
      "Epoch number:  50 \tLoss: 0.21110183078564204\n"
     ]
    }
   ],
   "source": [
    "learned_parameters = fit(X_train, y_train_one_hot, \n",
    "                         learning_rate=0.001,\n",
    "                         activation=\"relu\",\n",
    "                         initialization=\"xavier\",\n",
    "                         optimizer=\"nestrov\", \n",
    "                         batch_size=512,\n",
    "                         epochs=50,\n",
    "                         weight_decay=0.0005,\n",
    "                         num_neurons=128,\n",
    "                         num_hidden=3,\n",
    "                         beta=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 96.08888888888889 %\n",
      "Test accuracy: 88.03 %\n"
     ]
    }
   ],
   "source": [
    "train_accuracy, test_accuracy = accuracy(X_train, y_train, X_test, y_test, learned_parameters, \"relu\")\n",
    "print(f\"Training accuracy: {train_accuracy} %\")\n",
    "print(f\"Test accuracy: {test_accuracy} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300060017079182\n",
      "Epoch number:  4 \tLoss: 1.914966580881515\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3004538314554006\n",
      "Epoch number:  4 \tLoss: 2.2993415475028876\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.769081255604945\n",
      "Epoch number:  4 \tLoss: 0.794739551735999\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3004418092542998\n",
      "Epoch number:  4 \tLoss: 2.299326895916996\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.00002572745656\n",
      "Epoch number:  4 \tLoss: 0.5343331656175989\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3002575127770823\n",
      "Epoch number:  4 \tLoss: 2.2993173537169618\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.298069972259541\n",
      "Epoch number:  4 \tLoss: 1.578542045933245\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3004701204832436\n",
      "Epoch number:  4 \tLoss: 2.299326179144427\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.6068597633267816\n",
      "Epoch number:  4 \tLoss: 0.751049656158086\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3003870887449325\n",
      "Epoch number:  4 \tLoss: 2.2993384332522866\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.8881366679314788\n",
      "Epoch number:  4 \tLoss: 0.520778535232467\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300257408612554\n",
      "Epoch number:  4 \tLoss: 2.299319168342294\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3003389157558303\n",
      "Epoch number:  4 \tLoss: 2.298202296152235\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3005453901341655\n",
      "Epoch number:  4 \tLoss: 2.2994216961533627\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.9351991992544417\n",
      "Epoch number:  4 \tLoss: 1.0567886479149258\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3006273779378845\n",
      "Epoch number:  4 \tLoss: 2.299532149196146\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.0578624723559025\n",
      "Epoch number:  4 \tLoss: 0.5894388352051683\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3007436329782665\n",
      "Epoch number:  4 \tLoss: 2.2996922136075733\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2939821299248777\n",
      "Epoch number:  4 \tLoss: 1.3319935944943195\n",
      "Epoch number:  6 \tLoss: 0.7190723246283888\n",
      "Epoch number:  8 \tLoss: 0.570658773450622\n",
      "Epoch number:  10 \tLoss: 0.48396003815944655\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3004531390258354\n",
      "Epoch number:  4 \tLoss: 2.2993278463534037\n",
      "Epoch number:  6 \tLoss: 2.2993251666181975\n",
      "Epoch number:  8 \tLoss: 2.2993260349303477\n",
      "Epoch number:  10 \tLoss: 2.299326631689698\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.663725379538468\n",
      "Epoch number:  4 \tLoss: 0.835866961604968\n",
      "Epoch number:  6 \tLoss: 0.547015427729771\n",
      "Epoch number:  8 \tLoss: 0.4404124747580228\n",
      "Epoch number:  10 \tLoss: 0.39292396026247156\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3003741149752064\n",
      "Epoch number:  4 \tLoss: 2.299320686440711\n",
      "Epoch number:  6 \tLoss: 2.2993197093354447\n",
      "Epoch number:  8 \tLoss: 2.299319218318117\n",
      "Epoch number:  10 \tLoss: 2.29931894920331\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.9535945479448279\n",
      "Epoch number:  4 \tLoss: 0.534995553629528\n",
      "Epoch number:  6 \tLoss: 0.4342744376808556\n",
      "Epoch number:  8 \tLoss: 0.38055746716865085\n",
      "Epoch number:  10 \tLoss: 0.3420782890859373\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300357544219992\n",
      "Epoch number:  4 \tLoss: 2.2993557277260583\n",
      "Epoch number:  6 \tLoss: 2.2993527396745153\n",
      "Epoch number:  8 \tLoss: 2.2993498584720613\n",
      "Epoch number:  10 \tLoss: 2.2993464590499078\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3000114393692535\n",
      "Epoch number:  4 \tLoss: 1.7918164052585703\n",
      "Epoch number:  6 \tLoss: 0.9271210628986785\n",
      "Epoch number:  8 \tLoss: 0.6059222238893444\n",
      "Epoch number:  10 \tLoss: 0.4985159559055016\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3004453883287126\n",
      "Epoch number:  4 \tLoss: 2.299328123428126\n",
      "Epoch number:  6 \tLoss: 2.299326701031952\n",
      "Epoch number:  8 \tLoss: 2.299326820962582\n",
      "Epoch number:  10 \tLoss: 2.299326749493433\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.8050049748734722\n",
      "Epoch number:  4 \tLoss: 0.6860574447106365\n",
      "Epoch number:  6 \tLoss: 0.5069501095027884\n",
      "Epoch number:  8 \tLoss: 0.4262032897733082\n",
      "Epoch number:  10 \tLoss: 0.38506481271098236\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300351117567972\n",
      "Epoch number:  4 \tLoss: 2.299346623935718\n",
      "Epoch number:  6 \tLoss: 2.299345397045908\n",
      "Epoch number:  8 \tLoss: 2.299346003601888\n",
      "Epoch number:  10 \tLoss: 2.29934444277047\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.915958382461333\n",
      "Epoch number:  4 \tLoss: 0.5022907916289547\n",
      "Epoch number:  6 \tLoss: 0.4215520386261785\n",
      "Epoch number:  8 \tLoss: 0.37392426917128896\n",
      "Epoch number:  10 \tLoss: 0.33998735981356976\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300301461728755\n",
      "Epoch number:  4 \tLoss: 2.2993155758764456\n",
      "Epoch number:  6 \tLoss: 2.299311638559433\n",
      "Epoch number:  8 \tLoss: 2.2993090933888043\n",
      "Epoch number:  10 \tLoss: 2.299306134222942\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299831758673586\n",
      "Epoch number:  4 \tLoss: 1.6699639769713086\n",
      "Epoch number:  6 \tLoss: 1.3793511222567938\n",
      "Epoch number:  8 \tLoss: 0.7500299411254804\n",
      "Epoch number:  10 \tLoss: 0.5681556852052062\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3005175400706053\n",
      "Epoch number:  4 \tLoss: 2.2993711287953564\n",
      "Epoch number:  6 \tLoss: 2.2993607410466645\n",
      "Epoch number:  8 \tLoss: 2.299353559099739\n",
      "Epoch number:  10 \tLoss: 2.2993478707472272\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.8689369667930542\n",
      "Epoch number:  4 \tLoss: 0.7947238208904844\n",
      "Epoch number:  6 \tLoss: 0.6168223690878688\n",
      "Epoch number:  8 \tLoss: 0.5477179178606356\n",
      "Epoch number:  10 \tLoss: 0.45826593415958883\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3005903755855193\n",
      "Epoch number:  4 \tLoss: 2.2994826011549043\n",
      "Epoch number:  6 \tLoss: 2.2994501418946856\n",
      "Epoch number:  8 \tLoss: 2.2994252890486213\n",
      "Epoch number:  10 \tLoss: 2.2994055399453\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.9256509496909945\n",
      "Epoch number:  4 \tLoss: 0.5570000209637316\n",
      "Epoch number:  6 \tLoss: 0.46085970015043687\n",
      "Epoch number:  8 \tLoss: 0.41352987374659317\n",
      "Epoch number:  10 \tLoss: 0.383079232128062\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3006669502162804\n",
      "Epoch number:  4 \tLoss: 2.2995929709392486\n",
      "Epoch number:  6 \tLoss: 2.299540847192893\n",
      "Epoch number:  8 \tLoss: 2.2994987103748623\n",
      "Epoch number:  10 \tLoss: 2.299464754775013\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.33182640309508\n",
      "Epoch number:  4 \tLoss: 1.624967366502175\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.332816011650762\n",
      "Epoch number:  4 \tLoss: 2.3003235803979285\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.969493873269835\n",
      "Epoch number:  4 \tLoss: 0.7642264644681666\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.331828751507174\n",
      "Epoch number:  4 \tLoss: 2.3002330353399274\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.7332033780039637\n",
      "Epoch number:  4 \tLoss: 0.5032467335917645\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3275510672165693\n",
      "Epoch number:  4 \tLoss: 2.299918475287618\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3299221618690864\n",
      "Epoch number:  4 \tLoss: 1.7692528982424673\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3329985253083\n",
      "Epoch number:  4 \tLoss: 2.3003397422517367\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.7389871374787167\n",
      "Epoch number:  4 \tLoss: 1.0472526704726306\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.331068372090954\n",
      "Epoch number:  4 \tLoss: 2.300189754337911\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.9421904938393387\n",
      "Epoch number:  4 \tLoss: 0.5268220788629098\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.327584726262943\n",
      "Epoch number:  4 \tLoss: 2.2999192737227148\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.331925756250293\n",
      "Epoch number:  4 \tLoss: 1.720455018186166\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3306916806234783\n",
      "Epoch number:  4 \tLoss: 2.300361454045082\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.7261837935596593\n",
      "Epoch number:  4 \tLoss: 0.7252985187914149\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3307193838483338\n",
      "Epoch number:  4 \tLoss: 2.3004011320317894\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.9333070094730639\n",
      "Epoch number:  4 \tLoss: 0.528440520482242\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3294146166144665\n",
      "Epoch number:  4 \tLoss: 2.3004064207127195\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3319035782607975\n",
      "Epoch number:  4 \tLoss: 1.6849380036934078\n",
      "Epoch number:  6 \tLoss: 0.8138640664459372\n",
      "Epoch number:  8 \tLoss: 0.5643806952512787\n",
      "Epoch number:  10 \tLoss: 0.4700804333360986\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3320051621117446\n",
      "Epoch number:  4 \tLoss: 2.300234138890047\n",
      "Epoch number:  6 \tLoss: 2.2992738579373757\n",
      "Epoch number:  8 \tLoss: 2.299244385120488\n",
      "Epoch number:  10 \tLoss: 2.299243418489584\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.671675403503316\n",
      "Epoch number:  4 \tLoss: 0.7662110639716162\n",
      "Epoch number:  6 \tLoss: 0.5028051601677933\n",
      "Epoch number:  8 \tLoss: 0.4324601536252887\n",
      "Epoch number:  10 \tLoss: 0.3850070941976247\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3313420016433595\n",
      "Epoch number:  4 \tLoss: 2.300201860983067\n",
      "Epoch number:  6 \tLoss: 2.299294408789167\n",
      "Epoch number:  8 \tLoss: 2.299267836522977\n",
      "Epoch number:  10 \tLoss: 2.299266878357354\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.8240301981098572\n",
      "Epoch number:  4 \tLoss: 0.5110376767434301\n",
      "Epoch number:  6 \tLoss: 0.42063286067267086\n",
      "Epoch number:  8 \tLoss: 0.37550953270374354\n",
      "Epoch number:  10 \tLoss: 0.34407934081706604\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3286413985585868\n",
      "Epoch number:  4 \tLoss: 2.2999997095424107\n",
      "Epoch number:  6 \tLoss: 2.29928745409206\n",
      "Epoch number:  8 \tLoss: 2.2992681397786225\n",
      "Epoch number:  10 \tLoss: 2.299267067452144\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.332394886980747\n",
      "Epoch number:  4 \tLoss: 1.645516400237213\n",
      "Epoch number:  6 \tLoss: 1.2983984675569396\n",
      "Epoch number:  8 \tLoss: 0.6314679370765292\n",
      "Epoch number:  10 \tLoss: 0.5062619697923049\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.332711353157446\n",
      "Epoch number:  4 \tLoss: 2.3003208349287645\n",
      "Epoch number:  6 \tLoss: 2.299302766328654\n",
      "Epoch number:  8 \tLoss: 2.2992702965160627\n",
      "Epoch number:  10 \tLoss: 2.299269348917975\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.4550460798760114\n",
      "Epoch number:  4 \tLoss: 0.6425467745105893\n",
      "Epoch number:  6 \tLoss: 0.49960968093363595\n",
      "Epoch number:  8 \tLoss: 0.4216675132978048\n",
      "Epoch number:  10 \tLoss: 0.38244010873774203\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.332182163249817\n",
      "Epoch number:  4 \tLoss: 2.300262492202159\n",
      "Epoch number:  6 \tLoss: 2.299287081415385\n",
      "Epoch number:  8 \tLoss: 2.299256714286319\n",
      "Epoch number:  10 \tLoss: 2.29925579107491\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.9698305834187012\n",
      "Epoch number:  4 \tLoss: 0.5234789321821928\n",
      "Epoch number:  6 \tLoss: 0.42311080763387054\n",
      "Epoch number:  8 \tLoss: 0.37118681000709863\n",
      "Epoch number:  10 \tLoss: 0.33763359029894763\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3294330948332562\n",
      "Epoch number:  4 \tLoss: 2.3000763233716905\n",
      "Epoch number:  6 \tLoss: 2.2993098169414288\n",
      "Epoch number:  8 \tLoss: 2.299288200935379\n",
      "Epoch number:  10 \tLoss: 2.2992862830442813\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3316908226461055\n",
      "Epoch number:  4 \tLoss: 1.8385167569462555\n",
      "Epoch number:  6 \tLoss: 1.1462442111207602\n",
      "Epoch number:  8 \tLoss: 0.7125416986139779\n",
      "Epoch number:  10 \tLoss: 0.5493976676890674\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3324203685848715\n",
      "Epoch number:  4 \tLoss: 2.300387467618417\n",
      "Epoch number:  6 \tLoss: 2.299393485690362\n",
      "Epoch number:  8 \tLoss: 2.299357158851403\n",
      "Epoch number:  10 \tLoss: 2.2993511569843204\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.9326090683030706\n",
      "Epoch number:  4 \tLoss: 1.0475903732094418\n",
      "Epoch number:  6 \tLoss: 0.6030123404590753\n",
      "Epoch number:  8 \tLoss: 0.4747211297452497\n",
      "Epoch number:  10 \tLoss: 0.40751522383752503\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.331301757356436\n",
      "Epoch number:  4 \tLoss: 2.300393983682449\n",
      "Epoch number:  6 \tLoss: 2.2994901203521287\n",
      "Epoch number:  8 \tLoss: 2.2994528531163687\n",
      "Epoch number:  10 \tLoss: 2.2994417323676366\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.7838401096834321\n",
      "Epoch number:  4 \tLoss: 0.5375062581846478\n",
      "Epoch number:  6 \tLoss: 0.4318175450127454\n",
      "Epoch number:  8 \tLoss: 0.3779918917648406\n",
      "Epoch number:  10 \tLoss: 0.34733041951616983\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.330123822973277\n",
      "Epoch number:  4 \tLoss: 2.30043892103721\n",
      "Epoch number:  6 \tLoss: 2.2996205257897158\n",
      "Epoch number:  8 \tLoss: 2.299579199475301\n",
      "Epoch number:  10 \tLoss: 2.2995606524088603\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.471123285391412\n",
      "Epoch number:  4 \tLoss: 1.704160019489844\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.50054083390511\n",
      "Epoch number:  4 \tLoss: 2.3320359855304247\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.809323722037542\n",
      "Epoch number:  4 \tLoss: 0.9150890873166148\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.4932706304376846\n",
      "Epoch number:  4 \tLoss: 2.326759891302377\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.7242979816749309\n",
      "Epoch number:  4 \tLoss: 0.5113041993495331\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.4834832667068167\n",
      "Epoch number:  4 \tLoss: 2.321101390159062\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.4973389799106993\n",
      "Epoch number:  4 \tLoss: 1.4289541056178203\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.4982659247517622\n",
      "Epoch number:  4 \tLoss: 2.3302680896343735\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.7522410107492226\n",
      "Epoch number:  4 \tLoss: 0.6794065151832861\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.4840966830463467\n",
      "Epoch number:  4 \tLoss: 2.321437796964725\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.9835000546536705\n",
      "Epoch number:  4 \tLoss: 0.5229794375274815\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.479704051270393\n",
      "Epoch number:  4 \tLoss: 2.3193178615069825\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.5004408843595862\n",
      "Epoch number:  4 \tLoss: 2.3302872520367135\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.4940674012639903\n",
      "Epoch number:  4 \tLoss: 2.3274632203604253\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.7062793625926218\n",
      "Epoch number:  4 \tLoss: 0.9248535456863017\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.4911640010943437\n",
      "Epoch number:  4 \tLoss: 2.3256433072293246\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.8497782883743997\n",
      "Epoch number:  4 \tLoss: 0.514184635424201\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.4841385193069603\n",
      "Epoch number:  4 \tLoss: 2.3217790303818298\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.4929375081470724\n",
      "Epoch number:  4 \tLoss: 1.5331054898416914\n",
      "Epoch number:  6 \tLoss: 0.7196644078668073\n",
      "Epoch number:  8 \tLoss: 0.538852859775566\n",
      "Epoch number:  10 \tLoss: 0.46054583548582756\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.4970172240786126\n",
      "Epoch number:  4 \tLoss: 2.329343844351241\n",
      "Epoch number:  6 \tLoss: 2.3039509180646833\n",
      "Epoch number:  8 \tLoss: 2.299973598987512\n",
      "Epoch number:  10 \tLoss: 2.299348302455416\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.0450862913691226\n",
      "Epoch number:  4 \tLoss: 1.0787312105256455\n",
      "Epoch number:  6 \tLoss: 0.6493631107501892\n",
      "Epoch number:  8 \tLoss: 0.46360034303712305\n",
      "Epoch number:  10 \tLoss: 0.40092138274648553\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.491068235330367\n",
      "Epoch number:  4 \tLoss: 2.3253658467095817\n",
      "Epoch number:  6 \tLoss: 2.3027946394979892\n",
      "Epoch number:  8 \tLoss: 2.2997279247237774\n",
      "Epoch number:  10 \tLoss: 2.299312177745504\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.7550559518388325\n",
      "Epoch number:  4 \tLoss: 0.4962618642190647\n",
      "Epoch number:  6 \tLoss: 0.41409856896210673\n",
      "Epoch number:  8 \tLoss: 0.36395154578670885\n",
      "Epoch number:  10 \tLoss: 0.33382089607322674\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.480997463077883\n",
      "Epoch number:  4 \tLoss: 2.3198997769026515\n",
      "Epoch number:  6 \tLoss: 2.3014961433082233\n",
      "Epoch number:  8 \tLoss: 2.2994802651719484\n",
      "Epoch number:  10 \tLoss: 2.2992585586926926\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.469120175947041\n",
      "Epoch number:  4 \tLoss: 1.506906294473207\n",
      "Epoch number:  6 \tLoss: 0.7463965470326999\n",
      "Epoch number:  8 \tLoss: 0.5811919976658081\n",
      "Epoch number:  10 \tLoss: 0.48457199700928827\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.495362140198223\n",
      "Epoch number:  4 \tLoss: 2.3281712937273746\n",
      "Epoch number:  6 \tLoss: 2.3035917524071228\n",
      "Epoch number:  8 \tLoss: 2.2998959883744416\n",
      "Epoch number:  10 \tLoss: 2.299339539413453\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.7232853234930634\n",
      "Epoch number:  4 \tLoss: 0.8638342544658724\n",
      "Epoch number:  6 \tLoss: 0.5328226297381444\n",
      "Epoch number:  8 \tLoss: 0.4353889082225982\n",
      "Epoch number:  10 \tLoss: 0.39341818809843343\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.489277595856696\n",
      "Epoch number:  4 \tLoss: 2.324294632556388\n",
      "Epoch number:  6 \tLoss: 2.3025200205999767\n",
      "Epoch number:  8 \tLoss: 2.299677076611183\n",
      "Epoch number:  10 \tLoss: 2.2993064098698466\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.8652763055481629\n",
      "Epoch number:  4 \tLoss: 0.5106533364544632\n",
      "Epoch number:  6 \tLoss: 0.4217019951293859\n",
      "Epoch number:  8 \tLoss: 0.36942217619182494\n",
      "Epoch number:  10 \tLoss: 0.33926442584246425\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.4884940206196204\n",
      "Epoch number:  4 \tLoss: 2.323832922325282\n",
      "Epoch number:  6 \tLoss: 2.3024171511705718\n",
      "Epoch number:  8 \tLoss: 2.2996715473379594\n",
      "Epoch number:  10 \tLoss: 2.2993193026691343\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.4881839718422643\n",
      "Epoch number:  4 \tLoss: 1.6755196596089834\n",
      "Epoch number:  6 \tLoss: 0.8948022745042196\n",
      "Epoch number:  8 \tLoss: 0.6255477627416234\n",
      "Epoch number:  10 \tLoss: 0.5106529615353457\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.4971492375644737\n",
      "Epoch number:  4 \tLoss: 2.329558622182166\n",
      "Epoch number:  6 \tLoss: 2.304143293587058\n",
      "Epoch number:  8 \tLoss: 2.3001445971533374\n",
      "Epoch number:  10 \tLoss: 2.2995096077141217\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.7619543471563954\n",
      "Epoch number:  4 \tLoss: 0.9031870769776617\n",
      "Epoch number:  6 \tLoss: 0.5644381584363038\n",
      "Epoch number:  8 \tLoss: 0.4531305929327006\n",
      "Epoch number:  10 \tLoss: 0.40031113563561466\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.4880575799807914\n",
      "Epoch number:  4 \tLoss: 2.3238527110243563\n",
      "Epoch number:  6 \tLoss: 2.3026675390370794\n",
      "Epoch number:  8 \tLoss: 2.299966947696157\n",
      "Epoch number:  10 \tLoss: 2.2996197588997824\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.7408844047897408\n",
      "Epoch number:  4 \tLoss: 0.5103024411005649\n",
      "Epoch number:  6 \tLoss: 0.4215370882540693\n",
      "Epoch number:  8 \tLoss: 0.37340451429904226\n",
      "Epoch number:  10 \tLoss: 0.3433236424509709\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.476080300306274\n",
      "Epoch number:  4 \tLoss: 2.3181139477424355\n",
      "Epoch number:  6 \tLoss: 2.3015772420727973\n",
      "Epoch number:  8 \tLoss: 2.2999255250452664\n",
      "Epoch number:  10 \tLoss: 2.299755545665669\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.45915689154856903\n",
      "Epoch number:  4 \tLoss: 0.41727148453295654\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010648725968044\n",
      "Epoch number:  4 \tLoss: 2.3010615437448245\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.4193717725360068\n",
      "Epoch number:  4 \tLoss: 0.3916613071272141\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010668316170046\n",
      "Epoch number:  4 \tLoss: 2.301061552589899\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.40507819092992503\n",
      "Epoch number:  4 \tLoss: 0.32999849667343645\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3011337221839647\n",
      "Epoch number:  4 \tLoss: 2.3010393177481565\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.4531369692359819\n",
      "Epoch number:  4 \tLoss: 0.4020076555535041\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010818146985645\n",
      "Epoch number:  4 \tLoss: 2.301076812128407\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.4367258092325019\n",
      "Epoch number:  4 \tLoss: 0.38799722756157345\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010637510995324\n",
      "Epoch number:  4 \tLoss: 2.301052235752869\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.40231462466277534\n",
      "Epoch number:  4 \tLoss: 0.34833257390055655\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010729795681315\n",
      "Epoch number:  4 \tLoss: 2.3010505800028467\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.5106010912107852\n",
      "Epoch number:  4 \tLoss: 0.4435563336751201\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010614619547307\n",
      "Epoch number:  4 \tLoss: 2.3010532792302123\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.46861415621098274\n",
      "Epoch number:  4 \tLoss: 0.41468415613692233\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301066591479677\n",
      "Epoch number:  4 \tLoss: 2.3010542716195164\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.42054825287955155\n",
      "Epoch number:  4 \tLoss: 0.3965790535258268\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301109230247989\n",
      "Epoch number:  4 \tLoss: 2.301062735401282\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.4842111739744306\n",
      "Epoch number:  4 \tLoss: 0.45266202425311447\n",
      "Epoch number:  6 \tLoss: 0.42848070896318685\n",
      "Epoch number:  8 \tLoss: 0.42619280460514075\n",
      "Epoch number:  10 \tLoss: 0.40818476898718103\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010599625668746\n",
      "Epoch number:  4 \tLoss: 2.301055225838256\n",
      "Epoch number:  6 \tLoss: 2.3010533409372127\n",
      "Epoch number:  8 \tLoss: 2.3010502972243803\n",
      "Epoch number:  10 \tLoss: 2.3010493644374233\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.4577123510252399\n",
      "Epoch number:  4 \tLoss: 0.37776636283665166\n",
      "Epoch number:  6 \tLoss: 0.35867818670253293\n",
      "Epoch number:  8 \tLoss: 0.3576026155773107\n",
      "Epoch number:  10 \tLoss: 0.32935067466803924\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010954805098924\n",
      "Epoch number:  4 \tLoss: 2.301088955767492\n",
      "Epoch number:  6 \tLoss: 2.3010847146171636\n",
      "Epoch number:  8 \tLoss: 2.30108253471636\n",
      "Epoch number:  10 \tLoss: 2.301079273686752\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.40531201688580604\n",
      "Epoch number:  4 \tLoss: 0.3632799162954733\n",
      "Epoch number:  6 \tLoss: 0.333729042800647\n",
      "Epoch number:  8 \tLoss: 0.30894329478741417\n",
      "Epoch number:  10 \tLoss: 0.2808369923449637\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3011030957162575\n",
      "Epoch number:  4 \tLoss: 2.301055774514565\n",
      "Epoch number:  6 \tLoss: 1.382757572040404\n",
      "Epoch number:  8 \tLoss: 0.47785593767062173\n",
      "Epoch number:  10 \tLoss: 0.388067180928497\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.4340109936876839\n",
      "Epoch number:  4 \tLoss: 0.41012938369264\n",
      "Epoch number:  6 \tLoss: 0.39773530466937757\n",
      "Epoch number:  8 \tLoss: 0.38124662437391404\n",
      "Epoch number:  10 \tLoss: 0.3507937683186592\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301014429937965\n",
      "Epoch number:  4 \tLoss: 2.30101323288172\n",
      "Epoch number:  6 \tLoss: 2.3010090471247704\n",
      "Epoch number:  8 \tLoss: 2.301011290868756\n",
      "Epoch number:  10 \tLoss: 2.3010088335333534\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.4331094354046606\n",
      "Epoch number:  4 \tLoss: 0.3713226874450022\n",
      "Epoch number:  6 \tLoss: 0.3480520971491127\n",
      "Epoch number:  8 \tLoss: 0.33394594380372483\n",
      "Epoch number:  10 \tLoss: 0.3322085245682785\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3011207320883433\n",
      "Epoch number:  4 \tLoss: 2.301113671703022\n",
      "Epoch number:  6 \tLoss: 2.301109237971859\n",
      "Epoch number:  8 \tLoss: 2.3011050381032754\n",
      "Epoch number:  10 \tLoss: 2.3010971411611054\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.39266360973275644\n",
      "Epoch number:  4 \tLoss: 0.36379777105116423\n",
      "Epoch number:  6 \tLoss: 0.32742478675174397\n",
      "Epoch number:  8 \tLoss: 0.3075945883771819\n",
      "Epoch number:  10 \tLoss: 0.33211422008408353\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3011266031894415\n",
      "Epoch number:  4 \tLoss: 2.301102873548547\n",
      "Epoch number:  6 \tLoss: 2.3009699997276534\n",
      "Epoch number:  8 \tLoss: 0.8593180848212714\n",
      "Epoch number:  10 \tLoss: 0.4256631263808785\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.5211355088045129\n",
      "Epoch number:  4 \tLoss: 0.4602322313097913\n",
      "Epoch number:  6 \tLoss: 0.43809091712516063\n",
      "Epoch number:  8 \tLoss: 0.4197419946034949\n",
      "Epoch number:  10 \tLoss: 0.4165808873098798\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010640140761183\n",
      "Epoch number:  4 \tLoss: 2.3010537357244565\n",
      "Epoch number:  6 \tLoss: 2.301052237544615\n",
      "Epoch number:  8 \tLoss: 2.3010517701541424\n",
      "Epoch number:  10 \tLoss: 2.3010515464988255\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.4446591427038866\n",
      "Epoch number:  4 \tLoss: 0.398702053284239\n",
      "Epoch number:  6 \tLoss: 0.39866349259349326\n",
      "Epoch number:  8 \tLoss: 0.3865521286505778\n",
      "Epoch number:  10 \tLoss: 0.378237406916167\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301073114368742\n",
      "Epoch number:  4 \tLoss: 2.301055721902339\n",
      "Epoch number:  6 \tLoss: 2.3010530105970126\n",
      "Epoch number:  8 \tLoss: 2.3010521628345657\n",
      "Epoch number:  10 \tLoss: 2.3010517577571474\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.44060689068090275\n",
      "Epoch number:  4 \tLoss: 0.4146026699419655\n",
      "Epoch number:  6 \tLoss: 0.3979954390306141\n",
      "Epoch number:  8 \tLoss: 0.39669472640272024\n",
      "Epoch number:  10 \tLoss: 0.3845185225875203\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010884142393855\n",
      "Epoch number:  4 \tLoss: 2.3010587433571725\n",
      "Epoch number:  6 \tLoss: 2.3010541741824135\n",
      "Epoch number:  8 \tLoss: 2.301052753023266\n",
      "Epoch number:  10 \tLoss: 2.301052075195674\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.48240313590212114\n",
      "Epoch number:  4 \tLoss: 0.3929121810977465\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300081858196538\n",
      "Epoch number:  4 \tLoss: 2.3000798997362537\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.42661821115663845\n",
      "Epoch number:  4 \tLoss: 0.3674491627536536\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3001127398696233\n",
      "Epoch number:  4 \tLoss: 2.300108180217333\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.40075798674847635\n",
      "Epoch number:  4 \tLoss: 0.3399785259970855\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3000829617423353\n",
      "Epoch number:  4 \tLoss: 2.300045153804652\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.4514614542285225\n",
      "Epoch number:  4 \tLoss: 0.40421183857470705\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3000749427541223\n",
      "Epoch number:  4 \tLoss: 2.3000726652653114\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.43363098897098074\n",
      "Epoch number:  4 \tLoss: 0.3585620039617416\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3001391156405697\n",
      "Epoch number:  4 \tLoss: 2.3001365668257976\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.3898824204567147\n",
      "Epoch number:  4 \tLoss: 0.3498495507805148\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3000813351458573\n",
      "Epoch number:  4 \tLoss: 2.2999562858256404\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.48952828546932886\n",
      "Epoch number:  4 \tLoss: 0.4483895535405225\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3001351566082966\n",
      "Epoch number:  4 \tLoss: 2.300099322001544\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.4498524260832116\n",
      "Epoch number:  4 \tLoss: 0.40279185270118517\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300227824227343\n",
      "Epoch number:  4 \tLoss: 2.3001502149712834\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.4205516069726963\n",
      "Epoch number:  4 \tLoss: 0.3776878727689095\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300363062439766\n",
      "Epoch number:  4 \tLoss: 2.3002267054711787\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.4612239769021853\n",
      "Epoch number:  4 \tLoss: 0.42804025048031347\n",
      "Epoch number:  6 \tLoss: 0.38693039780128763\n",
      "Epoch number:  8 \tLoss: 0.3798312856586495\n",
      "Epoch number:  10 \tLoss: 0.3499528844984026\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3000970428295044\n",
      "Epoch number:  4 \tLoss: 2.3000949310952374\n",
      "Epoch number:  6 \tLoss: 2.3000932579634936\n",
      "Epoch number:  8 \tLoss: 2.300091911383799\n",
      "Epoch number:  10 \tLoss: 2.300090741656927\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.4672433718335715\n",
      "Epoch number:  4 \tLoss: 0.39157635088856363\n",
      "Epoch number:  6 \tLoss: 0.3468904826171173\n",
      "Epoch number:  8 \tLoss: 0.3192011075190548\n",
      "Epoch number:  10 \tLoss: 0.2965276074531152\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300036134597976\n",
      "Epoch number:  4 \tLoss: 2.300032661971516\n",
      "Epoch number:  6 \tLoss: 2.300029279375646\n",
      "Epoch number:  8 \tLoss: 2.3000255810970933\n",
      "Epoch number:  10 \tLoss: 2.300020980264598\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.3956462647298498\n",
      "Epoch number:  4 \tLoss: 0.33694010185762463\n",
      "Epoch number:  6 \tLoss: 0.32081054832965167\n",
      "Epoch number:  8 \tLoss: 0.3034114747859711\n",
      "Epoch number:  10 \tLoss: 0.2936034891449344\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300064479745553\n",
      "Epoch number:  4 \tLoss: 2.2999998088218807\n",
      "Epoch number:  6 \tLoss: 1.5475340784958111\n",
      "Epoch number:  8 \tLoss: 0.4648169932571696\n",
      "Epoch number:  10 \tLoss: 0.4060160348706918\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.47855076710228156\n",
      "Epoch number:  4 \tLoss: 0.42813505166966515\n",
      "Epoch number:  6 \tLoss: 0.38189386905390815\n",
      "Epoch number:  8 \tLoss: 0.3739367556510702\n",
      "Epoch number:  10 \tLoss: 0.3561439437544623\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300053927648797\n",
      "Epoch number:  4 \tLoss: 2.300053282423161\n",
      "Epoch number:  6 \tLoss: 2.300052784749852\n",
      "Epoch number:  8 \tLoss: 2.3000524771807798\n",
      "Epoch number:  10 \tLoss: 2.300052269541053\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.43314953957555064\n",
      "Epoch number:  4 \tLoss: 0.3764308275761485\n",
      "Epoch number:  6 \tLoss: 0.35043772422541064\n",
      "Epoch number:  8 \tLoss: 0.34042554189450863\n",
      "Epoch number:  10 \tLoss: 0.337079315636725\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300064058231455\n",
      "Epoch number:  4 \tLoss: 2.3000631223778956\n",
      "Epoch number:  6 \tLoss: 2.3000617391536573\n",
      "Epoch number:  8 \tLoss: 2.3000604817179435\n",
      "Epoch number:  10 \tLoss: 2.300059351184436\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.4135639516303277\n",
      "Epoch number:  4 \tLoss: 0.34207608407195444\n",
      "Epoch number:  6 \tLoss: 0.3130091546697184\n",
      "Epoch number:  8 \tLoss: 0.29937042617575493\n",
      "Epoch number:  10 \tLoss: 0.27101767533259447\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3000814036140214\n",
      "Epoch number:  4 \tLoss: 2.3000667453215504\n",
      "Epoch number:  6 \tLoss: 2.2999199459519075\n",
      "Epoch number:  8 \tLoss: 0.7379612138300996\n",
      "Epoch number:  10 \tLoss: 0.413005229540976\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.49774790217346376\n",
      "Epoch number:  4 \tLoss: 0.42261502454621624\n",
      "Epoch number:  6 \tLoss: 0.4163916634518595\n",
      "Epoch number:  8 \tLoss: 0.38577702682765386\n",
      "Epoch number:  10 \tLoss: 0.37665159913477736\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3002027804164835\n",
      "Epoch number:  4 \tLoss: 2.3001337077235133\n",
      "Epoch number:  6 \tLoss: 2.3000990219598885\n",
      "Epoch number:  8 \tLoss: 2.300081337440074\n",
      "Epoch number:  10 \tLoss: 2.300072141054989\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.4529071155096677\n",
      "Epoch number:  4 \tLoss: 0.4009485078090554\n",
      "Epoch number:  6 \tLoss: 0.39713133515516597\n",
      "Epoch number:  8 \tLoss: 0.3719648001969927\n",
      "Epoch number:  10 \tLoss: 0.39257335254223674\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300188393673386\n",
      "Epoch number:  4 \tLoss: 2.300127447256514\n",
      "Epoch number:  6 \tLoss: 2.3000961762509244\n",
      "Epoch number:  8 \tLoss: 2.300079972815647\n",
      "Epoch number:  10 \tLoss: 2.300071446706688\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.40985126085136275\n",
      "Epoch number:  4 \tLoss: 0.3591853333816842\n",
      "Epoch number:  6 \tLoss: 0.35005967939150107\n",
      "Epoch number:  8 \tLoss: 0.3431216781007933\n",
      "Epoch number:  10 \tLoss: 0.34506587461666927\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3003168911509273\n",
      "Epoch number:  4 \tLoss: 2.3001982256922804\n",
      "Epoch number:  6 \tLoss: 2.3001350269205054\n",
      "Epoch number:  8 \tLoss: 2.300101524942535\n",
      "Epoch number:  10 \tLoss: 2.300083636487587\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.4586732795134884\n",
      "Epoch number:  4 \tLoss: 0.43071472637051805\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2996272499641663\n",
      "Epoch number:  4 \tLoss: 2.2996241221161884\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.43134544148717774\n",
      "Epoch number:  4 \tLoss: 0.3717261272210974\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2996725808691383\n",
      "Epoch number:  4 \tLoss: 2.2996650075342235\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.39108899949012227\n",
      "Epoch number:  4 \tLoss: 0.41481297256782496\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299634795325405\n",
      "Epoch number:  4 \tLoss: 2.299598897529725\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.48091896169081155\n",
      "Epoch number:  4 \tLoss: 0.4195666704972694\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2996292713433846\n",
      "Epoch number:  4 \tLoss: 2.299626402678814\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.43939487436077596\n",
      "Epoch number:  4 \tLoss: 0.40810989269436654\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299773769888666\n",
      "Epoch number:  4 \tLoss: 2.2997684377640546\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.4104295869270133\n",
      "Epoch number:  4 \tLoss: 0.388999340949486\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299680726038085\n",
      "Epoch number:  4 \tLoss: 2.299604078172458\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.5057059237215702\n",
      "Epoch number:  4 \tLoss: 0.416130119293755\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299661010175044\n",
      "Epoch number:  4 \tLoss: 2.299646250088145\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.4000614462677728\n",
      "Epoch number:  4 \tLoss: 0.38159613424948124\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2998462021448502\n",
      "Epoch number:  4 \tLoss: 2.2998078924079333\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.42693359740939474\n",
      "Epoch number:  4 \tLoss: 0.39681446721931973\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2999496201308585\n",
      "Epoch number:  4 \tLoss: 2.2998853284966754\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.43515717245373353\n",
      "Epoch number:  4 \tLoss: 0.40133719355878544\n",
      "Epoch number:  6 \tLoss: 0.3827734414081946\n",
      "Epoch number:  8 \tLoss: 0.3658389128861778\n",
      "Epoch number:  10 \tLoss: 0.3499075799491175\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2996489824917257\n",
      "Epoch number:  4 \tLoss: 2.2996462459422777\n",
      "Epoch number:  6 \tLoss: 2.299643694303636\n",
      "Epoch number:  8 \tLoss: 2.299641316079294\n",
      "Epoch number:  10 \tLoss: 2.299639084213045\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.4320935932113901\n",
      "Epoch number:  4 \tLoss: 0.38125691923413413\n",
      "Epoch number:  6 \tLoss: 0.345073412417161\n",
      "Epoch number:  8 \tLoss: 0.33999346087882004\n",
      "Epoch number:  10 \tLoss: 0.3298107769193562\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299698918555656\n",
      "Epoch number:  4 \tLoss: 2.2996943193304724\n",
      "Epoch number:  6 \tLoss: 2.29969028630626\n",
      "Epoch number:  8 \tLoss: 2.2996867834524273\n",
      "Epoch number:  10 \tLoss: 2.2996836276356083\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.4027507136552437\n",
      "Epoch number:  4 \tLoss: 0.3700573332342646\n",
      "Epoch number:  6 \tLoss: 0.3404206717408634\n",
      "Epoch number:  8 \tLoss: 0.31932625926958014\n",
      "Epoch number:  10 \tLoss: 0.3014996718023045\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2996577410984176\n",
      "Epoch number:  4 \tLoss: 2.299608337413836\n",
      "Epoch number:  6 \tLoss: 2.289039078708721\n",
      "Epoch number:  8 \tLoss: 0.5932097880346558\n",
      "Epoch number:  10 \tLoss: 0.39903685879752016\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.44873089058993065\n",
      "Epoch number:  4 \tLoss: 0.4165723065353618\n",
      "Epoch number:  6 \tLoss: 0.3946703309056726\n",
      "Epoch number:  8 \tLoss: 0.39865404413103767\n",
      "Epoch number:  10 \tLoss: 0.39273729259830625\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2996301153723766\n",
      "Epoch number:  4 \tLoss: 2.2996259022453898\n",
      "Epoch number:  6 \tLoss: 2.2996220624886186\n",
      "Epoch number:  8 \tLoss: 2.2996185495807024\n",
      "Epoch number:  10 \tLoss: 2.299615325187281\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.4373054188795457\n",
      "Epoch number:  4 \tLoss: 0.4030004279118605\n",
      "Epoch number:  6 \tLoss: 0.3783097794731255\n",
      "Epoch number:  8 \tLoss: 0.33226992911867226\n",
      "Epoch number:  10 \tLoss: 0.31743111267028185\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2996202790482623\n",
      "Epoch number:  4 \tLoss: 2.299614509444123\n",
      "Epoch number:  6 \tLoss: 2.2996090144699086\n",
      "Epoch number:  8 \tLoss: 2.2996037025431293\n",
      "Epoch number:  10 \tLoss: 2.2995973295180283\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.42342814618485963\n",
      "Epoch number:  4 \tLoss: 0.38942436746108816\n",
      "Epoch number:  6 \tLoss: 0.33643322501682543\n",
      "Epoch number:  8 \tLoss: 0.32366517018268387\n",
      "Epoch number:  10 \tLoss: 0.2904877531748883\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2997732249648806\n",
      "Epoch number:  4 \tLoss: 1.661773388523546\n",
      "Epoch number:  6 \tLoss: 0.4698065342146455\n",
      "Epoch number:  8 \tLoss: 0.36859396852579024\n",
      "Epoch number:  10 \tLoss: 0.33334534005150057\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.46317079621584806\n",
      "Epoch number:  4 \tLoss: 0.41275817265729386\n",
      "Epoch number:  6 \tLoss: 0.40532641816231146\n",
      "Epoch number:  8 \tLoss: 0.3608393326660712\n",
      "Epoch number:  10 \tLoss: 0.3965459925314747\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299743878786578\n",
      "Epoch number:  4 \tLoss: 2.299713285790232\n",
      "Epoch number:  6 \tLoss: 2.2996878175864524\n",
      "Epoch number:  8 \tLoss: 2.2996667165683786\n",
      "Epoch number:  10 \tLoss: 2.2996492962193607\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.4088458390499543\n",
      "Epoch number:  4 \tLoss: 0.3961547157687242\n",
      "Epoch number:  6 \tLoss: 0.38281289598242535\n",
      "Epoch number:  8 \tLoss: 0.348180026982295\n",
      "Epoch number:  10 \tLoss: 0.32288142566493255\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2998124631130588\n",
      "Epoch number:  4 \tLoss: 2.2997781293671307\n",
      "Epoch number:  6 \tLoss: 2.2997473366838546\n",
      "Epoch number:  8 \tLoss: 2.299720162969408\n",
      "Epoch number:  10 \tLoss: 2.2996964973277376\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.42664718265685436\n",
      "Epoch number:  4 \tLoss: 0.399685828784418\n",
      "Epoch number:  6 \tLoss: 0.3529971764703838\n",
      "Epoch number:  8 \tLoss: 0.3161098704069285\n",
      "Epoch number:  10 \tLoss: 0.3120318029158712\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300162658047166\n",
      "Epoch number:  4 \tLoss: 2.3000773643292924\n",
      "Epoch number:  6 \tLoss: 2.3000023225957666\n",
      "Epoch number:  8 \tLoss: 2.2999363455303254\n",
      "Epoch number:  10 \tLoss: 2.2998787722471374\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.4369043698481699\n",
      "Epoch number:  4 \tLoss: 0.39045541012141094\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010512005725565\n",
      "Epoch number:  4 \tLoss: 2.3010468024732167\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.42593726683402516\n",
      "Epoch number:  4 \tLoss: 0.3674264655287742\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010571506453896\n",
      "Epoch number:  4 \tLoss: 2.3010520248324524\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.40658083541652695\n",
      "Epoch number:  4 \tLoss: 0.35363282808993274\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3011304102441907\n",
      "Epoch number:  4 \tLoss: 2.301059601072317\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.47785425173189655\n",
      "Epoch number:  4 \tLoss: 0.42289285976362795\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010469245478222\n",
      "Epoch number:  4 \tLoss: 2.3010427185322886\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.4058781937919957\n",
      "Epoch number:  4 \tLoss: 0.36248181224938275\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300974567907582\n",
      "Epoch number:  4 \tLoss: 2.3009717408300028\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.397511571923037\n",
      "Epoch number:  4 \tLoss: 0.3435289050889712\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301078139471126\n",
      "Epoch number:  4 \tLoss: 2.3010564719655253\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.5411628049109881\n",
      "Epoch number:  4 \tLoss: 0.45055121246585356\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010565213266685\n",
      "Epoch number:  4 \tLoss: 2.301040140111127\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.4436837661093827\n",
      "Epoch number:  4 \tLoss: 0.39623337292424776\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010585503350325\n",
      "Epoch number:  4 \tLoss: 2.301040553168795\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.44091850262523985\n",
      "Epoch number:  4 \tLoss: 0.414932025378132\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010969932448515\n",
      "Epoch number:  4 \tLoss: 2.301047840049709\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.431160546097947\n",
      "Epoch number:  4 \tLoss: 0.38342641404121347\n",
      "Epoch number:  6 \tLoss: 0.36833844822813844\n",
      "Epoch number:  8 \tLoss: 0.3485982506032656\n",
      "Epoch number:  10 \tLoss: 0.33118925266619564\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301041130793858\n",
      "Epoch number:  4 \tLoss: 2.301039111979894\n",
      "Epoch number:  6 \tLoss: 2.301036896306238\n",
      "Epoch number:  8 \tLoss: 2.301036008998348\n",
      "Epoch number:  10 \tLoss: 2.3010353994186823\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.43147531133571865\n",
      "Epoch number:  4 \tLoss: 0.396997746583006\n",
      "Epoch number:  6 \tLoss: 0.3361676092021473\n",
      "Epoch number:  8 \tLoss: 0.3178109040205039\n",
      "Epoch number:  10 \tLoss: 0.3307732946966786\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3011272452057048\n",
      "Epoch number:  4 \tLoss: 2.3011202696797763\n",
      "Epoch number:  6 \tLoss: 2.301111644311514\n",
      "Epoch number:  8 \tLoss: 2.3011037886585437\n",
      "Epoch number:  10 \tLoss: 2.3010985239532245\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.40305533724808446\n",
      "Epoch number:  4 \tLoss: 0.3606613126673341\n",
      "Epoch number:  6 \tLoss: 0.31163620879496784\n",
      "Epoch number:  8 \tLoss: 0.2995110032467568\n",
      "Epoch number:  10 \tLoss: 0.28556312753158053\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010341574547426\n",
      "Epoch number:  4 \tLoss: 2.3009816050739635\n",
      "Epoch number:  6 \tLoss: 1.6075141161745927\n",
      "Epoch number:  8 \tLoss: 0.44656608124432345\n",
      "Epoch number:  10 \tLoss: 0.38103145490611556\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.4504447986890665\n",
      "Epoch number:  4 \tLoss: 0.40845795173990923\n",
      "Epoch number:  6 \tLoss: 0.36655097235668477\n",
      "Epoch number:  8 \tLoss: 0.3575378402261313\n",
      "Epoch number:  10 \tLoss: 0.3579039803093586\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010994036771173\n",
      "Epoch number:  4 \tLoss: 2.3010921065738787\n",
      "Epoch number:  6 \tLoss: 2.301086834140455\n",
      "Epoch number:  8 \tLoss: 2.301082857314384\n",
      "Epoch number:  10 \tLoss: 2.301079728373066\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.44012376368715495\n",
      "Epoch number:  4 \tLoss: 0.38195320913669123\n",
      "Epoch number:  6 \tLoss: 0.34859827108544306\n",
      "Epoch number:  8 \tLoss: 0.32724717769546696\n",
      "Epoch number:  10 \tLoss: 0.33645378713792634\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010508411004706\n",
      "Epoch number:  4 \tLoss: 2.301043621547796\n",
      "Epoch number:  6 \tLoss: 2.301036877898107\n",
      "Epoch number:  8 \tLoss: 2.301033815612583\n",
      "Epoch number:  10 \tLoss: 2.3010325106141383\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.4151594125192389\n",
      "Epoch number:  4 \tLoss: 0.36063166257445545\n",
      "Epoch number:  6 \tLoss: 0.31939598137788966\n",
      "Epoch number:  8 \tLoss: 0.29870272227012323\n",
      "Epoch number:  10 \tLoss: 0.27909439437745454\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.30109728393543\n",
      "Epoch number:  4 \tLoss: 2.301054081709856\n",
      "Epoch number:  6 \tLoss: 2.3006039909619895\n",
      "Epoch number:  8 \tLoss: 0.6142484234121313\n",
      "Epoch number:  10 \tLoss: 0.42028952035863265\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.488408267772917\n",
      "Epoch number:  4 \tLoss: 0.4278689333219047\n",
      "Epoch number:  6 \tLoss: 0.4187429813151564\n",
      "Epoch number:  8 \tLoss: 0.4143294535338773\n",
      "Epoch number:  10 \tLoss: 0.4003494217499835\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301051625333863\n",
      "Epoch number:  4 \tLoss: 2.301039163437811\n",
      "Epoch number:  6 \tLoss: 2.301037332528565\n",
      "Epoch number:  8 \tLoss: 2.301036762823772\n",
      "Epoch number:  10 \tLoss: 2.30103648945451\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.48035026987038765\n",
      "Epoch number:  4 \tLoss: 0.4343738733206957\n",
      "Epoch number:  6 \tLoss: 0.41610844097674143\n",
      "Epoch number:  8 \tLoss: 0.4120436449797592\n",
      "Epoch number:  10 \tLoss: 0.41705087298858073\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010659257692057\n",
      "Epoch number:  4 \tLoss: 2.301041850053677\n",
      "Epoch number:  6 \tLoss: 2.3010383673578416\n",
      "Epoch number:  8 \tLoss: 2.3010372905388103\n",
      "Epoch number:  10 \tLoss: 2.301036774870586\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.4352940505316998\n",
      "Epoch number:  4 \tLoss: 0.41090999210819146\n",
      "Epoch number:  6 \tLoss: 0.4028998405006447\n",
      "Epoch number:  8 \tLoss: 0.39298061532483686\n",
      "Epoch number:  10 \tLoss: 0.39417989479012333\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301099312686962\n",
      "Epoch number:  4 \tLoss: 2.3010483962236754\n",
      "Epoch number:  6 \tLoss: 2.301040887820526\n",
      "Epoch number:  8 \tLoss: 2.3010385740780532\n",
      "Epoch number:  10 \tLoss: 2.301037468860012\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.4273344538229099\n",
      "Epoch number:  4 \tLoss: 0.4035673800617689\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300082087090347\n",
      "Epoch number:  4 \tLoss: 2.300080271897987\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.4225173844939621\n",
      "Epoch number:  4 \tLoss: 0.3601341488662458\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3000711442626587\n",
      "Epoch number:  4 \tLoss: 2.300064471074888\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.3917101210354563\n",
      "Epoch number:  4 \tLoss: 0.345867048952693\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300060272183005\n",
      "Epoch number:  4 \tLoss: 2.3000260795273184\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.48683339026996825\n",
      "Epoch number:  4 \tLoss: 0.3944283089741159\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300059127224913\n",
      "Epoch number:  4 \tLoss: 2.30005785016972\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.42974673663471574\n",
      "Epoch number:  4 \tLoss: 0.3602290233266409\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300038751142551\n",
      "Epoch number:  4 \tLoss: 2.3000337286154835\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.39298937823923996\n",
      "Epoch number:  4 \tLoss: 0.3322962727001319\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3000962129852387\n",
      "Epoch number:  4 \tLoss: 2.2982165538694397\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.4786055594562571\n",
      "Epoch number:  4 \tLoss: 0.40541100978794564\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3001006182329267\n",
      "Epoch number:  4 \tLoss: 2.3000782984829975\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.4428136693519699\n",
      "Epoch number:  4 \tLoss: 0.38672522387997055\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300182262650376\n",
      "Epoch number:  4 \tLoss: 2.3001236199190447\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.40244993075801905\n",
      "Epoch number:  4 \tLoss: 0.36747078245184894\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300256241117073\n",
      "Epoch number:  4 \tLoss: 2.3001575089059547\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.45076778329088246\n",
      "Epoch number:  4 \tLoss: 0.3869777992539824\n",
      "Epoch number:  6 \tLoss: 0.3586696347678775\n",
      "Epoch number:  8 \tLoss: 0.35884688728503605\n",
      "Epoch number:  10 \tLoss: 0.3485328643662736\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3000643995666357\n",
      "Epoch number:  4 \tLoss: 2.300063222076811\n",
      "Epoch number:  6 \tLoss: 2.300062430533217\n",
      "Epoch number:  8 \tLoss: 2.3000619212661055\n",
      "Epoch number:  10 \tLoss: 2.3000616233977342\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.4279993912121527\n",
      "Epoch number:  4 \tLoss: 0.37308585645014736\n",
      "Epoch number:  6 \tLoss: 0.3290864868667879\n",
      "Epoch number:  8 \tLoss: 0.3042446931988226\n",
      "Epoch number:  10 \tLoss: 0.29834936331007006\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300202538533254\n",
      "Epoch number:  4 \tLoss: 2.300197195839777\n",
      "Epoch number:  6 \tLoss: 2.300192724322126\n",
      "Epoch number:  8 \tLoss: 2.3001889053276767\n",
      "Epoch number:  10 \tLoss: 2.3001855012118066\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.4057501879996054\n",
      "Epoch number:  4 \tLoss: 0.332584808854589\n",
      "Epoch number:  6 \tLoss: 0.2982432875862652\n",
      "Epoch number:  8 \tLoss: 0.2858274362234967\n",
      "Epoch number:  10 \tLoss: 0.2587107367825641\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300269998302588\n",
      "Epoch number:  4 \tLoss: 2.300258898335927\n",
      "Epoch number:  6 \tLoss: 2.3001122513812913\n",
      "Epoch number:  8 \tLoss: 0.712938422835493\n",
      "Epoch number:  10 \tLoss: 0.42278168654173964\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.4437725298985862\n",
      "Epoch number:  4 \tLoss: 0.39110750702514796\n",
      "Epoch number:  6 \tLoss: 0.37179086493606656\n",
      "Epoch number:  8 \tLoss: 0.3541711229611106\n",
      "Epoch number:  10 \tLoss: 0.32255495201473555\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3001025039707046\n",
      "Epoch number:  4 \tLoss: 2.3001001563200982\n",
      "Epoch number:  6 \tLoss: 2.3000982534446797\n",
      "Epoch number:  8 \tLoss: 2.3000966751375427\n",
      "Epoch number:  10 \tLoss: 2.3000953331416487\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.4211337403160251\n",
      "Epoch number:  4 \tLoss: 0.37140004989046355\n",
      "Epoch number:  6 \tLoss: 0.3336035702664812\n",
      "Epoch number:  8 \tLoss: 0.3332681299369539\n",
      "Epoch number:  10 \tLoss: 0.3022536096057868\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3000930588676196\n",
      "Epoch number:  4 \tLoss: 2.3000888878568726\n",
      "Epoch number:  6 \tLoss: 2.3000853505920618\n",
      "Epoch number:  8 \tLoss: 2.3000824441954673\n",
      "Epoch number:  10 \tLoss: 2.3000800316583443\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.42675904470383025\n",
      "Epoch number:  4 \tLoss: 0.3679871590623026\n",
      "Epoch number:  6 \tLoss: 0.31559355473859046\n",
      "Epoch number:  8 \tLoss: 0.2826019298843979\n",
      "Epoch number:  10 \tLoss: 0.26857811654561253\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3001814639657643\n",
      "Epoch number:  4 \tLoss: 2.300111322788789\n",
      "Epoch number:  6 \tLoss: 1.4133000933263395\n",
      "Epoch number:  8 \tLoss: 0.45628428058052617\n",
      "Epoch number:  10 \tLoss: 0.4031370622500544\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.487258007030047\n",
      "Epoch number:  4 \tLoss: 0.4329828465686291\n",
      "Epoch number:  6 \tLoss: 0.405655028262269\n",
      "Epoch number:  8 \tLoss: 0.3862644501550508\n",
      "Epoch number:  10 \tLoss: 0.3778575366639204\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3001122000892322\n",
      "Epoch number:  4 \tLoss: 2.3000860906883145\n",
      "Epoch number:  6 \tLoss: 2.300071648597614\n",
      "Epoch number:  8 \tLoss: 2.3000637684405305\n",
      "Epoch number:  10 \tLoss: 2.300059472076972\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.4301482308674325\n",
      "Epoch number:  4 \tLoss: 0.37794399047778116\n",
      "Epoch number:  6 \tLoss: 0.3640258921128454\n",
      "Epoch number:  8 \tLoss: 0.3627144628076069\n",
      "Epoch number:  10 \tLoss: 0.3490529146733132\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3001594193668455\n",
      "Epoch number:  4 \tLoss: 2.300111857311011\n",
      "Epoch number:  6 \tLoss: 2.3000857153659138\n",
      "Epoch number:  8 \tLoss: 2.3000715466758344\n",
      "Epoch number:  10 \tLoss: 2.3000638658906514\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.41115779573171934\n",
      "Epoch number:  4 \tLoss: 0.3647682063854376\n",
      "Epoch number:  6 \tLoss: 0.3504067782394505\n",
      "Epoch number:  8 \tLoss: 0.3394145780176285\n",
      "Epoch number:  10 \tLoss: 0.3237978014771065\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300324627335017\n",
      "Epoch number:  4 \tLoss: 2.300204596794334\n",
      "Epoch number:  6 \tLoss: 2.3001372909186686\n",
      "Epoch number:  8 \tLoss: 2.3001004156614218\n",
      "Epoch number:  10 \tLoss: 2.3000802947884362\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.4500561268362806\n",
      "Epoch number:  4 \tLoss: 0.3920233139192587\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.29960942399679\n",
      "Epoch number:  4 \tLoss: 2.2996049454270806\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.4161044080103375\n",
      "Epoch number:  4 \tLoss: 0.3575681808112611\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2995761952109923\n",
      "Epoch number:  4 \tLoss: 2.299570430545396\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.4022139398401521\n",
      "Epoch number:  4 \tLoss: 0.3520114813308406\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299669034735895\n",
      "Epoch number:  4 \tLoss: 2.2995872337681464\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.43939798265238017\n",
      "Epoch number:  4 \tLoss: 0.3853927806286738\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299594219893051\n",
      "Epoch number:  4 \tLoss: 2.2995889123659654\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.4048740898464881\n",
      "Epoch number:  4 \tLoss: 0.35411663475768257\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2996114083691896\n",
      "Epoch number:  4 \tLoss: 2.29960781523821\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.38411482416069526\n",
      "Epoch number:  4 \tLoss: 0.3521125145307429\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299632329097891\n",
      "Epoch number:  4 \tLoss: 1.8982960331734342\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.451526142774289\n",
      "Epoch number:  4 \tLoss: 0.3756915466157715\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2996647912936528\n",
      "Epoch number:  4 \tLoss: 2.2996498246115777\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.42523697415186534\n",
      "Epoch number:  4 \tLoss: 0.36765887939095976\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299887597426706\n",
      "Epoch number:  4 \tLoss: 2.299843264541326\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.3955033266010413\n",
      "Epoch number:  4 \tLoss: 0.3440317990073582\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299879192951989\n",
      "Epoch number:  4 \tLoss: 2.299831624842918\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.43973136756674097\n",
      "Epoch number:  4 \tLoss: 0.404659194895624\n",
      "Epoch number:  6 \tLoss: 0.3605363331247042\n",
      "Epoch number:  8 \tLoss: 0.34854316899376264\n",
      "Epoch number:  10 \tLoss: 0.338634949842393\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299594294237679\n",
      "Epoch number:  4 \tLoss: 2.2995907404668654\n",
      "Epoch number:  6 \tLoss: 2.2995875048068024\n",
      "Epoch number:  8 \tLoss: 2.2995844335271016\n",
      "Epoch number:  10 \tLoss: 2.2995818601103823\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.43098073839717693\n",
      "Epoch number:  4 \tLoss: 0.37100074945506384\n",
      "Epoch number:  6 \tLoss: 0.3400793265451902\n",
      "Epoch number:  8 \tLoss: 0.3550653216719924\n",
      "Epoch number:  10 \tLoss: 0.3406362839955957\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2996671809357734\n",
      "Epoch number:  4 \tLoss: 2.299660287882368\n",
      "Epoch number:  6 \tLoss: 2.2996541746685892\n",
      "Epoch number:  8 \tLoss: 2.2996485216771605\n",
      "Epoch number:  10 \tLoss: 2.2996429915865324\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.3842835504959159\n",
      "Epoch number:  4 \tLoss: 0.33485171364319233\n",
      "Epoch number:  6 \tLoss: 0.30459650460306886\n",
      "Epoch number:  8 \tLoss: 0.2915694795658245\n",
      "Epoch number:  10 \tLoss: 0.2624104140475415\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2997064656760338\n",
      "Epoch number:  4 \tLoss: 2.299666207720412\n",
      "Epoch number:  6 \tLoss: 2.299524814201122\n",
      "Epoch number:  8 \tLoss: 0.7839498970299822\n",
      "Epoch number:  10 \tLoss: 0.45160808709770234\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.4931031123018521\n",
      "Epoch number:  4 \tLoss: 0.40503548892782887\n",
      "Epoch number:  6 \tLoss: 0.3841882813308722\n",
      "Epoch number:  8 \tLoss: 0.3726895522796451\n",
      "Epoch number:  10 \tLoss: 0.36656456204984067\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299602140126204\n",
      "Epoch number:  4 \tLoss: 2.299596092737892\n",
      "Epoch number:  6 \tLoss: 2.299590530447971\n",
      "Epoch number:  8 \tLoss: 2.299585475177768\n",
      "Epoch number:  10 \tLoss: 2.2995808683423555\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.4163078052494055\n",
      "Epoch number:  4 \tLoss: 0.3673796677857662\n",
      "Epoch number:  6 \tLoss: 0.357817792960362\n",
      "Epoch number:  8 \tLoss: 0.3355062590840302\n",
      "Epoch number:  10 \tLoss: 0.3051355614426241\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299648157895975\n",
      "Epoch number:  4 \tLoss: 2.299643522903537\n",
      "Epoch number:  6 \tLoss: 2.299642419253913\n",
      "Epoch number:  8 \tLoss: 2.299637540294095\n",
      "Epoch number:  10 \tLoss: 2.2996342654123354\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.38581267224696103\n",
      "Epoch number:  4 \tLoss: 0.3305143123759989\n",
      "Epoch number:  6 \tLoss: 0.3158417652140442\n",
      "Epoch number:  8 \tLoss: 0.3019880385566885\n",
      "Epoch number:  10 \tLoss: 0.2789708469543268\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299553021457407\n",
      "Epoch number:  4 \tLoss: 2.2994482296403063\n",
      "Epoch number:  6 \tLoss: 1.1984679217945207\n",
      "Epoch number:  8 \tLoss: 0.4435997695052569\n",
      "Epoch number:  10 \tLoss: 0.36762012368865277\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.5013396093169274\n",
      "Epoch number:  4 \tLoss: 0.41027546756853156\n",
      "Epoch number:  6 \tLoss: 0.40202165308788235\n",
      "Epoch number:  8 \tLoss: 0.37338206835159793\n",
      "Epoch number:  10 \tLoss: 0.3609504385863555\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2996638844534125\n",
      "Epoch number:  4 \tLoss: 2.29964868600386\n",
      "Epoch number:  6 \tLoss: 2.2996355827043105\n",
      "Epoch number:  8 \tLoss: 2.29962437205242\n",
      "Epoch number:  10 \tLoss: 2.299614858941625\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.4083051962181081\n",
      "Epoch number:  4 \tLoss: 0.3647727780227079\n",
      "Epoch number:  6 \tLoss: 0.34838117735659063\n",
      "Epoch number:  8 \tLoss: 0.3236854900235037\n",
      "Epoch number:  10 \tLoss: 0.33363829630693875\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2998636679129216\n",
      "Epoch number:  4 \tLoss: 2.2998184209881667\n",
      "Epoch number:  6 \tLoss: 2.299778817886667\n",
      "Epoch number:  8 \tLoss: 2.2997445417630282\n",
      "Epoch number:  10 \tLoss: 2.2997151841319186\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.37915718999968795\n",
      "Epoch number:  4 \tLoss: 0.340587986626968\n",
      "Epoch number:  6 \tLoss: 0.3201276692219328\n",
      "Epoch number:  8 \tLoss: 0.2981554668904476\n",
      "Epoch number:  10 \tLoss: 0.27900437071113854\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=random, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2999571940216974\n",
      "Epoch number:  4 \tLoss: 2.2999037985104\n",
      "Epoch number:  6 \tLoss: 2.2998573821558472\n",
      "Epoch number:  8 \tLoss: 2.2998160318466687\n",
      "Epoch number:  10 \tLoss: 2.2997795385488486\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300485720486288\n",
      "Epoch number:  4 \tLoss: 2.299326610614581\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300485057995601\n",
      "Epoch number:  4 \tLoss: 2.299326280721572\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300487358977095\n",
      "Epoch number:  4 \tLoss: 2.2993260823379975\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3004853667317837\n",
      "Epoch number:  4 \tLoss: 2.2993262584965577\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3004853853616987\n",
      "Epoch number:  4 \tLoss: 2.2993266121558404\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300485556821871\n",
      "Epoch number:  4 \tLoss: 2.2993261576604667\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300485809879291\n",
      "Epoch number:  4 \tLoss: 2.2993270375386206\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300485325738926\n",
      "Epoch number:  4 \tLoss: 2.299326243273328\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300484709278418\n",
      "Epoch number:  4 \tLoss: 2.29932584906392\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.30048265833311\n",
      "Epoch number:  4 \tLoss: 2.2993248419346917\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3004848193921057\n",
      "Epoch number:  4 \tLoss: 2.2993261433673786\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300485303220816\n",
      "Epoch number:  4 \tLoss: 2.2993262331598414\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300491448981906\n",
      "Epoch number:  4 \tLoss: 2.299331280706532\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300489358868006\n",
      "Epoch number:  4 \tLoss: 2.2993276809772336\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300490407798155\n",
      "Epoch number:  4 \tLoss: 2.2993288199214237\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300490241673077\n",
      "Epoch number:  4 \tLoss: 2.299328829416158\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3004904350502224\n",
      "Epoch number:  4 \tLoss: 2.299329437680303\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3004913323233884\n",
      "Epoch number:  4 \tLoss: 2.299330808169158\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3004867523365475\n",
      "Epoch number:  4 \tLoss: 2.299326005104289\n",
      "Epoch number:  6 \tLoss: 2.2993247666565404\n",
      "Epoch number:  8 \tLoss: 2.2993247394128096\n",
      "Epoch number:  10 \tLoss: 2.2993247545553444\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300484494415861\n",
      "Epoch number:  4 \tLoss: 2.2993258736539253\n",
      "Epoch number:  6 \tLoss: 2.299324527315271\n",
      "Epoch number:  8 \tLoss: 2.2993245251204018\n",
      "Epoch number:  10 \tLoss: 2.2993245245016007\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.30048658806129\n",
      "Epoch number:  4 \tLoss: 2.299326496975003\n",
      "Epoch number:  6 \tLoss: 2.2993251310589646\n",
      "Epoch number:  8 \tLoss: 2.2993251418613236\n",
      "Epoch number:  10 \tLoss: 2.2993251267580166\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300485739705277\n",
      "Epoch number:  4 \tLoss: 2.299325801713323\n",
      "Epoch number:  6 \tLoss: 2.2993244522311485\n",
      "Epoch number:  8 \tLoss: 2.2993244503757806\n",
      "Epoch number:  10 \tLoss: 2.299324450090762\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3004848021145747\n",
      "Epoch number:  4 \tLoss: 2.2993263302725993\n",
      "Epoch number:  6 \tLoss: 2.2993249690633073\n",
      "Epoch number:  8 \tLoss: 2.2993249576088894\n",
      "Epoch number:  10 \tLoss: 2.2993249404333196\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3004840891141303\n",
      "Epoch number:  4 \tLoss: 2.299326010184722\n",
      "Epoch number:  6 \tLoss: 2.299324665616669\n",
      "Epoch number:  8 \tLoss: 2.299324663768443\n",
      "Epoch number:  10 \tLoss: 2.2993246634850597\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3004854025410797\n",
      "Epoch number:  4 \tLoss: 2.299326340050511\n",
      "Epoch number:  6 \tLoss: 2.299324985395213\n",
      "Epoch number:  8 \tLoss: 2.299324970654044\n",
      "Epoch number:  10 \tLoss: 2.299324962528133\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3004834463196926\n",
      "Epoch number:  4 \tLoss: 2.299325680063368\n",
      "Epoch number:  6 \tLoss: 2.29932433645586\n",
      "Epoch number:  8 \tLoss: 2.2993243347397714\n",
      "Epoch number:  10 \tLoss: 2.29932433458457\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300483489674227\n",
      "Epoch number:  4 \tLoss: 2.2993263625105556\n",
      "Epoch number:  6 \tLoss: 2.29932500494044\n",
      "Epoch number:  8 \tLoss: 2.299325002370892\n",
      "Epoch number:  10 \tLoss: 2.2993249764106776\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300485917449203\n",
      "Epoch number:  4 \tLoss: 2.299326396469909\n",
      "Epoch number:  6 \tLoss: 2.29932504787749\n",
      "Epoch number:  8 \tLoss: 2.2993250458087378\n",
      "Epoch number:  10 \tLoss: 2.2993250453088976\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300485126553133\n",
      "Epoch number:  4 \tLoss: 2.299326242906203\n",
      "Epoch number:  6 \tLoss: 2.299324874334401\n",
      "Epoch number:  8 \tLoss: 2.299324870036875\n",
      "Epoch number:  10 \tLoss: 2.299324842551856\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3004861263423093\n",
      "Epoch number:  4 \tLoss: 2.299326402260678\n",
      "Epoch number:  6 \tLoss: 2.299325053140989\n",
      "Epoch number:  8 \tLoss: 2.299325051084474\n",
      "Epoch number:  10 \tLoss: 2.2993250505970693\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300489729151917\n",
      "Epoch number:  4 \tLoss: 2.2993283065172485\n",
      "Epoch number:  6 \tLoss: 2.2993265438036734\n",
      "Epoch number:  8 \tLoss: 2.299326210310464\n",
      "Epoch number:  10 \tLoss: 2.2993259410746156\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3004887801252245\n",
      "Epoch number:  4 \tLoss: 2.299326597881207\n",
      "Epoch number:  6 \tLoss: 2.299325154075201\n",
      "Epoch number:  8 \tLoss: 2.2993250820461957\n",
      "Epoch number:  10 \tLoss: 2.299325026025426\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.30049155440036\n",
      "Epoch number:  4 \tLoss: 2.299331103807997\n",
      "Epoch number:  6 \tLoss: 2.299328787080939\n",
      "Epoch number:  8 \tLoss: 2.299328002607725\n",
      "Epoch number:  10 \tLoss: 2.2993273800065586\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3004902505232123\n",
      "Epoch number:  4 \tLoss: 2.2993291458360114\n",
      "Epoch number:  6 \tLoss: 2.299327215658435\n",
      "Epoch number:  8 \tLoss: 2.2993267485537\n",
      "Epoch number:  10 \tLoss: 2.299326374113089\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300491505418867\n",
      "Epoch number:  4 \tLoss: 2.299331385921093\n",
      "Epoch number:  6 \tLoss: 2.2993290392558263\n",
      "Epoch number:  8 \tLoss: 2.2993282308211205\n",
      "Epoch number:  10 \tLoss: 2.299327576205719\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3004904017393044\n",
      "Epoch number:  4 \tLoss: 2.299329319216088\n",
      "Epoch number:  6 \tLoss: 2.2993273476493328\n",
      "Epoch number:  8 \tLoss: 2.2993268498399604\n",
      "Epoch number:  10 \tLoss: 2.2993264524862935\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3337841771903842\n",
      "Epoch number:  4 \tLoss: 2.3004100027050645\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3338027268934516\n",
      "Epoch number:  4 \tLoss: 2.3004121945273113\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.333777447287027\n",
      "Epoch number:  4 \tLoss: 2.300409894774078\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3337972171591748\n",
      "Epoch number:  4 \tLoss: 2.30041195984561\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.333820199627421\n",
      "Epoch number:  4 \tLoss: 2.300413838823516\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3337570540656962\n",
      "Epoch number:  4 \tLoss: 2.300407682262763\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3338345806238783\n",
      "Epoch number:  4 \tLoss: 2.3004151124452736\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3338390303745546\n",
      "Epoch number:  4 \tLoss: 2.3004158313932686\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.333794362110572\n",
      "Epoch number:  4 \tLoss: 2.300411280670763\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3337992584597056\n",
      "Epoch number:  4 \tLoss: 2.300412279424913\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.333777388350661\n",
      "Epoch number:  4 \tLoss: 2.300410068162276\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3338039757312283\n",
      "Epoch number:  4 \tLoss: 2.300412433425888\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.333786997781557\n",
      "Epoch number:  4 \tLoss: 2.3004165709562403\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.333773285348279\n",
      "Epoch number:  4 \tLoss: 2.300415303845572\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3337826504810657\n",
      "Epoch number:  4 \tLoss: 2.300416243326821\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.333810615616121\n",
      "Epoch number:  4 \tLoss: 2.3004170572020235\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3337760948553545\n",
      "Epoch number:  4 \tLoss: 2.300415938609065\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.333783685909739\n",
      "Epoch number:  4 \tLoss: 2.300416283130907\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.333816091604996\n",
      "Epoch number:  4 \tLoss: 2.300413527611161\n",
      "Epoch number:  6 \tLoss: 2.2992959559382977\n",
      "Epoch number:  8 \tLoss: 2.299257935052679\n",
      "Epoch number:  10 \tLoss: 2.299256634645171\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.333733358749243\n",
      "Epoch number:  4 \tLoss: 2.300405648865428\n",
      "Epoch number:  6 \tLoss: 2.2992957646241656\n",
      "Epoch number:  8 \tLoss: 2.2992581968789323\n",
      "Epoch number:  10 \tLoss: 2.2992569277800445\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.333801772314733\n",
      "Epoch number:  4 \tLoss: 2.30041240742682\n",
      "Epoch number:  6 \tLoss: 2.2992962156118035\n",
      "Epoch number:  8 \tLoss: 2.2992582804599033\n",
      "Epoch number:  10 \tLoss: 2.299256983877443\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.333793762150655\n",
      "Epoch number:  4 \tLoss: 2.3004114584443736\n",
      "Epoch number:  6 \tLoss: 2.2992960049400737\n",
      "Epoch number:  8 \tLoss: 2.2992581142989006\n",
      "Epoch number:  10 \tLoss: 2.299256828012834\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3338001074760535\n",
      "Epoch number:  4 \tLoss: 2.300412025179277\n",
      "Epoch number:  6 \tLoss: 2.2992959384132874\n",
      "Epoch number:  8 \tLoss: 2.29925799222832\n",
      "Epoch number:  10 \tLoss: 2.299256693661324\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.333794943436716\n",
      "Epoch number:  4 \tLoss: 2.3004111882425664\n",
      "Epoch number:  6 \tLoss: 2.2992955886585547\n",
      "Epoch number:  8 \tLoss: 2.2992576895551466\n",
      "Epoch number:  10 \tLoss: 2.2992564028081155\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3337970934280423\n",
      "Epoch number:  4 \tLoss: 2.300411203941192\n",
      "Epoch number:  6 \tLoss: 2.2992953579967677\n",
      "Epoch number:  8 \tLoss: 2.2992574378324897\n",
      "Epoch number:  10 \tLoss: 2.2992561503943203\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.333829552226592\n",
      "Epoch number:  4 \tLoss: 2.3004148422449338\n",
      "Epoch number:  6 \tLoss: 2.299296072816761\n",
      "Epoch number:  8 \tLoss: 2.2992579894672245\n",
      "Epoch number:  10 \tLoss: 2.2992566930105434\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3338116581500494\n",
      "Epoch number:  4 \tLoss: 2.300413380390014\n",
      "Epoch number:  6 \tLoss: 2.2992962631914686\n",
      "Epoch number:  8 \tLoss: 2.2992582650530586\n",
      "Epoch number:  10 \tLoss: 2.2992569827265847\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.333776036569926\n",
      "Epoch number:  4 \tLoss: 2.3004100259527434\n",
      "Epoch number:  6 \tLoss: 2.2992962355999014\n",
      "Epoch number:  8 \tLoss: 2.2992584413679213\n",
      "Epoch number:  10 \tLoss: 2.299257160099398\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.333753974383437\n",
      "Epoch number:  4 \tLoss: 2.3004077645020806\n",
      "Epoch number:  6 \tLoss: 2.299295953632542\n",
      "Epoch number:  8 \tLoss: 2.29925826507505\n",
      "Epoch number:  10 \tLoss: 2.2992569806856205\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.333769278749848\n",
      "Epoch number:  4 \tLoss: 2.3004089282811115\n",
      "Epoch number:  6 \tLoss: 2.2992957173665585\n",
      "Epoch number:  8 \tLoss: 2.299257956760486\n",
      "Epoch number:  10 \tLoss: 2.2992566772678624\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.333836919404061\n",
      "Epoch number:  4 \tLoss: 2.300417552992674\n",
      "Epoch number:  6 \tLoss: 2.2992980718851563\n",
      "Epoch number:  8 \tLoss: 2.299259833905037\n",
      "Epoch number:  10 \tLoss: 2.2992584210917437\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3337921856129777\n",
      "Epoch number:  4 \tLoss: 2.300416567404545\n",
      "Epoch number:  6 \tLoss: 2.2993012536425645\n",
      "Epoch number:  8 \tLoss: 2.2992630738703363\n",
      "Epoch number:  10 \tLoss: 2.2992615001349552\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.333798965655016\n",
      "Epoch number:  4 \tLoss: 2.3004167533820405\n",
      "Epoch number:  6 \tLoss: 2.2993007625846777\n",
      "Epoch number:  8 \tLoss: 2.2992625709789567\n",
      "Epoch number:  10 \tLoss: 2.299261015973375\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.333814969257993\n",
      "Epoch number:  4 \tLoss: 2.3004169799644845\n",
      "Epoch number:  6 \tLoss: 2.299299557296191\n",
      "Epoch number:  8 \tLoss: 2.299261354804285\n",
      "Epoch number:  10 \tLoss: 2.2992598711930095\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.333764204610216\n",
      "Epoch number:  4 \tLoss: 2.3004161668124805\n",
      "Epoch number:  6 \tLoss: 2.2993034230845986\n",
      "Epoch number:  8 \tLoss: 2.299265261080746\n",
      "Epoch number:  10 \tLoss: 2.299263569638123\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.333802471509613\n",
      "Epoch number:  4 \tLoss: 2.3004166837203157\n",
      "Epoch number:  6 \tLoss: 2.2993004187034947\n",
      "Epoch number:  8 \tLoss: 2.2992622349124665\n",
      "Epoch number:  10 \tLoss: 2.2992607077827816\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.5024345784911777\n",
      "Epoch number:  4 \tLoss: 2.333593852811949\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.502572927393934\n",
      "Epoch number:  4 \tLoss: 2.3337114091106015\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.502147580419339\n",
      "Epoch number:  4 \tLoss: 2.333352565508367\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.50233979899635\n",
      "Epoch number:  4 \tLoss: 2.333514087362319\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.5024720370229017\n",
      "Epoch number:  4 \tLoss: 2.3336257532039566\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.5024350000876234\n",
      "Epoch number:  4 \tLoss: 2.333594461507935\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.502380521714856\n",
      "Epoch number:  4 \tLoss: 2.333548400436761\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.5022454739511324\n",
      "Epoch number:  4 \tLoss: 2.3334345431941337\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.5023379055033694\n",
      "Epoch number:  4 \tLoss: 2.333512433660512\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.5024342858425315\n",
      "Epoch number:  4 \tLoss: 2.3335937851067734\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.5022901242102042\n",
      "Epoch number:  4 \tLoss: 2.3334721585920364\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.5024532032655857\n",
      "Epoch number:  4 \tLoss: 2.3336098617514733\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.5026174630478875\n",
      "Epoch number:  4 \tLoss: 2.3337514129907078\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.502390664937246\n",
      "Epoch number:  4 \tLoss: 2.333563793274808\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.502311503640412\n",
      "Epoch number:  4 \tLoss: 2.3334986631209023\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.502404956861952\n",
      "Epoch number:  4 \tLoss: 2.3335757335770735\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.5023971948236676\n",
      "Epoch number:  4 \tLoss: 2.333569251447492\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.502399167362944\n",
      "Epoch number:  4 \tLoss: 2.333570730498064\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.5024184317824405\n",
      "Epoch number:  4 \tLoss: 2.33358044236245\n",
      "Epoch number:  6 \tLoss: 2.305438926378696\n",
      "Epoch number:  8 \tLoss: 2.300368750135751\n",
      "Epoch number:  10 \tLoss: 2.2994434116909073\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.5023756968077846\n",
      "Epoch number:  4 \tLoss: 2.3335441072046366\n",
      "Epoch number:  6 \tLoss: 2.3054247422557\n",
      "Epoch number:  8 \tLoss: 2.3003643968420886\n",
      "Epoch number:  10 \tLoss: 2.299442023439074\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.50223318164485\n",
      "Epoch number:  4 \tLoss: 2.3334246195826336\n",
      "Epoch number:  6 \tLoss: 2.3053803146264085\n",
      "Epoch number:  8 \tLoss: 2.30035299971962\n",
      "Epoch number:  10 \tLoss: 2.2994404890218854\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.5024301138253913\n",
      "Epoch number:  4 \tLoss: 2.333590234674597\n",
      "Epoch number:  6 \tLoss: 2.3054425355325794\n",
      "Epoch number:  8 \tLoss: 2.300369647958292\n",
      "Epoch number:  10 \tLoss: 2.2994435083555214\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.502360478067155\n",
      "Epoch number:  4 \tLoss: 2.333531628712583\n",
      "Epoch number:  6 \tLoss: 2.305420541914425\n",
      "Epoch number:  8 \tLoss: 2.300363824765849\n",
      "Epoch number:  10 \tLoss: 2.2994425295471586\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.502436160714883\n",
      "Epoch number:  4 \tLoss: 2.3335954059937434\n",
      "Epoch number:  6 \tLoss: 2.3054446125690653\n",
      "Epoch number:  8 \tLoss: 2.3003703528743085\n",
      "Epoch number:  10 \tLoss: 2.2994438008255105\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.5026346535865276\n",
      "Epoch number:  4 \tLoss: 2.333763770988737\n",
      "Epoch number:  6 \tLoss: 2.3055090059878083\n",
      "Epoch number:  8 \tLoss: 2.3003884174602747\n",
      "Epoch number:  10 \tLoss: 2.2994478702681693\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.502608572182015\n",
      "Epoch number:  4 \tLoss: 2.3337419118064298\n",
      "Epoch number:  6 \tLoss: 2.305501133513344\n",
      "Epoch number:  8 \tLoss: 2.300386839520912\n",
      "Epoch number:  10 \tLoss: 2.2994482125238513\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.502471338696134\n",
      "Epoch number:  4 \tLoss: 2.3336248157751007\n",
      "Epoch number:  6 \tLoss: 2.3054552930285865\n",
      "Epoch number:  8 \tLoss: 2.3003727369127254\n",
      "Epoch number:  10 \tLoss: 2.299443681477539\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.5022801738316685\n",
      "Epoch number:  4 \tLoss: 2.333463807758911\n",
      "Epoch number:  6 \tLoss: 2.305394642307811\n",
      "Epoch number:  8 \tLoss: 2.3003564234523464\n",
      "Epoch number:  10 \tLoss: 2.299440674166622\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.502362451270294\n",
      "Epoch number:  4 \tLoss: 2.333533154466501\n",
      "Epoch number:  6 \tLoss: 2.305420906215603\n",
      "Epoch number:  8 \tLoss: 2.3003636774289147\n",
      "Epoch number:  10 \tLoss: 2.2994422246113757\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.5023976085008957\n",
      "Epoch number:  4 \tLoss: 2.3335627618129138\n",
      "Epoch number:  6 \tLoss: 2.305432112361558\n",
      "Epoch number:  8 \tLoss: 2.3003667728406763\n",
      "Epoch number:  10 \tLoss: 2.299442900524952\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.5021370029029772\n",
      "Epoch number:  4 \tLoss: 2.33335517354243\n",
      "Epoch number:  6 \tLoss: 2.3053647950329963\n",
      "Epoch number:  8 \tLoss: 2.3003592977039493\n",
      "Epoch number:  10 \tLoss: 2.299452806703353\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.5025201642559627\n",
      "Epoch number:  4 \tLoss: 2.3336707610442486\n",
      "Epoch number:  6 \tLoss: 2.3054775703730233\n",
      "Epoch number:  8 \tLoss: 2.3003838290837857\n",
      "Epoch number:  10 \tLoss: 2.299451230628525\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.502392548968409\n",
      "Epoch number:  4 \tLoss: 2.333565425163441\n",
      "Epoch number:  6 \tLoss: 2.305439988727886\n",
      "Epoch number:  8 \tLoss: 2.300375869741431\n",
      "Epoch number:  10 \tLoss: 2.2994520526398703\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.5024324347114724\n",
      "Epoch number:  4 \tLoss: 2.333598313936242\n",
      "Epoch number:  6 \tLoss: 2.305451763934136\n",
      "Epoch number:  8 \tLoss: 2.3003784372784732\n",
      "Epoch number:  10 \tLoss: 2.299451904681208\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.5022515469214848\n",
      "Epoch number:  4 \tLoss: 2.33344934763891\n",
      "Epoch number:  6 \tLoss: 2.3053985796561105\n",
      "Epoch number:  8 \tLoss: 2.3003669273566576\n",
      "Epoch number:  10 \tLoss: 2.299452718756737\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.502295890430578\n",
      "Epoch number:  4 \tLoss: 2.333485860621763\n",
      "Epoch number:  6 \tLoss: 2.3054117685422604\n",
      "Epoch number:  8 \tLoss: 2.3003700120396573\n",
      "Epoch number:  10 \tLoss: 2.299452825632191\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010514173287206\n",
      "Epoch number:  4 \tLoss: 2.301051279421745\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010519866604926\n",
      "Epoch number:  4 \tLoss: 2.301051847042733\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301052875118466\n",
      "Epoch number:  4 \tLoss: 2.301052494656196\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301054113743127\n",
      "Epoch number:  4 \tLoss: 2.301053831829028\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010520869405946\n",
      "Epoch number:  4 \tLoss: 2.301051775780997\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010531010807225\n",
      "Epoch number:  4 \tLoss: 2.3010528147515212\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301052192818758\n",
      "Epoch number:  4 \tLoss: 2.3010517392508323\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010513828637738\n",
      "Epoch number:  4 \tLoss: 2.3010513360209472\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010515311877473\n",
      "Epoch number:  4 \tLoss: 2.30105118475084\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301051072830879\n",
      "Epoch number:  4 \tLoss: 2.3010509096723317\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010529045490227\n",
      "Epoch number:  4 \tLoss: 2.3010527548221487\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301052039547171\n",
      "Epoch number:  4 \tLoss: 2.301051812625087\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010515558961067\n",
      "Epoch number:  4 \tLoss: 2.3010513385785516\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010520650490234\n",
      "Epoch number:  4 \tLoss: 2.3010514403288784\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301052171168972\n",
      "Epoch number:  4 \tLoss: 2.301051460041844\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.30105199798352\n",
      "Epoch number:  4 \tLoss: 2.3010514244787688\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301052060479759\n",
      "Epoch number:  4 \tLoss: 2.3010514388010375\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301051958941781\n",
      "Epoch number:  4 \tLoss: 2.3010514184861868\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010513690713115\n",
      "Epoch number:  4 \tLoss: 2.3010511692844435\n",
      "Epoch number:  6 \tLoss: 2.3010510029217075\n",
      "Epoch number:  8 \tLoss: 2.3010508104361023\n",
      "Epoch number:  10 \tLoss: 2.301050457102458\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301052039275189\n",
      "Epoch number:  4 \tLoss: 2.3010518552671715\n",
      "Epoch number:  6 \tLoss: 2.3010517358182803\n",
      "Epoch number:  8 \tLoss: 2.301051656312099\n",
      "Epoch number:  10 \tLoss: 2.3010516020988816\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010527314603095\n",
      "Epoch number:  4 \tLoss: 2.3010525563923925\n",
      "Epoch number:  6 \tLoss: 2.301052385018963\n",
      "Epoch number:  8 \tLoss: 2.3010521973417095\n",
      "Epoch number:  10 \tLoss: 2.3010518459913145\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301049783576623\n",
      "Epoch number:  4 \tLoss: 2.301049675696457\n",
      "Epoch number:  6 \tLoss: 2.3010496292207208\n",
      "Epoch number:  8 \tLoss: 2.3010496201577535\n",
      "Epoch number:  10 \tLoss: 2.3010498135235244\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010529091803544\n",
      "Epoch number:  4 \tLoss: 2.3010526367479978\n",
      "Epoch number:  6 \tLoss: 2.3010523638237883\n",
      "Epoch number:  8 \tLoss: 2.301052027825401\n",
      "Epoch number:  10 \tLoss: 2.301050619933418\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301052731458129\n",
      "Epoch number:  4 \tLoss: 2.3010524913452963\n",
      "Epoch number:  6 \tLoss: 2.301052326725366\n",
      "Epoch number:  8 \tLoss: 2.3010522087786125\n",
      "Epoch number:  10 \tLoss: 2.3010520803945798\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010521892835856\n",
      "Epoch number:  4 \tLoss: 2.30105189483024\n",
      "Epoch number:  6 \tLoss: 2.301051686494038\n",
      "Epoch number:  8 \tLoss: 2.3010514121685217\n",
      "Epoch number:  10 \tLoss: 2.3010508218878396\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010506895825658\n",
      "Epoch number:  4 \tLoss: 2.301050613513387\n",
      "Epoch number:  6 \tLoss: 2.3010506291848274\n",
      "Epoch number:  8 \tLoss: 2.301050620140311\n",
      "Epoch number:  10 \tLoss: 2.301050626628501\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010511824078015\n",
      "Epoch number:  4 \tLoss: 2.3010508703996004\n",
      "Epoch number:  6 \tLoss: 2.301050580563927\n",
      "Epoch number:  8 \tLoss: 2.3010501150904554\n",
      "Epoch number:  10 \tLoss: 2.301048283810043\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010522553133033\n",
      "Epoch number:  4 \tLoss: 2.301051966550474\n",
      "Epoch number:  6 \tLoss: 2.301051767926551\n",
      "Epoch number:  8 \tLoss: 2.301051639881221\n",
      "Epoch number:  10 \tLoss: 2.301051638087019\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010525968962203\n",
      "Epoch number:  4 \tLoss: 2.30105236479125\n",
      "Epoch number:  6 \tLoss: 2.301052164756632\n",
      "Epoch number:  8 \tLoss: 2.3010518700910674\n",
      "Epoch number:  10 \tLoss: 2.301050976594278\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010520698218087\n",
      "Epoch number:  4 \tLoss: 2.3010518540580747\n",
      "Epoch number:  6 \tLoss: 2.301051689761122\n",
      "Epoch number:  8 \tLoss: 2.301051623948059\n",
      "Epoch number:  10 \tLoss: 2.3010514352418467\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301051831033339\n",
      "Epoch number:  4 \tLoss: 2.301051394684261\n",
      "Epoch number:  6 \tLoss: 2.301051328292831\n",
      "Epoch number:  8 \tLoss: 2.301051307452184\n",
      "Epoch number:  10 \tLoss: 2.3010512974688067\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010518078182995\n",
      "Epoch number:  4 \tLoss: 2.301051393872691\n",
      "Epoch number:  6 \tLoss: 2.3010513280812077\n",
      "Epoch number:  8 \tLoss: 2.301051307345884\n",
      "Epoch number:  10 \tLoss: 2.3010512974115915\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301051963113538\n",
      "Epoch number:  4 \tLoss: 2.301051418140951\n",
      "Epoch number:  6 \tLoss: 2.3010513373064345\n",
      "Epoch number:  8 \tLoss: 2.3010513120331053\n",
      "Epoch number:  10 \tLoss: 2.3010512999338757\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010515682108768\n",
      "Epoch number:  4 \tLoss: 2.3010513390723246\n",
      "Epoch number:  6 \tLoss: 2.3010513064209475\n",
      "Epoch number:  8 \tLoss: 2.3010512963017984\n",
      "Epoch number:  10 \tLoss: 2.3010512914658556\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301052370584012\n",
      "Epoch number:  4 \tLoss: 2.301051498567753\n",
      "Epoch number:  6 \tLoss: 2.3010513686080776\n",
      "Epoch number:  8 \tLoss: 2.3010513279650393\n",
      "Epoch number:  10 \tLoss: 2.3010513085088924\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010518620963487\n",
      "Epoch number:  4 \tLoss: 2.3010513989850425\n",
      "Epoch number:  6 \tLoss: 2.3010513298970587\n",
      "Epoch number:  8 \tLoss: 2.3010513082655537\n",
      "Epoch number:  10 \tLoss: 2.301051297906394\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300060215232353\n",
      "Epoch number:  4 \tLoss: 2.3000600677642002\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3000637141055793\n",
      "Epoch number:  4 \tLoss: 2.3000634207467447\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3000615636637107\n",
      "Epoch number:  4 \tLoss: 2.300061316023364\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3000606988497236\n",
      "Epoch number:  4 \tLoss: 2.3000605600095057\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3000619669527365\n",
      "Epoch number:  4 \tLoss: 2.3000617737113807\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300062336377342\n",
      "Epoch number:  4 \tLoss: 2.3000622189060653\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300059832222768\n",
      "Epoch number:  4 \tLoss: 2.3000596976605765\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3000609215487713\n",
      "Epoch number:  4 \tLoss: 2.300060776671948\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300063423849302\n",
      "Epoch number:  4 \tLoss: 2.300063226814784\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3000617530483303\n",
      "Epoch number:  4 \tLoss: 2.30006167592633\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3000616378770258\n",
      "Epoch number:  4 \tLoss: 2.3000614277745766\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3000612237831874\n",
      "Epoch number:  4 \tLoss: 2.3000611393498493\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300062535964111\n",
      "Epoch number:  4 \tLoss: 2.300061717998259\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3000640856567744\n",
      "Epoch number:  4 \tLoss: 2.3000624882061387\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3000636506671652\n",
      "Epoch number:  4 \tLoss: 2.300062234023151\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3000624527343176\n",
      "Epoch number:  4 \tLoss: 2.300061638490789\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3000645548836545\n",
      "Epoch number:  4 \tLoss: 2.3000627362839197\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3000652414442224\n",
      "Epoch number:  4 \tLoss: 2.3000631199555412\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300062524816615\n",
      "Epoch number:  4 \tLoss: 2.300062313070654\n",
      "Epoch number:  6 \tLoss: 2.3000623309650683\n",
      "Epoch number:  8 \tLoss: 2.300062088493289\n",
      "Epoch number:  10 \tLoss: 2.3000617545292443\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300061467779409\n",
      "Epoch number:  4 \tLoss: 2.300061420470591\n",
      "Epoch number:  6 \tLoss: 2.3000613824727827\n",
      "Epoch number:  8 \tLoss: 2.300061351387589\n",
      "Epoch number:  10 \tLoss: 2.300061325454426\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300062355901237\n",
      "Epoch number:  4 \tLoss: 2.3000622044740777\n",
      "Epoch number:  6 \tLoss: 2.300062069061379\n",
      "Epoch number:  8 \tLoss: 2.3000619231068185\n",
      "Epoch number:  10 \tLoss: 2.300061630129241\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300061889316801\n",
      "Epoch number:  4 \tLoss: 2.300061671705644\n",
      "Epoch number:  6 \tLoss: 2.300061492680759\n",
      "Epoch number:  8 \tLoss: 2.3000613425949377\n",
      "Epoch number:  10 \tLoss: 2.300061214352361\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300060952650969\n",
      "Epoch number:  4 \tLoss: 2.3000607767275114\n",
      "Epoch number:  6 \tLoss: 2.3000605727099286\n",
      "Epoch number:  8 \tLoss: 2.3000602006221578\n",
      "Epoch number:  10 \tLoss: 2.300058910002473\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300061386415544\n",
      "Epoch number:  4 \tLoss: 2.300061286512559\n",
      "Epoch number:  6 \tLoss: 2.3000612179369675\n",
      "Epoch number:  8 \tLoss: 2.3000611658146117\n",
      "Epoch number:  10 \tLoss: 2.300061131746527\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300062233603738\n",
      "Epoch number:  4 \tLoss: 2.300062144151577\n",
      "Epoch number:  6 \tLoss: 2.300062040076734\n",
      "Epoch number:  8 \tLoss: 2.3000618857677506\n",
      "Epoch number:  10 \tLoss: 2.3000615771658914\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3000609310814646\n",
      "Epoch number:  4 \tLoss: 2.3000608425803453\n",
      "Epoch number:  6 \tLoss: 2.300060776297851\n",
      "Epoch number:  8 \tLoss: 2.300060726325172\n",
      "Epoch number:  10 \tLoss: 2.3000606883270294\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3000613597028825\n",
      "Epoch number:  4 \tLoss: 2.3000612681457824\n",
      "Epoch number:  6 \tLoss: 2.3000611844706373\n",
      "Epoch number:  8 \tLoss: 2.3000610748385633\n",
      "Epoch number:  10 \tLoss: 2.3000608345263367\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300061896964637\n",
      "Epoch number:  4 \tLoss: 2.3000617464154085\n",
      "Epoch number:  6 \tLoss: 2.3000616198122814\n",
      "Epoch number:  8 \tLoss: 2.3000615112523737\n",
      "Epoch number:  10 \tLoss: 2.3000614163874875\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300061666023877\n",
      "Epoch number:  4 \tLoss: 2.3000615384033845\n",
      "Epoch number:  6 \tLoss: 2.300061369682087\n",
      "Epoch number:  8 \tLoss: 2.300061037820206\n",
      "Epoch number:  10 \tLoss: 2.3000592355670233\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3000630708778056\n",
      "Epoch number:  4 \tLoss: 2.3000628949600954\n",
      "Epoch number:  6 \tLoss: 2.3000627552719553\n",
      "Epoch number:  8 \tLoss: 2.3000626423314126\n",
      "Epoch number:  10 \tLoss: 2.3000625491874147\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300062309155093\n",
      "Epoch number:  4 \tLoss: 2.3000614594874165\n",
      "Epoch number:  6 \tLoss: 2.300061050815581\n",
      "Epoch number:  8 \tLoss: 2.300060848383453\n",
      "Epoch number:  10 \tLoss: 2.30006074510334\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300062258746048\n",
      "Epoch number:  4 \tLoss: 2.300061625469058\n",
      "Epoch number:  6 \tLoss: 2.300061208210705\n",
      "Epoch number:  8 \tLoss: 2.3000609590435266\n",
      "Epoch number:  10 \tLoss: 2.3000608161286586\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3000647611645286\n",
      "Epoch number:  4 \tLoss: 2.3000628672731507\n",
      "Epoch number:  6 \tLoss: 2.3000618475228682\n",
      "Epoch number:  8 \tLoss: 2.3000613001639727\n",
      "Epoch number:  10 \tLoss: 2.3000610048827044\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3000646876545825\n",
      "Epoch number:  4 \tLoss: 2.3000627271083465\n",
      "Epoch number:  6 \tLoss: 2.3000617359668984\n",
      "Epoch number:  8 \tLoss: 2.3000612258444804\n",
      "Epoch number:  10 \tLoss: 2.3000609582245497\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300063898579191\n",
      "Epoch number:  4 \tLoss: 2.3000623182391635\n",
      "Epoch number:  6 \tLoss: 2.300061518694299\n",
      "Epoch number:  8 \tLoss: 2.3000611073427737\n",
      "Epoch number:  10 \tLoss: 2.300060891709121\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3000647059304304\n",
      "Epoch number:  4 \tLoss: 2.3000627543616337\n",
      "Epoch number:  6 \tLoss: 2.3000617561032213\n",
      "Epoch number:  8 \tLoss: 2.300061238572462\n",
      "Epoch number:  10 \tLoss: 2.3000609658821625\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2995733687934146\n",
      "Epoch number:  4 \tLoss: 2.299573133741867\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2995715935796555\n",
      "Epoch number:  4 \tLoss: 2.2995713563796816\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2995742168510795\n",
      "Epoch number:  4 \tLoss: 2.2995738084198125\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299573582908306\n",
      "Epoch number:  4 \tLoss: 2.2995733430900756\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.29957358143917\n",
      "Epoch number:  4 \tLoss: 2.2995732521623853\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2995746445849257\n",
      "Epoch number:  4 \tLoss: 2.299574321408398\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299573376577713\n",
      "Epoch number:  4 \tLoss: 2.2995731039686813\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299571447140973\n",
      "Epoch number:  4 \tLoss: 2.299571361654218\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2995737416961806\n",
      "Epoch number:  4 \tLoss: 2.299573439361602\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2995727478977743\n",
      "Epoch number:  4 \tLoss: 2.2995725680914605\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2995736058650773\n",
      "Epoch number:  4 \tLoss: 2.2995731468889913\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2995742791606997\n",
      "Epoch number:  4 \tLoss: 2.299574014230363\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2995754462261453\n",
      "Epoch number:  4 \tLoss: 2.2995746474668812\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299580412859178\n",
      "Epoch number:  4 \tLoss: 2.2995786936572484\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2995783222524273\n",
      "Epoch number:  4 \tLoss: 2.299576989483094\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299578595857729\n",
      "Epoch number:  4 \tLoss: 2.2995773026845687\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299577322382009\n",
      "Epoch number:  4 \tLoss: 2.299576170452068\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2995785122628525\n",
      "Epoch number:  4 \tLoss: 2.2995771569191494\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2995721613803513\n",
      "Epoch number:  4 \tLoss: 2.2995719702391644\n",
      "Epoch number:  6 \tLoss: 2.2995718069384723\n",
      "Epoch number:  8 \tLoss: 2.2995716689246106\n",
      "Epoch number:  10 \tLoss: 2.2995715392325096\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299572773841007\n",
      "Epoch number:  4 \tLoss: 2.2995725950025148\n",
      "Epoch number:  6 \tLoss: 2.299572440170118\n",
      "Epoch number:  8 \tLoss: 2.2995723005414432\n",
      "Epoch number:  10 \tLoss: 2.299572174294786\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299573369256742\n",
      "Epoch number:  4 \tLoss: 2.299573062454292\n",
      "Epoch number:  6 \tLoss: 2.299572684928945\n",
      "Epoch number:  8 \tLoss: 2.2995721601078536\n",
      "Epoch number:  10 \tLoss: 2.299570956156529\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299572992875618\n",
      "Epoch number:  4 \tLoss: 2.299572802199934\n",
      "Epoch number:  6 \tLoss: 2.2995726369168126\n",
      "Epoch number:  8 \tLoss: 2.299572488078235\n",
      "Epoch number:  10 \tLoss: 2.299572353702639\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.29957552433231\n",
      "Epoch number:  4 \tLoss: 2.2995750727866837\n",
      "Epoch number:  6 \tLoss: 2.299574555164867\n",
      "Epoch number:  8 \tLoss: 2.2995739540227613\n",
      "Epoch number:  10 \tLoss: 2.299572873804688\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.29957430710987\n",
      "Epoch number:  4 \tLoss: 2.29957396349592\n",
      "Epoch number:  6 \tLoss: 2.299573661212737\n",
      "Epoch number:  8 \tLoss: 2.2995733896580934\n",
      "Epoch number:  10 \tLoss: 2.2995731451067885\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2995734595964934\n",
      "Epoch number:  4 \tLoss: 2.2995730359969735\n",
      "Epoch number:  6 \tLoss: 2.299572590762728\n",
      "Epoch number:  8 \tLoss: 2.2995720628607748\n",
      "Epoch number:  10 \tLoss: 2.299570888091162\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2995755874408412\n",
      "Epoch number:  4 \tLoss: 2.2995754376504434\n",
      "Epoch number:  6 \tLoss: 2.299575305589802\n",
      "Epoch number:  8 \tLoss: 2.299575183088128\n",
      "Epoch number:  10 \tLoss: 2.2995750690011967\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299572304471146\n",
      "Epoch number:  4 \tLoss: 2.299572088403636\n",
      "Epoch number:  6 \tLoss: 2.299571837853046\n",
      "Epoch number:  8 \tLoss: 2.29957143573571\n",
      "Epoch number:  10 \tLoss: 2.2995704769055596\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299573040897409\n",
      "Epoch number:  4 \tLoss: 2.2995728532270583\n",
      "Epoch number:  6 \tLoss: 2.2995726901250175\n",
      "Epoch number:  8 \tLoss: 2.299572542733016\n",
      "Epoch number:  10 \tLoss: 2.2995724091703575\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2995729586244282\n",
      "Epoch number:  4 \tLoss: 2.2995726178500937\n",
      "Epoch number:  6 \tLoss: 2.2995723122393468\n",
      "Epoch number:  8 \tLoss: 2.299571915117764\n",
      "Epoch number:  10 \tLoss: 2.2995711132055807\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2995757885635246\n",
      "Epoch number:  4 \tLoss: 2.2995753970434225\n",
      "Epoch number:  6 \tLoss: 2.299575045678307\n",
      "Epoch number:  8 \tLoss: 2.299574724139134\n",
      "Epoch number:  10 \tLoss: 2.299574428966005\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2995769849506105\n",
      "Epoch number:  4 \tLoss: 2.2995758807028035\n",
      "Epoch number:  6 \tLoss: 2.299574990783304\n",
      "Epoch number:  8 \tLoss: 2.299574266366129\n",
      "Epoch number:  10 \tLoss: 2.299573675548396\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2995804196612766\n",
      "Epoch number:  4 \tLoss: 2.2995786217612704\n",
      "Epoch number:  6 \tLoss: 2.2995771748876472\n",
      "Epoch number:  8 \tLoss: 2.299576005833572\n",
      "Epoch number:  10 \tLoss: 2.299575060884313\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2995829994978223\n",
      "Epoch number:  4 \tLoss: 2.29958081881744\n",
      "Epoch number:  6 \tLoss: 2.29957903776719\n",
      "Epoch number:  8 \tLoss: 2.29957757923884\n",
      "Epoch number:  10 \tLoss: 2.2995763865558696\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299581264181724\n",
      "Epoch number:  4 \tLoss: 2.2995793722905895\n",
      "Epoch number:  6 \tLoss: 2.2995778330133136\n",
      "Epoch number:  8 \tLoss: 2.2995765769563294\n",
      "Epoch number:  10 \tLoss: 2.2995755523928385\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2995773577511263\n",
      "Epoch number:  4 \tLoss: 2.2995762803037554\n",
      "Epoch number:  6 \tLoss: 2.29957538417911\n",
      "Epoch number:  8 \tLoss: 2.2995746370325163\n",
      "Epoch number:  10 \tLoss: 2.299574015384809\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2995805146319586\n",
      "Epoch number:  4 \tLoss: 2.2995788351647883\n",
      "Epoch number:  6 \tLoss: 2.299577451501299\n",
      "Epoch number:  8 \tLoss: 2.2995763089040193\n",
      "Epoch number:  10 \tLoss: 2.2995753666077623\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010348456684273\n",
      "Epoch number:  4 \tLoss: 2.3010346249181057\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010354364760124\n",
      "Epoch number:  4 \tLoss: 2.301035437639109\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301035923582192\n",
      "Epoch number:  4 \tLoss: 2.3010357879211187\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010362570532696\n",
      "Epoch number:  4 \tLoss: 2.3010361237887333\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010375371104432\n",
      "Epoch number:  4 \tLoss: 2.301037304207727\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301036227155962\n",
      "Epoch number:  4 \tLoss: 2.3010360069271996\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301035766348649\n",
      "Epoch number:  4 \tLoss: 2.301035630398178\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010355747932207\n",
      "Epoch number:  4 \tLoss: 2.301035479171552\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010356435317143\n",
      "Epoch number:  4 \tLoss: 2.3010354457184845\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010356812944446\n",
      "Epoch number:  4 \tLoss: 2.301035537488317\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301037369283999\n",
      "Epoch number:  4 \tLoss: 2.301037011133046\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301036300006821\n",
      "Epoch number:  4 \tLoss: 2.301036191205808\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301036900823968\n",
      "Epoch number:  4 \tLoss: 2.301036315461835\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010370035601384\n",
      "Epoch number:  4 \tLoss: 2.30103633440561\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010368768183085\n",
      "Epoch number:  4 \tLoss: 2.3010363098339717\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301036496170467\n",
      "Epoch number:  4 \tLoss: 2.3010362317436166\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301036856847336\n",
      "Epoch number:  4 \tLoss: 2.301036305899385\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301036965819353\n",
      "Epoch number:  4 \tLoss: 2.3010363226286406\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301038434498182\n",
      "Epoch number:  4 \tLoss: 2.3010382528441493\n",
      "Epoch number:  6 \tLoss: 2.301038125483241\n",
      "Epoch number:  8 \tLoss: 2.301038018042153\n",
      "Epoch number:  10 \tLoss: 2.3010378537346017\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301038260645642\n",
      "Epoch number:  4 \tLoss: 2.3010380839863838\n",
      "Epoch number:  6 \tLoss: 2.3010379585653538\n",
      "Epoch number:  8 \tLoss: 2.3010378273593406\n",
      "Epoch number:  10 \tLoss: 2.3010375493476642\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301036558280854\n",
      "Epoch number:  4 \tLoss: 2.301036455140742\n",
      "Epoch number:  6 \tLoss: 2.301036271182275\n",
      "Epoch number:  8 \tLoss: 2.3010360572200286\n",
      "Epoch number:  10 \tLoss: 2.3010348213635914\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301038120825582\n",
      "Epoch number:  4 \tLoss: 2.3010379385979145\n",
      "Epoch number:  6 \tLoss: 2.3010379244883805\n",
      "Epoch number:  8 \tLoss: 2.301037823979246\n",
      "Epoch number:  10 \tLoss: 2.301037744296169\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301037532548558\n",
      "Epoch number:  4 \tLoss: 2.3010372315036873\n",
      "Epoch number:  6 \tLoss: 2.301036938212156\n",
      "Epoch number:  8 \tLoss: 2.301036606752624\n",
      "Epoch number:  10 \tLoss: 2.3010356242338217\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010371771084994\n",
      "Epoch number:  4 \tLoss: 2.3010369448954435\n",
      "Epoch number:  6 \tLoss: 2.301036753785276\n",
      "Epoch number:  8 \tLoss: 2.3010366510652758\n",
      "Epoch number:  10 \tLoss: 2.3010365473412095\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010363753832217\n",
      "Epoch number:  4 \tLoss: 2.3010360154417766\n",
      "Epoch number:  6 \tLoss: 2.301035659781225\n",
      "Epoch number:  8 \tLoss: 2.301035210715779\n",
      "Epoch number:  10 \tLoss: 2.3010342649006392\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301038223040904\n",
      "Epoch number:  4 \tLoss: 2.301037877967593\n",
      "Epoch number:  6 \tLoss: 2.3010376315409817\n",
      "Epoch number:  8 \tLoss: 2.301037447958393\n",
      "Epoch number:  10 \tLoss: 2.301037227183246\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010368932816916\n",
      "Epoch number:  4 \tLoss: 2.30103672052394\n",
      "Epoch number:  6 \tLoss: 2.301036588256499\n",
      "Epoch number:  8 \tLoss: 2.3010365207078505\n",
      "Epoch number:  10 \tLoss: 2.3010363788270345\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010360131045666\n",
      "Epoch number:  4 \tLoss: 2.3010358815803085\n",
      "Epoch number:  6 \tLoss: 2.301035796877222\n",
      "Epoch number:  8 \tLoss: 2.301035751986142\n",
      "Epoch number:  10 \tLoss: 2.3010357155675045\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301037419187256\n",
      "Epoch number:  4 \tLoss: 2.30103720296272\n",
      "Epoch number:  6 \tLoss: 2.3010370021974302\n",
      "Epoch number:  8 \tLoss: 2.301036625926371\n",
      "Epoch number:  10 \tLoss: 2.3010358760448977\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301036769688974\n",
      "Epoch number:  4 \tLoss: 2.301036514852882\n",
      "Epoch number:  6 \tLoss: 2.3010363585227283\n",
      "Epoch number:  8 \tLoss: 2.301036238199945\n",
      "Epoch number:  10 \tLoss: 2.301036121141744\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010368400088135\n",
      "Epoch number:  4 \tLoss: 2.3010363004237973\n",
      "Epoch number:  6 \tLoss: 2.3010362189070737\n",
      "Epoch number:  8 \tLoss: 2.3010361935810795\n",
      "Epoch number:  10 \tLoss: 2.3010361814426186\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010366920670107\n",
      "Epoch number:  4 \tLoss: 2.301036274637985\n",
      "Epoch number:  6 \tLoss: 2.301036209300993\n",
      "Epoch number:  8 \tLoss: 2.301036188718694\n",
      "Epoch number:  10 \tLoss: 2.3010361788160414\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010371408737096\n",
      "Epoch number:  4 \tLoss: 2.3010363645866643\n",
      "Epoch number:  6 \tLoss: 2.3010362444098975\n",
      "Epoch number:  8 \tLoss: 2.3010362066732992\n",
      "Epoch number:  10 \tLoss: 2.301036188531889\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010370572662584\n",
      "Epoch number:  4 \tLoss: 2.3010363411911063\n",
      "Epoch number:  6 \tLoss: 2.301036235060733\n",
      "Epoch number:  8 \tLoss: 2.301036201885209\n",
      "Epoch number:  10 \tLoss: 2.3010361859405615\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301036882651136\n",
      "Epoch number:  4 \tLoss: 2.3010363039249655\n",
      "Epoch number:  6 \tLoss: 2.3010362202503156\n",
      "Epoch number:  8 \tLoss: 2.3010361942826947\n",
      "Epoch number:  10 \tLoss: 2.3010361818239753\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010367977661232\n",
      "Epoch number:  4 \tLoss: 2.301036286415594\n",
      "Epoch number:  6 \tLoss: 2.3010362134897537\n",
      "Epoch number:  8 \tLoss: 2.30103619083713\n",
      "Epoch number:  10 \tLoss: 2.3010361799606107\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300053940721172\n",
      "Epoch number:  4 \tLoss: 2.300053736663306\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3000534112938342\n",
      "Epoch number:  4 \tLoss: 2.3000533759321264\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300056185872791\n",
      "Epoch number:  4 \tLoss: 2.3000559642525586\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300054981627377\n",
      "Epoch number:  4 \tLoss: 2.3000548472637097\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300054755574311\n",
      "Epoch number:  4 \tLoss: 2.3000546155902746\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300055322411322\n",
      "Epoch number:  4 \tLoss: 2.300055216621129\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3000567900367423\n",
      "Epoch number:  4 \tLoss: 2.3000565150752252\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300055758115008\n",
      "Epoch number:  4 \tLoss: 2.3000556744217615\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300057249157931\n",
      "Epoch number:  4 \tLoss: 2.3000570468968276\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300055281166471\n",
      "Epoch number:  4 \tLoss: 2.300055208912859\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300055674375047\n",
      "Epoch number:  4 \tLoss: 2.3000554450896864\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3000546395071613\n",
      "Epoch number:  4 \tLoss: 2.3000545086153816\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3000566749011617\n",
      "Epoch number:  4 \tLoss: 2.30005541064966\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3000589099343514\n",
      "Epoch number:  4 \tLoss: 2.300056484590699\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3000579703679715\n",
      "Epoch number:  4 \tLoss: 2.3000561508183552\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300055442467088\n",
      "Epoch number:  4 \tLoss: 2.3000545746779664\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3000581423563404\n",
      "Epoch number:  4 \tLoss: 2.30005610161948\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300056882670695\n",
      "Epoch number:  4 \tLoss: 2.3000553956052503\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300056111867651\n",
      "Epoch number:  4 \tLoss: 2.300056048812101\n",
      "Epoch number:  6 \tLoss: 2.300056007402835\n",
      "Epoch number:  8 \tLoss: 2.3000560106803527\n",
      "Epoch number:  10 \tLoss: 2.3000558967942517\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300054016746315\n",
      "Epoch number:  4 \tLoss: 2.3000539774528685\n",
      "Epoch number:  6 \tLoss: 2.3000539479214086\n",
      "Epoch number:  8 \tLoss: 2.300053925652489\n",
      "Epoch number:  10 \tLoss: 2.3000539087900043\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3000540599147756\n",
      "Epoch number:  4 \tLoss: 2.300053884740638\n",
      "Epoch number:  6 \tLoss: 2.3000537163743413\n",
      "Epoch number:  8 \tLoss: 2.3000535046077664\n",
      "Epoch number:  10 \tLoss: 2.300053075813959\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300055011432427\n",
      "Epoch number:  4 \tLoss: 2.3000549496546117\n",
      "Epoch number:  6 \tLoss: 2.3000549031931548\n",
      "Epoch number:  8 \tLoss: 2.300054867814789\n",
      "Epoch number:  10 \tLoss: 2.300054840413131\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3000561812618607\n",
      "Epoch number:  4 \tLoss: 2.300055984974761\n",
      "Epoch number:  6 \tLoss: 2.300055804832741\n",
      "Epoch number:  8 \tLoss: 2.3000556325526404\n",
      "Epoch number:  10 \tLoss: 2.3000548532286365\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3000556286401115\n",
      "Epoch number:  4 \tLoss: 2.300055483997421\n",
      "Epoch number:  6 \tLoss: 2.300055368672172\n",
      "Epoch number:  8 \tLoss: 2.300055275088556\n",
      "Epoch number:  10 \tLoss: 2.300055197695141\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3000535180158552\n",
      "Epoch number:  4 \tLoss: 2.300053329311071\n",
      "Epoch number:  6 \tLoss: 2.300052909592672\n",
      "Epoch number:  8 \tLoss: 2.300050715194801\n",
      "Epoch number:  10 \tLoss: 1.6086128661409491\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300054227503202\n",
      "Epoch number:  4 \tLoss: 2.300054170406401\n",
      "Epoch number:  6 \tLoss: 2.300054128785236\n",
      "Epoch number:  8 \tLoss: 2.3000540993086633\n",
      "Epoch number:  10 \tLoss: 2.300054077672608\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3000537194209865\n",
      "Epoch number:  4 \tLoss: 2.300053587996353\n",
      "Epoch number:  6 \tLoss: 2.300053445152654\n",
      "Epoch number:  8 \tLoss: 2.300053335424205\n",
      "Epoch number:  10 \tLoss: 2.300052697973478\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3000543002044096\n",
      "Epoch number:  4 \tLoss: 2.300054195423124\n",
      "Epoch number:  6 \tLoss: 2.3000540815445203\n",
      "Epoch number:  8 \tLoss: 2.300053987587757\n",
      "Epoch number:  10 \tLoss: 2.3000539086131533\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3000543856695694\n",
      "Epoch number:  4 \tLoss: 2.30005424152632\n",
      "Epoch number:  6 \tLoss: 2.3000541304456994\n",
      "Epoch number:  8 \tLoss: 2.300053975828045\n",
      "Epoch number:  10 \tLoss: 2.300053654341443\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.30005451103182\n",
      "Epoch number:  4 \tLoss: 2.3000544171941\n",
      "Epoch number:  6 \tLoss: 2.300054347880488\n",
      "Epoch number:  8 \tLoss: 2.300054296552629\n",
      "Epoch number:  10 \tLoss: 2.3000542654484835\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.30005698090526\n",
      "Epoch number:  4 \tLoss: 2.3000555154173723\n",
      "Epoch number:  6 \tLoss: 2.3000547258496713\n",
      "Epoch number:  8 \tLoss: 2.300054302688591\n",
      "Epoch number:  10 \tLoss: 2.3000540748498093\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300054949673752\n",
      "Epoch number:  4 \tLoss: 2.300054402781294\n",
      "Epoch number:  6 \tLoss: 2.3000541147591886\n",
      "Epoch number:  8 \tLoss: 2.30005396263823\n",
      "Epoch number:  10 \tLoss: 2.300053881539245\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3000574565317766\n",
      "Epoch number:  4 \tLoss: 2.300055808378737\n",
      "Epoch number:  6 \tLoss: 2.300054898129572\n",
      "Epoch number:  8 \tLoss: 2.3000544025307676\n",
      "Epoch number:  10 \tLoss: 2.300054132966767\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3000561752917204\n",
      "Epoch number:  4 \tLoss: 2.3000551510228258\n",
      "Epoch number:  6 \tLoss: 2.30005455326953\n",
      "Epoch number:  8 \tLoss: 2.3000542167462728\n",
      "Epoch number:  10 \tLoss: 2.300054029744692\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300058114099895\n",
      "Epoch number:  4 \tLoss: 2.300056062668501\n",
      "Epoch number:  6 \tLoss: 2.3000550015374923\n",
      "Epoch number:  8 \tLoss: 2.3000544472387965\n",
      "Epoch number:  10 \tLoss: 2.300054153725894\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300058261301078\n",
      "Epoch number:  4 \tLoss: 2.300056116670307\n",
      "Epoch number:  6 \tLoss: 2.300055023161991\n",
      "Epoch number:  8 \tLoss: 2.30005445689168\n",
      "Epoch number:  10 \tLoss: 2.300054158518053\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2995704678561006\n",
      "Epoch number:  4 \tLoss: 2.299570198409872\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2995715556370504\n",
      "Epoch number:  4 \tLoss: 2.299571203852424\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2995682051978488\n",
      "Epoch number:  4 \tLoss: 2.2995678487016025\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.29957112197541\n",
      "Epoch number:  4 \tLoss: 2.2995707823794453\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.29956918271219\n",
      "Epoch number:  4 \tLoss: 2.299568722859796\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2995706939511114\n",
      "Epoch number:  4 \tLoss: 2.299570479151442\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2995685184232753\n",
      "Epoch number:  4 \tLoss: 2.2995683981136623\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299570716092357\n",
      "Epoch number:  4 \tLoss: 2.2995704598817723\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299569956361621\n",
      "Epoch number:  4 \tLoss: 2.299569603934687\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2995744743427338\n",
      "Epoch number:  4 \tLoss: 2.2995741849191336\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2995695612625906\n",
      "Epoch number:  4 \tLoss: 2.2995691920561843\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2995720370076276\n",
      "Epoch number:  4 \tLoss: 2.299571686435604\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299577728911707\n",
      "Epoch number:  4 \tLoss: 2.2995759923449133\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2995731673922752\n",
      "Epoch number:  4 \tLoss: 2.2995722521691113\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2995790015750055\n",
      "Epoch number:  4 \tLoss: 2.299576812648248\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299574087902272\n",
      "Epoch number:  4 \tLoss: 2.2995729308758754\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2995738922433144\n",
      "Epoch number:  4 \tLoss: 2.2995727160816197\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2995771324599987\n",
      "Epoch number:  4 \tLoss: 2.2995753693976146\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2995678354435642\n",
      "Epoch number:  4 \tLoss: 2.299567802231219\n",
      "Epoch number:  6 \tLoss: 2.299567578579566\n",
      "Epoch number:  8 \tLoss: 2.2995673280827935\n",
      "Epoch number:  10 \tLoss: 2.299565877318807\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2995731047128074\n",
      "Epoch number:  4 \tLoss: 2.299572625890702\n",
      "Epoch number:  6 \tLoss: 2.299572197746782\n",
      "Epoch number:  8 \tLoss: 2.2995718076173697\n",
      "Epoch number:  10 \tLoss: 2.2995714511640637\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299569821806671\n",
      "Epoch number:  4 \tLoss: 2.2995695952039514\n",
      "Epoch number:  6 \tLoss: 2.2995693159391113\n",
      "Epoch number:  8 \tLoss: 2.2995688207312814\n",
      "Epoch number:  10 \tLoss: 2.299564939180012\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2995719024494443\n",
      "Epoch number:  4 \tLoss: 2.2995715431828097\n",
      "Epoch number:  6 \tLoss: 2.299571224774885\n",
      "Epoch number:  8 \tLoss: 2.2995709354107543\n",
      "Epoch number:  10 \tLoss: 2.29957067167506\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2995702347573355\n",
      "Epoch number:  4 \tLoss: 2.2995700500652387\n",
      "Epoch number:  6 \tLoss: 2.2995695769938096\n",
      "Epoch number:  8 \tLoss: 2.299568998557726\n",
      "Epoch number:  10 \tLoss: 2.299564004095801\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2995696853830467\n",
      "Epoch number:  4 \tLoss: 2.2995694577475203\n",
      "Epoch number:  6 \tLoss: 2.2995692598802844\n",
      "Epoch number:  8 \tLoss: 2.299569081018914\n",
      "Epoch number:  10 \tLoss: 2.2995689189128417\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2995673608777207\n",
      "Epoch number:  4 \tLoss: 2.2995670625624975\n",
      "Epoch number:  6 \tLoss: 2.2995667547165013\n",
      "Epoch number:  8 \tLoss: 2.2995664628154358\n",
      "Epoch number:  10 \tLoss: 2.2995658048624947\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2995699642849656\n",
      "Epoch number:  4 \tLoss: 2.299569794355289\n",
      "Epoch number:  6 \tLoss: 2.2995696466527074\n",
      "Epoch number:  8 \tLoss: 2.2995695111906476\n",
      "Epoch number:  10 \tLoss: 2.2995693865695417\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299566325985592\n",
      "Epoch number:  4 \tLoss: 2.2995659303003513\n",
      "Epoch number:  6 \tLoss: 2.2995656493842076\n",
      "Epoch number:  8 \tLoss: 2.2995652437459664\n",
      "Epoch number:  10 \tLoss: 2.2995641092184185\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.29957252805699\n",
      "Epoch number:  4 \tLoss: 2.2995722352816617\n",
      "Epoch number:  6 \tLoss: 2.2995719782798685\n",
      "Epoch number:  8 \tLoss: 2.2995717456916767\n",
      "Epoch number:  10 \tLoss: 2.2995715346070575\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2995706025640916\n",
      "Epoch number:  4 \tLoss: 2.2995701003429163\n",
      "Epoch number:  6 \tLoss: 2.2995696682263094\n",
      "Epoch number:  8 \tLoss: 2.2995688630643163\n",
      "Epoch number:  10 \tLoss: 2.2995666372678896\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2995708786310556\n",
      "Epoch number:  4 \tLoss: 2.299570577246832\n",
      "Epoch number:  6 \tLoss: 2.2995703132414804\n",
      "Epoch number:  8 \tLoss: 2.299570075129075\n",
      "Epoch number:  10 \tLoss: 2.2995698598533822\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2995734651198982\n",
      "Epoch number:  4 \tLoss: 2.299572298694675\n",
      "Epoch number:  6 \tLoss: 2.2995713621360334\n",
      "Epoch number:  8 \tLoss: 2.2995706048072124\n",
      "Epoch number:  10 \tLoss: 2.2995699917802472\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299574291269607\n",
      "Epoch number:  4 \tLoss: 2.2995731287794743\n",
      "Epoch number:  6 \tLoss: 2.2995721550425925\n",
      "Epoch number:  8 \tLoss: 2.2995713368086426\n",
      "Epoch number:  10 \tLoss: 2.2995706514713046\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2995716596276985\n",
      "Epoch number:  4 \tLoss: 2.2995709825137363\n",
      "Epoch number:  6 \tLoss: 2.2995704119236926\n",
      "Epoch number:  8 \tLoss: 2.2995699205110163\n",
      "Epoch number:  10 \tLoss: 2.2995695006667862\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299575954782439\n",
      "Epoch number:  4 \tLoss: 2.2995744196691112\n",
      "Epoch number:  6 \tLoss: 2.299573158280827\n",
      "Epoch number:  8 \tLoss: 2.299572117769556\n",
      "Epoch number:  10 \tLoss: 2.2995712604530594\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299576860032087\n",
      "Epoch number:  4 \tLoss: 2.2995751115450958\n",
      "Epoch number:  6 \tLoss: 2.2995736826661557\n",
      "Epoch number:  8 \tLoss: 2.2995725223737886\n",
      "Epoch number:  10 \tLoss: 2.2995715749471453\n",
      "Training with params: learning_rate=0.001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299577023106273\n",
      "Epoch number:  4 \tLoss: 2.29957524918696\n",
      "Epoch number:  6 \tLoss: 2.299573802644784\n",
      "Epoch number:  8 \tLoss: 2.2995726185607372\n",
      "Epoch number:  10 \tLoss: 2.2995716498894083\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300979097410354\n",
      "Epoch number:  4 \tLoss: 2.30089337511437\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300968110870052\n",
      "Epoch number:  4 \tLoss: 2.300882781221424\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3024059624981983\n",
      "Epoch number:  4 \tLoss: 2.302085977456545\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.302402667639242\n",
      "Epoch number:  4 \tLoss: 2.302080589877325\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3050463682813196\n",
      "Epoch number:  4 \tLoss: 2.3038880264619626\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3050052474568266\n",
      "Epoch number:  4 \tLoss: 2.3038717924997467\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3009734239802024\n",
      "Epoch number:  4 \tLoss: 2.3008867635387906\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.30097325528568\n",
      "Epoch number:  4 \tLoss: 2.3008871801212116\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.302415337861033\n",
      "Epoch number:  4 \tLoss: 2.3020928765143576\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3024415601664736\n",
      "Epoch number:  4 \tLoss: 2.3021155643861566\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.304969598972398\n",
      "Epoch number:  4 \tLoss: 2.3038400771357823\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.305019350360783\n",
      "Epoch number:  4 \tLoss: 2.3038681170036086\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2973440192080608\n",
      "Epoch number:  4 \tLoss: 2.297336873827785\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2973505719539182\n",
      "Epoch number:  4 \tLoss: 2.297341686231221\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3003529463855346\n",
      "Epoch number:  4 \tLoss: 2.3001726467499775\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.30040605040584\n",
      "Epoch number:  4 \tLoss: 2.3002096299637738\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3040598837573323\n",
      "Epoch number:  4 \tLoss: 2.3032547322494517\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3041081513042614\n",
      "Epoch number:  4 \tLoss: 2.3032889862495445\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300972012065155\n",
      "Epoch number:  4 \tLoss: 2.3008869656331674\n",
      "Epoch number:  6 \tLoss: 2.3008067956535223\n",
      "Epoch number:  8 \tLoss: 2.30073127180375\n",
      "Epoch number:  10 \tLoss: 2.300660173756509\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3009523361118753\n",
      "Epoch number:  4 \tLoss: 2.300868149833737\n",
      "Epoch number:  6 \tLoss: 2.300788897108703\n",
      "Epoch number:  8 \tLoss: 2.3007143159341283\n",
      "Epoch number:  10 \tLoss: 2.300644160349349\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3024502108704663\n",
      "Epoch number:  4 \tLoss: 2.3021199337319587\n",
      "Epoch number:  6 \tLoss: 2.3018278111554342\n",
      "Epoch number:  8 \tLoss: 2.3015702019511024\n",
      "Epoch number:  10 \tLoss: 2.301343777415548\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.302412085763759\n",
      "Epoch number:  4 \tLoss: 2.3020902300974453\n",
      "Epoch number:  6 \tLoss: 2.3018054550341014\n",
      "Epoch number:  8 \tLoss: 2.3015541379539237\n",
      "Epoch number:  10 \tLoss: 2.3013330136662122\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.305036864067068\n",
      "Epoch number:  4 \tLoss: 2.303880482711544\n",
      "Epoch number:  6 \tLoss: 2.3029699396270518\n",
      "Epoch number:  8 \tLoss: 2.302260579011892\n",
      "Epoch number:  10 \tLoss: 2.301708113316517\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.305022659238219\n",
      "Epoch number:  4 \tLoss: 2.3038735676296636\n",
      "Epoch number:  6 \tLoss: 2.302973407350364\n",
      "Epoch number:  8 \tLoss: 2.3022789917276976\n",
      "Epoch number:  10 \tLoss: 2.3017498421893885\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3009658163495263\n",
      "Epoch number:  4 \tLoss: 2.300880482695877\n",
      "Epoch number:  6 \tLoss: 2.3008001489960948\n",
      "Epoch number:  8 \tLoss: 2.3007245617951075\n",
      "Epoch number:  10 \tLoss: 2.30065348589254\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300954465586348\n",
      "Epoch number:  4 \tLoss: 2.300870222770393\n",
      "Epoch number:  6 \tLoss: 2.3007909245317775\n",
      "Epoch number:  8 \tLoss: 2.3007163084982576\n",
      "Epoch number:  10 \tLoss: 2.300646127800206\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.30242922465008\n",
      "Epoch number:  4 \tLoss: 2.3021055496491427\n",
      "Epoch number:  6 \tLoss: 2.3018191249663236\n",
      "Epoch number:  8 \tLoss: 2.301566410152289\n",
      "Epoch number:  10 \tLoss: 2.301344129634904\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3024159130431734\n",
      "Epoch number:  4 \tLoss: 2.302094721868203\n",
      "Epoch number:  6 \tLoss: 2.3018105535303675\n",
      "Epoch number:  8 \tLoss: 2.301559782068253\n",
      "Epoch number:  10 \tLoss: 2.301339145897589\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3049709034111427\n",
      "Epoch number:  4 \tLoss: 2.3038349936877385\n",
      "Epoch number:  6 \tLoss: 2.3029420665114895\n",
      "Epoch number:  8 \tLoss: 2.3022492867057673\n",
      "Epoch number:  10 \tLoss: 2.3017163067677684\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3049881195393036\n",
      "Epoch number:  4 \tLoss: 2.303853349402084\n",
      "Epoch number:  6 \tLoss: 2.3029625288574946\n",
      "Epoch number:  8 \tLoss: 2.3022739564761348\n",
      "Epoch number:  10 \tLoss: 2.30174842875366\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.297330626838688\n",
      "Epoch number:  4 \tLoss: 2.297327009397716\n",
      "Epoch number:  6 \tLoss: 2.2973254035305164\n",
      "Epoch number:  8 \tLoss: 2.297325126477599\n",
      "Epoch number:  10 \tLoss: 2.2973257281927504\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2973539784365693\n",
      "Epoch number:  4 \tLoss: 2.2973440833558394\n",
      "Epoch number:  6 \tLoss: 2.2973378982412216\n",
      "Epoch number:  8 \tLoss: 2.297334278612508\n",
      "Epoch number:  10 \tLoss: 2.2973324380074462\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3003854547386697\n",
      "Epoch number:  4 \tLoss: 2.3001961532538098\n",
      "Epoch number:  6 \tLoss: 2.300064002767344\n",
      "Epoch number:  8 \tLoss: 2.29997111106739\n",
      "Epoch number:  10 \tLoss: 2.2999054194689625\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3003785954124805\n",
      "Epoch number:  4 \tLoss: 2.3001916058215572\n",
      "Epoch number:  6 \tLoss: 2.3000608832359974\n",
      "Epoch number:  8 \tLoss: 2.299968925791923\n",
      "Epoch number:  10 \tLoss: 2.2999038691775544\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3040623058595\n",
      "Epoch number:  4 \tLoss: 2.3032520828328655\n",
      "Epoch number:  6 \tLoss: 2.3027308522300585\n",
      "Epoch number:  8 \tLoss: 2.302391375525996\n",
      "Epoch number:  10 \tLoss: 2.3021673765201287\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.304040263487126\n",
      "Epoch number:  4 \tLoss: 2.3032409039517923\n",
      "Epoch number:  6 \tLoss: 2.302725845016729\n",
      "Epoch number:  8 \tLoss: 2.3023900013267884\n",
      "Epoch number:  10 \tLoss: 2.302168121266793\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300914744894326\n",
      "Epoch number:  4 \tLoss: 2.3008272969221677\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300910222969999\n",
      "Epoch number:  4 \tLoss: 2.300823479940701\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.302383441082282\n",
      "Epoch number:  4 \tLoss: 2.3020463440084318\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.302373392061185\n",
      "Epoch number:  4 \tLoss: 2.3020346540533683\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3050381162089417\n",
      "Epoch number:  4 \tLoss: 2.3037965352396985\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.305046154515806\n",
      "Epoch number:  4 \tLoss: 2.3037987706867415\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300931754463778\n",
      "Epoch number:  4 \tLoss: 2.3008438222124354\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300927473087383\n",
      "Epoch number:  4 \tLoss: 2.300839835034857\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.302391741856757\n",
      "Epoch number:  4 \tLoss: 2.3020486299230813\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.302356092377279\n",
      "Epoch number:  4 \tLoss: 2.3020177987546293\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3050903781873493\n",
      "Epoch number:  4 \tLoss: 2.303840727875234\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3050825986665715\n",
      "Epoch number:  4 \tLoss: 2.3038291299948725\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299976968063119\n",
      "Epoch number:  4 \tLoss: 2.299911405229099\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2999813552634754\n",
      "Epoch number:  4 \tLoss: 2.2999157470845883\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301829255571096\n",
      "Epoch number:  4 \tLoss: 2.3015368939833842\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301849269522643\n",
      "Epoch number:  4 \tLoss: 2.3015521982351834\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3048681907031745\n",
      "Epoch number:  4 \tLoss: 2.303726376333554\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.304816147714108\n",
      "Epoch number:  4 \tLoss: 2.303696094991722\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3009343816718957\n",
      "Epoch number:  4 \tLoss: 2.300846725594819\n",
      "Epoch number:  6 \tLoss: 2.300764482536955\n",
      "Epoch number:  8 \tLoss: 2.3006872916267547\n",
      "Epoch number:  10 \tLoss: 2.3006148205648556\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300933655440114\n",
      "Epoch number:  4 \tLoss: 2.3008465296410217\n",
      "Epoch number:  6 \tLoss: 2.3007645858270185\n",
      "Epoch number:  8 \tLoss: 2.3006875271567413\n",
      "Epoch number:  10 \tLoss: 2.300615077119428\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.302366936917234\n",
      "Epoch number:  4 \tLoss: 2.3020331247311203\n",
      "Epoch number:  6 \tLoss: 2.301739247238686\n",
      "Epoch number:  8 \tLoss: 2.3014808859249425\n",
      "Epoch number:  10 \tLoss: 2.3012541263579873\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.302361661022637\n",
      "Epoch number:  4 \tLoss: 2.302024488099816\n",
      "Epoch number:  6 \tLoss: 2.3017282298567308\n",
      "Epoch number:  8 \tLoss: 2.301468415667171\n",
      "Epoch number:  10 \tLoss: 2.3012411236993\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.305040051492557\n",
      "Epoch number:  4 \tLoss: 2.3037837083154886\n",
      "Epoch number:  6 \tLoss: 2.3028202386950025\n",
      "Epoch number:  8 \tLoss: 2.3020861279306084\n",
      "Epoch number:  10 \tLoss: 2.3015247941252945\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.305062267514993\n",
      "Epoch number:  4 \tLoss: 2.303811411355809\n",
      "Epoch number:  6 \tLoss: 2.3028537882653732\n",
      "Epoch number:  8 \tLoss: 2.3021292445218173\n",
      "Epoch number:  10 \tLoss: 2.301586325634575\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300926474607198\n",
      "Epoch number:  4 \tLoss: 2.3008391535885364\n",
      "Epoch number:  6 \tLoss: 2.300757021451081\n",
      "Epoch number:  8 \tLoss: 2.300679770050083\n",
      "Epoch number:  10 \tLoss: 2.3006071133562007\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300926560608002\n",
      "Epoch number:  4 \tLoss: 2.300839046716902\n",
      "Epoch number:  6 \tLoss: 2.3007568089312307\n",
      "Epoch number:  8 \tLoss: 2.3006795436245002\n",
      "Epoch number:  10 \tLoss: 2.300606967579147\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3023712523216875\n",
      "Epoch number:  4 \tLoss: 2.3020345597704814\n",
      "Epoch number:  6 \tLoss: 2.301738191568965\n",
      "Epoch number:  8 \tLoss: 2.3014775694056695\n",
      "Epoch number:  10 \tLoss: 2.301248645199794\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3023820055518707\n",
      "Epoch number:  4 \tLoss: 2.3020415973894863\n",
      "Epoch number:  6 \tLoss: 2.30174267721408\n",
      "Epoch number:  8 \tLoss: 2.301480683982593\n",
      "Epoch number:  10 \tLoss: 2.3012516160222845\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3050639346307733\n",
      "Epoch number:  4 \tLoss: 2.3038036633373022\n",
      "Epoch number:  6 \tLoss: 2.302843291545637\n",
      "Epoch number:  8 \tLoss: 2.3021182463424252\n",
      "Epoch number:  10 \tLoss: 2.301570262186159\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.305097879900064\n",
      "Epoch number:  4 \tLoss: 2.3038516422329023\n",
      "Epoch number:  6 \tLoss: 2.302893905273405\n",
      "Epoch number:  8 \tLoss: 2.3021662150038167\n",
      "Epoch number:  10 \tLoss: 2.301618539378283\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2999884081486925\n",
      "Epoch number:  4 \tLoss: 2.29992163204324\n",
      "Epoch number:  6 \tLoss: 2.299863067623257\n",
      "Epoch number:  8 \tLoss: 2.2998116369305297\n",
      "Epoch number:  10 \tLoss: 2.2997664061906447\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299982714299855\n",
      "Epoch number:  4 \tLoss: 2.2999170598849847\n",
      "Epoch number:  6 \tLoss: 2.2998593735326165\n",
      "Epoch number:  8 \tLoss: 2.299808634838291\n",
      "Epoch number:  10 \tLoss: 2.299763953356988\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301851612818502\n",
      "Epoch number:  4 \tLoss: 2.3015548812171085\n",
      "Epoch number:  6 \tLoss: 2.3013094564696415\n",
      "Epoch number:  8 \tLoss: 2.3011061837007922\n",
      "Epoch number:  10 \tLoss: 2.300937532031505\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301864960654851\n",
      "Epoch number:  4 \tLoss: 2.3015653752557426\n",
      "Epoch number:  6 \tLoss: 2.301317752000108\n",
      "Epoch number:  8 \tLoss: 2.301112810751026\n",
      "Epoch number:  10 \tLoss: 2.3009429063648965\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3048328074965907\n",
      "Epoch number:  4 \tLoss: 2.303690447162626\n",
      "Epoch number:  6 \tLoss: 2.3028574628193925\n",
      "Epoch number:  8 \tLoss: 2.302250826157349\n",
      "Epoch number:  10 \tLoss: 2.3018073958212617\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.304885826342206\n",
      "Epoch number:  4 \tLoss: 2.303731834522431\n",
      "Epoch number:  6 \tLoss: 2.3028897363851395\n",
      "Epoch number:  8 \tLoss: 2.30227627566219\n",
      "Epoch number:  10 \tLoss: 2.301828491042846\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300919261428323\n",
      "Epoch number:  4 \tLoss: 2.300829212192555\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3009218098120225\n",
      "Epoch number:  4 \tLoss: 2.3008319452307395\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3023701559156327\n",
      "Epoch number:  4 \tLoss: 2.3020072600863797\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.302412430732142\n",
      "Epoch number:  4 \tLoss: 2.3020445149591175\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.305172151992559\n",
      "Epoch number:  4 \tLoss: 2.3037407816773827\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.305227227071969\n",
      "Epoch number:  4 \tLoss: 2.3037738880153302\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3009307734543833\n",
      "Epoch number:  4 \tLoss: 2.3008394536032317\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3009115642934885\n",
      "Epoch number:  4 \tLoss: 2.300821979420872\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3023727085822854\n",
      "Epoch number:  4 \tLoss: 2.3020087788726507\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3023729362704213\n",
      "Epoch number:  4 \tLoss: 2.3020102414291213\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3051622249476735\n",
      "Epoch number:  4 \tLoss: 2.3037307611319218\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3051769031512497\n",
      "Epoch number:  4 \tLoss: 2.303749214646818\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3007115453792553\n",
      "Epoch number:  4 \tLoss: 2.3006271258206596\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3007054827762907\n",
      "Epoch number:  4 \tLoss: 2.300621609994191\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.30228462696777\n",
      "Epoch number:  4 \tLoss: 2.301931954503497\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.302283198049475\n",
      "Epoch number:  4 \tLoss: 2.30193075277952\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3052114729737627\n",
      "Epoch number:  4 \tLoss: 2.3037881583972926\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3052553678145293\n",
      "Epoch number:  4 \tLoss: 2.3038502914664\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300919039873447\n",
      "Epoch number:  4 \tLoss: 2.300830284327066\n",
      "Epoch number:  6 \tLoss: 2.300747226366081\n",
      "Epoch number:  8 \tLoss: 2.3006694845742905\n",
      "Epoch number:  10 \tLoss: 2.3005967079447376\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.30090728699357\n",
      "Epoch number:  4 \tLoss: 2.300818107666006\n",
      "Epoch number:  6 \tLoss: 2.300734616101187\n",
      "Epoch number:  8 \tLoss: 2.300656446157165\n",
      "Epoch number:  10 \tLoss: 2.300583263224811\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3023622968917445\n",
      "Epoch number:  4 \tLoss: 2.3020006067835195\n",
      "Epoch number:  6 \tLoss: 2.301687361912252\n",
      "Epoch number:  8 \tLoss: 2.3014159683638584\n",
      "Epoch number:  10 \tLoss: 2.3011808908430917\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.302386542902185\n",
      "Epoch number:  4 \tLoss: 2.3020223806912115\n",
      "Epoch number:  6 \tLoss: 2.3017075951330055\n",
      "Epoch number:  8 \tLoss: 2.301435504420366\n",
      "Epoch number:  10 \tLoss: 2.3012005511519975\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3051795887434365\n",
      "Epoch number:  4 \tLoss: 2.303736399658989\n",
      "Epoch number:  6 \tLoss: 2.302692616240067\n",
      "Epoch number:  8 \tLoss: 2.3019350946435906\n",
      "Epoch number:  10 \tLoss: 2.30137882307487\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3051972507649423\n",
      "Epoch number:  4 \tLoss: 2.303757650122554\n",
      "Epoch number:  6 \tLoss: 2.3027187913948244\n",
      "Epoch number:  8 \tLoss: 2.3019693177325835\n",
      "Epoch number:  10 \tLoss: 2.30142848647265\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3009090974950794\n",
      "Epoch number:  4 \tLoss: 2.300820192351909\n",
      "Epoch number:  6 \tLoss: 2.3007370467790365\n",
      "Epoch number:  8 \tLoss: 2.3006592633742544\n",
      "Epoch number:  10 \tLoss: 2.30058647975705\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300910605137485\n",
      "Epoch number:  4 \tLoss: 2.3008208755663397\n",
      "Epoch number:  6 \tLoss: 2.300736909922166\n",
      "Epoch number:  8 \tLoss: 2.300658337794485\n",
      "Epoch number:  10 \tLoss: 2.300584819622361\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3023722741710606\n",
      "Epoch number:  4 \tLoss: 2.302008632702186\n",
      "Epoch number:  6 \tLoss: 2.3016942083558822\n",
      "Epoch number:  8 \tLoss: 2.301422265300669\n",
      "Epoch number:  10 \tLoss: 2.3011871630723246\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.302351045987722\n",
      "Epoch number:  4 \tLoss: 2.301990666768905\n",
      "Epoch number:  6 \tLoss: 2.3016790490298438\n",
      "Epoch number:  8 \tLoss: 2.3014096404514985\n",
      "Epoch number:  10 \tLoss: 2.3011769757449563\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.305187987407264\n",
      "Epoch number:  4 \tLoss: 2.3037441416740467\n",
      "Epoch number:  6 \tLoss: 2.302699643638739\n",
      "Epoch number:  8 \tLoss: 2.3019416694914203\n",
      "Epoch number:  10 \tLoss: 2.3013838364965777\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3051697374681424\n",
      "Epoch number:  4 \tLoss: 2.3037401341356456\n",
      "Epoch number:  6 \tLoss: 2.3027076700107254\n",
      "Epoch number:  8 \tLoss: 2.3019624413944957\n",
      "Epoch number:  10 \tLoss: 2.3014245470829007\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3007156944572267\n",
      "Epoch number:  4 \tLoss: 2.3006307777645962\n",
      "Epoch number:  6 \tLoss: 2.3005527400212515\n",
      "Epoch number:  8 \tLoss: 2.300481019105036\n",
      "Epoch number:  10 \tLoss: 2.300415098690929\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300705886151329\n",
      "Epoch number:  4 \tLoss: 2.300622844186614\n",
      "Epoch number:  6 \tLoss: 2.3005465524231234\n",
      "Epoch number:  8 \tLoss: 2.3004764544517795\n",
      "Epoch number:  10 \tLoss: 2.3004120411845284\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.302278564621818\n",
      "Epoch number:  4 \tLoss: 2.3019256773558383\n",
      "Epoch number:  6 \tLoss: 2.3016251262530285\n",
      "Epoch number:  8 \tLoss: 2.301368526558696\n",
      "Epoch number:  10 \tLoss: 2.3011489795499114\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3023118688642\n",
      "Epoch number:  4 \tLoss: 2.3019564514036555\n",
      "Epoch number:  6 \tLoss: 2.301654107912982\n",
      "Epoch number:  8 \tLoss: 2.301396765935623\n",
      "Epoch number:  10 \tLoss: 2.3011777452113393\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3051731409482943\n",
      "Epoch number:  4 \tLoss: 2.3037774340916974\n",
      "Epoch number:  6 \tLoss: 2.30277807893868\n",
      "Epoch number:  8 \tLoss: 2.302060001598194\n",
      "Epoch number:  10 \tLoss: 2.301539125970067\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3052649499992808\n",
      "Epoch number:  4 \tLoss: 2.3038439347560624\n",
      "Epoch number:  6 \tLoss: 2.302829937210299\n",
      "Epoch number:  8 \tLoss: 2.302104983002459\n",
      "Epoch number:  10 \tLoss: 2.301585191602637\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299565381163985\n",
      "Epoch number:  4 \tLoss: 2.297853376108737\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300035832466782\n",
      "Epoch number:  4 \tLoss: 2.299443086520913\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.298848323983595\n",
      "Epoch number:  4 \tLoss: 2.295876019789715\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.298233401476292\n",
      "Epoch number:  4 \tLoss: 2.2976661842485537\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2979592195475433\n",
      "Epoch number:  4 \tLoss: 2.294867382754131\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2963564038711515\n",
      "Epoch number:  4 \tLoss: 2.295719420631589\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2994294731367395\n",
      "Epoch number:  4 \tLoss: 2.298231535909614\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300032373244239\n",
      "Epoch number:  4 \tLoss: 2.2994400671040562\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2987184280960777\n",
      "Epoch number:  4 \tLoss: 2.2952679992908105\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2982481952364497\n",
      "Epoch number:  4 \tLoss: 2.297686535174476\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.298008824094093\n",
      "Epoch number:  4 \tLoss: 2.2958566867789445\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.296326066243256\n",
      "Epoch number:  4 \tLoss: 2.2956879025104326\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3026703445931753\n",
      "Epoch number:  4 \tLoss: 2.3023500786773785\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3027008246913474\n",
      "Epoch number:  4 \tLoss: 2.3024161787058235\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301823665050888\n",
      "Epoch number:  4 \tLoss: 2.3008800328357752\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.302048920471445\n",
      "Epoch number:  4 \tLoss: 2.3018803645050374\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2995909946903637\n",
      "Epoch number:  4 \tLoss: 2.298410252381432\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.298790790867703\n",
      "Epoch number:  4 \tLoss: 2.298716951913179\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299571813705393\n",
      "Epoch number:  4 \tLoss: 2.297279557617016\n",
      "Epoch number:  6 \tLoss: 1.6594435651251598\n",
      "Epoch number:  8 \tLoss: 1.04467431876907\n",
      "Epoch number:  10 \tLoss: 0.6574458138247592\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300031162601821\n",
      "Epoch number:  4 \tLoss: 2.2994331457189237\n",
      "Epoch number:  6 \tLoss: 2.2991589273520434\n",
      "Epoch number:  8 \tLoss: 2.2989821174465344\n",
      "Epoch number:  10 \tLoss: 2.2988564925516553\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2989525060161706\n",
      "Epoch number:  4 \tLoss: 2.296226493509618\n",
      "Epoch number:  6 \tLoss: 1.1493234988549792\n",
      "Epoch number:  8 \tLoss: 0.5745906935247304\n",
      "Epoch number:  10 \tLoss: 0.3876619886443298\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2982287892342765\n",
      "Epoch number:  4 \tLoss: 2.297658829024307\n",
      "Epoch number:  6 \tLoss: 2.29732369418905\n",
      "Epoch number:  8 \tLoss: 2.2971264016219703\n",
      "Epoch number:  10 \tLoss: 2.2970110547239244\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2978362623256468\n",
      "Epoch number:  4 \tLoss: 2.294690411104514\n",
      "Epoch number:  6 \tLoss: 0.5634911257479824\n",
      "Epoch number:  8 \tLoss: 0.3703938583900469\n",
      "Epoch number:  10 \tLoss: 0.32469641044423325\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2963528598385103\n",
      "Epoch number:  4 \tLoss: 2.2957074019891732\n",
      "Epoch number:  6 \tLoss: 2.295424304179499\n",
      "Epoch number:  8 \tLoss: 2.295321449850403\n",
      "Epoch number:  10 \tLoss: 2.2953046558317522\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299926774099489\n",
      "Epoch number:  4 \tLoss: 2.2988420233279414\n",
      "Epoch number:  6 \tLoss: 1.7220553713858016\n",
      "Epoch number:  8 \tLoss: 1.6621737798486678\n",
      "Epoch number:  10 \tLoss: 1.0654137190141868\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300023862538923\n",
      "Epoch number:  4 \tLoss: 2.299428570992544\n",
      "Epoch number:  6 \tLoss: 2.2991546762225537\n",
      "Epoch number:  8 \tLoss: 2.2989784466252603\n",
      "Epoch number:  10 \tLoss: 2.29885374401589\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.298143970737916\n",
      "Epoch number:  4 \tLoss: 2.2957048293493725\n",
      "Epoch number:  6 \tLoss: 1.3497453067459948\n",
      "Epoch number:  8 \tLoss: 0.534227493542743\n",
      "Epoch number:  10 \tLoss: 0.40295751708658584\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2982323340138837\n",
      "Epoch number:  4 \tLoss: 2.297660863995651\n",
      "Epoch number:  6 \tLoss: 2.2973232233380316\n",
      "Epoch number:  8 \tLoss: 2.297122664407328\n",
      "Epoch number:  10 \tLoss: 2.297003449701857\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2983507505550826\n",
      "Epoch number:  4 \tLoss: 2.2952989636993615\n",
      "Epoch number:  6 \tLoss: 0.5083724442661728\n",
      "Epoch number:  8 \tLoss: 0.36593678859149004\n",
      "Epoch number:  10 \tLoss: 0.31949371214398803\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2963878010583616\n",
      "Epoch number:  4 \tLoss: 2.2957458589512294\n",
      "Epoch number:  6 \tLoss: 2.295465355762646\n",
      "Epoch number:  8 \tLoss: 2.295365239218645\n",
      "Epoch number:  10 \tLoss: 2.2953520337613544\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.30264747209724\n",
      "Epoch number:  4 \tLoss: 2.302330167094139\n",
      "Epoch number:  6 \tLoss: 2.302033026695571\n",
      "Epoch number:  8 \tLoss: 2.3017153440717877\n",
      "Epoch number:  10 \tLoss: 2.3013532742871856\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3026967118242454\n",
      "Epoch number:  4 \tLoss: 2.302413562798649\n",
      "Epoch number:  6 \tLoss: 2.3021578479462645\n",
      "Epoch number:  8 \tLoss: 2.3019037110236447\n",
      "Epoch number:  10 \tLoss: 2.3016505870276296\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3018662794551443\n",
      "Epoch number:  4 \tLoss: 2.300930589908261\n",
      "Epoch number:  6 \tLoss: 2.2843569921614906\n",
      "Epoch number:  8 \tLoss: 1.7121563450081752\n",
      "Epoch number:  10 \tLoss: 1.7009133078014707\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.302051737682464\n",
      "Epoch number:  4 \tLoss: 2.30188140953326\n",
      "Epoch number:  6 \tLoss: 2.3018090602381354\n",
      "Epoch number:  8 \tLoss: 2.3017456652433648\n",
      "Epoch number:  10 \tLoss: 2.301683067042103\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2995265746629343\n",
      "Epoch number:  4 \tLoss: 2.2980933575460085\n",
      "Epoch number:  6 \tLoss: 1.7544651487929046\n",
      "Epoch number:  8 \tLoss: 1.7475476150799545\n",
      "Epoch number:  10 \tLoss: 1.7403452110404714\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2987879843980035\n",
      "Epoch number:  4 \tLoss: 2.2987140559982917\n",
      "Epoch number:  6 \tLoss: 2.2987821898768663\n",
      "Epoch number:  8 \tLoss: 2.2988752195386053\n",
      "Epoch number:  10 \tLoss: 2.298981330673117\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301609618078279\n",
      "Epoch number:  4 \tLoss: 2.3014795940988755\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.302161541263609\n",
      "Epoch number:  4 \tLoss: 2.3022121328065914\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3011700029571593\n",
      "Epoch number:  4 \tLoss: 2.3010139140479655\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301767702614188\n",
      "Epoch number:  4 \tLoss: 2.3011973726264237\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299881416036503\n",
      "Epoch number:  4 \tLoss: 2.2997300843162436\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299389203235296\n",
      "Epoch number:  4 \tLoss: 2.2994852895291067\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301438372902294\n",
      "Epoch number:  4 \tLoss: 2.3012844162211255\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.302156839728002\n",
      "Epoch number:  4 \tLoss: 2.302206184516371\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3011582900257133\n",
      "Epoch number:  4 \tLoss: 2.300895470066511\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3017453035146054\n",
      "Epoch number:  4 \tLoss: 2.3011757738519742\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299978753821106\n",
      "Epoch number:  4 \tLoss: 2.2999924323398937\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299403622893381\n",
      "Epoch number:  4 \tLoss: 2.299494985786777\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3014846760849768\n",
      "Epoch number:  4 \tLoss: 2.301290462959078\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301919157768324\n",
      "Epoch number:  4 \tLoss: 2.3020706077633615\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.302046230617341\n",
      "Epoch number:  4 \tLoss: 2.3017847331191095\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3025786940546844\n",
      "Epoch number:  4 \tLoss: 2.302326178111248\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300131283169755\n",
      "Epoch number:  4 \tLoss: 2.299853971454956\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2998393553445884\n",
      "Epoch number:  4 \tLoss: 2.2994194438060003\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301501761485915\n",
      "Epoch number:  4 \tLoss: 2.3012624041034893\n",
      "Epoch number:  6 \tLoss: 2.300922344227871\n",
      "Epoch number:  8 \tLoss: 2.299352063050489\n",
      "Epoch number:  10 \tLoss: 1.2782536760159835\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.30217352640712\n",
      "Epoch number:  4 \tLoss: 2.302218201586251\n",
      "Epoch number:  6 \tLoss: 2.3022297069988706\n",
      "Epoch number:  8 \tLoss: 2.3021784259536924\n",
      "Epoch number:  10 \tLoss: 2.3020871056367267\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301217931275405\n",
      "Epoch number:  4 \tLoss: 2.3010648509681078\n",
      "Epoch number:  6 \tLoss: 2.300167252486581\n",
      "Epoch number:  8 \tLoss: 1.3607680247585803\n",
      "Epoch number:  10 \tLoss: 0.43435065106737897\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3017484698205792\n",
      "Epoch number:  4 \tLoss: 2.301170057398006\n",
      "Epoch number:  6 \tLoss: 2.3008572430994922\n",
      "Epoch number:  8 \tLoss: 2.3006662974448973\n",
      "Epoch number:  10 \tLoss: 2.3005440454506045\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3001351651391886\n",
      "Epoch number:  4 \tLoss: 2.300026260766454\n",
      "Epoch number:  6 \tLoss: 0.6136137828094501\n",
      "Epoch number:  8 \tLoss: 0.3946410303836737\n",
      "Epoch number:  10 \tLoss: 0.3210318382739191\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2993725574620676\n",
      "Epoch number:  4 \tLoss: 2.299479981994122\n",
      "Epoch number:  6 \tLoss: 2.2995606494237917\n",
      "Epoch number:  8 \tLoss: 2.2995944337210252\n",
      "Epoch number:  10 \tLoss: 2.2996059240584943\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3014487582050775\n",
      "Epoch number:  4 \tLoss: 2.3014345665999985\n",
      "Epoch number:  6 \tLoss: 2.301114709286884\n",
      "Epoch number:  8 \tLoss: 2.29796434174093\n",
      "Epoch number:  10 \tLoss: 1.20760828042994\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3021672636169943\n",
      "Epoch number:  4 \tLoss: 2.3022133957995283\n",
      "Epoch number:  6 \tLoss: 2.3022285575238723\n",
      "Epoch number:  8 \tLoss: 2.3021819778238175\n",
      "Epoch number:  10 \tLoss: 2.302095416938957\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3013514508693\n",
      "Epoch number:  4 \tLoss: 2.3009705296884837\n",
      "Epoch number:  6 \tLoss: 2.3000246209975685\n",
      "Epoch number:  8 \tLoss: 0.8859178772837648\n",
      "Epoch number:  10 \tLoss: 0.41788209294621526\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301755983831136\n",
      "Epoch number:  4 \tLoss: 2.30118457291566\n",
      "Epoch number:  6 \tLoss: 2.3008739939125276\n",
      "Epoch number:  8 \tLoss: 2.300683898573825\n",
      "Epoch number:  10 \tLoss: 2.3005620673167493\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300086507687699\n",
      "Epoch number:  4 \tLoss: 2.300101397605881\n",
      "Epoch number:  6 \tLoss: 0.6250732290150669\n",
      "Epoch number:  8 \tLoss: 0.3584441725363578\n",
      "Epoch number:  10 \tLoss: 0.32414802489747\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2993947971967907\n",
      "Epoch number:  4 \tLoss: 2.299492169407679\n",
      "Epoch number:  6 \tLoss: 2.2995710472733504\n",
      "Epoch number:  8 \tLoss: 2.2996046189530213\n",
      "Epoch number:  10 \tLoss: 2.2996161740441248\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3015139694693563\n",
      "Epoch number:  4 \tLoss: 2.3013363866776366\n",
      "Epoch number:  6 \tLoss: 2.3009598906395667\n",
      "Epoch number:  8 \tLoss: 2.299334349078036\n",
      "Epoch number:  10 \tLoss: 1.6864987100503657\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301912203807657\n",
      "Epoch number:  4 \tLoss: 2.3020692400744047\n",
      "Epoch number:  6 \tLoss: 2.3021138552030465\n",
      "Epoch number:  8 \tLoss: 2.3021226583753696\n",
      "Epoch number:  10 \tLoss: 2.3021152654311963\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3019241745803383\n",
      "Epoch number:  4 \tLoss: 2.3016868397588817\n",
      "Epoch number:  6 \tLoss: 2.301022693158536\n",
      "Epoch number:  8 \tLoss: 1.9269151839069238\n",
      "Epoch number:  10 \tLoss: 1.6536743579417106\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3025836447439496\n",
      "Epoch number:  4 \tLoss: 2.3023330584330606\n",
      "Epoch number:  6 \tLoss: 2.302244061943677\n",
      "Epoch number:  8 \tLoss: 2.3022321855727825\n",
      "Epoch number:  10 \tLoss: 2.302259480283154\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3003146932674694\n",
      "Epoch number:  4 \tLoss: 2.3000606838287743\n",
      "Epoch number:  6 \tLoss: 2.299467133781198\n",
      "Epoch number:  8 \tLoss: 2.297236113679721\n",
      "Epoch number:  10 \tLoss: 1.681548422911227\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299803822351244\n",
      "Epoch number:  4 \tLoss: 2.299404607242357\n",
      "Epoch number:  6 \tLoss: 2.2992890848412966\n",
      "Epoch number:  8 \tLoss: 2.299283432126103\n",
      "Epoch number:  10 \tLoss: 2.29933384652406\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301417686944506\n",
      "Epoch number:  4 \tLoss: 2.3002903245748487\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301716693958213\n",
      "Epoch number:  4 \tLoss: 2.3006009832955074\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3005211622414903\n",
      "Epoch number:  4 \tLoss: 2.300331175910348\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3006244421336963\n",
      "Epoch number:  4 \tLoss: 2.3006318724132004\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301835407210434\n",
      "Epoch number:  4 \tLoss: 2.3021345146269683\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.302172317413703\n",
      "Epoch number:  4 \tLoss: 2.302713395789186\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301393518772523\n",
      "Epoch number:  4 \tLoss: 2.3003655346834617\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3016820080413622\n",
      "Epoch number:  4 \tLoss: 2.300580833402942\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3005227090865676\n",
      "Epoch number:  4 \tLoss: 2.300434184721318\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3006222229041393\n",
      "Epoch number:  4 \tLoss: 2.300646725389473\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3019161509281205\n",
      "Epoch number:  4 \tLoss: 2.3023340524309654\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3021980382884895\n",
      "Epoch number:  4 \tLoss: 2.302736346290877\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3021445660325512\n",
      "Epoch number:  4 \tLoss: 2.301038646634939\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.302380601655633\n",
      "Epoch number:  4 \tLoss: 2.301455432617642\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3011031844111924\n",
      "Epoch number:  4 \tLoss: 2.300617213316468\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.30115075085397\n",
      "Epoch number:  4 \tLoss: 2.300947813475155\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3021734176275053\n",
      "Epoch number:  4 \tLoss: 2.3021682643091035\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3027652258085776\n",
      "Epoch number:  4 \tLoss: 2.302896328810558\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301530451488854\n",
      "Epoch number:  4 \tLoss: 2.300367181036995\n",
      "Epoch number:  6 \tLoss: 2.299803460983151\n",
      "Epoch number:  8 \tLoss: 2.299580348812283\n",
      "Epoch number:  10 \tLoss: 2.299467212159076\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3017149616411396\n",
      "Epoch number:  4 \tLoss: 2.3006023652159415\n",
      "Epoch number:  6 \tLoss: 2.3002672568559293\n",
      "Epoch number:  8 \tLoss: 2.300224389348884\n",
      "Epoch number:  10 \tLoss: 2.3003074959869556\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300575362556939\n",
      "Epoch number:  4 \tLoss: 2.3005223550395337\n",
      "Epoch number:  6 \tLoss: 2.300976891012546\n",
      "Epoch number:  8 \tLoss: 2.3014231095470046\n",
      "Epoch number:  10 \tLoss: 2.3015912147838065\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.30061276027822\n",
      "Epoch number:  4 \tLoss: 2.3006341163870663\n",
      "Epoch number:  6 \tLoss: 2.301177779165971\n",
      "Epoch number:  8 \tLoss: 2.301728760911704\n",
      "Epoch number:  10 \tLoss: 2.3021639932422513\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3018408408868876\n",
      "Epoch number:  4 \tLoss: 2.3021610634581506\n",
      "Epoch number:  6 \tLoss: 0.6439157439106444\n",
      "Epoch number:  8 \tLoss: 0.4025922168475896\n",
      "Epoch number:  10 \tLoss: 0.33446518915801343\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3021676333324637\n",
      "Epoch number:  4 \tLoss: 2.3027104874052218\n",
      "Epoch number:  6 \tLoss: 2.303334688832093\n",
      "Epoch number:  8 \tLoss: 2.3037482891764918\n",
      "Epoch number:  10 \tLoss: 2.3040034990604\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3014636502002124\n",
      "Epoch number:  4 \tLoss: 2.3003739025250214\n",
      "Epoch number:  6 \tLoss: 2.299805263626757\n",
      "Epoch number:  8 \tLoss: 2.299606743569471\n",
      "Epoch number:  10 \tLoss: 2.299514696040072\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3017161563836295\n",
      "Epoch number:  4 \tLoss: 2.300588586747247\n",
      "Epoch number:  6 \tLoss: 2.3002576411498166\n",
      "Epoch number:  8 \tLoss: 2.3002216552856445\n",
      "Epoch number:  10 \tLoss: 2.300311545939066\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300414673973623\n",
      "Epoch number:  4 \tLoss: 2.300200708858997\n",
      "Epoch number:  6 \tLoss: 2.3006830458414758\n",
      "Epoch number:  8 \tLoss: 2.3011481673407785\n",
      "Epoch number:  10 \tLoss: 0.6467078702697356\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3006184557980545\n",
      "Epoch number:  4 \tLoss: 2.300612828225904\n",
      "Epoch number:  6 \tLoss: 2.301141410714899\n",
      "Epoch number:  8 \tLoss: 2.301686736647083\n",
      "Epoch number:  10 \tLoss: 2.302121823656794\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3018518723273713\n",
      "Epoch number:  4 \tLoss: 2.3022878046497617\n",
      "Epoch number:  6 \tLoss: 0.7926950132103602\n",
      "Epoch number:  8 \tLoss: 0.4455946884865714\n",
      "Epoch number:  10 \tLoss: 0.34646938954333184\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3021844624551635\n",
      "Epoch number:  4 \tLoss: 2.3027205992909567\n",
      "Epoch number:  6 \tLoss: 2.3033494332123743\n",
      "Epoch number:  8 \tLoss: 2.303766518901657\n",
      "Epoch number:  10 \tLoss: 2.3040238096254786\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.302191456932891\n",
      "Epoch number:  4 \tLoss: 2.3009764263708057\n",
      "Epoch number:  6 \tLoss: 2.300393630493759\n",
      "Epoch number:  8 \tLoss: 2.3000762371824663\n",
      "Epoch number:  10 \tLoss: 2.2997855614190024\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3023671677225286\n",
      "Epoch number:  4 \tLoss: 2.3014453544221847\n",
      "Epoch number:  6 \tLoss: 2.3010086760865605\n",
      "Epoch number:  8 \tLoss: 2.3007849706097185\n",
      "Epoch number:  10 \tLoss: 2.300657840413068\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3009867891324305\n",
      "Epoch number:  4 \tLoss: 2.300433499528945\n",
      "Epoch number:  6 \tLoss: 2.3004755439989117\n",
      "Epoch number:  8 \tLoss: 2.300438218412153\n",
      "Epoch number:  10 \tLoss: 2.300252110613009\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301156823134574\n",
      "Epoch number:  4 \tLoss: 2.300972882916682\n",
      "Epoch number:  6 \tLoss: 2.301149979000853\n",
      "Epoch number:  8 \tLoss: 2.3013682925849333\n",
      "Epoch number:  10 \tLoss: 2.301564357893586\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3024465208193114\n",
      "Epoch number:  4 \tLoss: 2.3024374648842927\n",
      "Epoch number:  6 \tLoss: 2.3024027469681774\n",
      "Epoch number:  8 \tLoss: 2.3017513718026352\n",
      "Epoch number:  10 \tLoss: 0.5238396987051867\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3027563134020403\n",
      "Epoch number:  4 \tLoss: 2.3028906372638573\n",
      "Epoch number:  6 \tLoss: 2.303144797465217\n",
      "Epoch number:  8 \tLoss: 2.3033575110131435\n",
      "Epoch number:  10 \tLoss: 2.303519632631207\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2994066597721816\n",
      "Epoch number:  4 \tLoss: 2.298058034860307\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300103940549071\n",
      "Epoch number:  4 \tLoss: 2.2995205827273204\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2991679991649248\n",
      "Epoch number:  4 \tLoss: 2.2964995463627007\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.298447732661071\n",
      "Epoch number:  4 \tLoss: 2.297975140863999\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2982174272976907\n",
      "Epoch number:  4 \tLoss: 2.295101485191874\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2968138381314818\n",
      "Epoch number:  4 \tLoss: 2.2965006671326806\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2995723491069064\n",
      "Epoch number:  4 \tLoss: 2.2979910831028962\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3000890773643623\n",
      "Epoch number:  4 \tLoss: 2.299500045793846\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.298880350954085\n",
      "Epoch number:  4 \tLoss: 2.2970646469903695\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.298476882822175\n",
      "Epoch number:  4 \tLoss: 2.2980014348588655\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2981411685758206\n",
      "Epoch number:  4 \tLoss: 2.295115509185307\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2968206141468137\n",
      "Epoch number:  4 \tLoss: 2.2965059730408806\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.302386637492679\n",
      "Epoch number:  4 \tLoss: 2.3020967752329398\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.302410026011932\n",
      "Epoch number:  4 \tLoss: 2.3021473170538673\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3019513798059212\n",
      "Epoch number:  4 \tLoss: 2.3012301678081397\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.302147012944029\n",
      "Epoch number:  4 \tLoss: 2.301964923097238\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3000236239429706\n",
      "Epoch number:  4 \tLoss: 2.298499270322116\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.29951845740991\n",
      "Epoch number:  4 \tLoss: 2.299424254645189\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300000475260625\n",
      "Epoch number:  4 \tLoss: 2.298876176817685\n",
      "Epoch number:  6 \tLoss: 1.6783517899767642\n",
      "Epoch number:  8 \tLoss: 1.1074913889584752\n",
      "Epoch number:  10 \tLoss: 0.8659747699473599\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3001117491972316\n",
      "Epoch number:  4 \tLoss: 2.2995308893543256\n",
      "Epoch number:  6 \tLoss: 2.2992720353406764\n",
      "Epoch number:  8 \tLoss: 2.2991147263362817\n",
      "Epoch number:  10 \tLoss: 2.299009450707788\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2988933085807104\n",
      "Epoch number:  4 \tLoss: 2.2967226952354953\n",
      "Epoch number:  6 \tLoss: 1.4670778403149536\n",
      "Epoch number:  8 \tLoss: 0.6694924489036365\n",
      "Epoch number:  10 \tLoss: 0.40656659534208883\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2984491979638366\n",
      "Epoch number:  4 \tLoss: 2.2979753919306916\n",
      "Epoch number:  6 \tLoss: 2.2977541683097638\n",
      "Epoch number:  8 \tLoss: 2.297644578676061\n",
      "Epoch number:  10 \tLoss: 2.297593440562548\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2979721288692385\n",
      "Epoch number:  4 \tLoss: 2.2946450665406375\n",
      "Epoch number:  6 \tLoss: 0.49239347363282887\n",
      "Epoch number:  8 \tLoss: 0.33854675487343877\n",
      "Epoch number:  10 \tLoss: 0.3052725517469239\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.296804185992229\n",
      "Epoch number:  4 \tLoss: 2.2964985584066557\n",
      "Epoch number:  6 \tLoss: 2.2964066699319154\n",
      "Epoch number:  8 \tLoss: 2.2964096818357476\n",
      "Epoch number:  10 \tLoss: 2.2964540467074133\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2996408137974895\n",
      "Epoch number:  4 \tLoss: 2.297556689179267\n",
      "Epoch number:  6 \tLoss: 1.5706163285738461\n",
      "Epoch number:  8 \tLoss: 0.9563101190053337\n",
      "Epoch number:  10 \tLoss: 0.5850625456410936\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300112257214217\n",
      "Epoch number:  4 \tLoss: 2.2995300423337524\n",
      "Epoch number:  6 \tLoss: 2.299271553632223\n",
      "Epoch number:  8 \tLoss: 2.2991147881297196\n",
      "Epoch number:  10 \tLoss: 2.299010008709768\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2991093016853896\n",
      "Epoch number:  4 \tLoss: 2.29538364283593\n",
      "Epoch number:  6 \tLoss: 1.2520024638592289\n",
      "Epoch number:  8 \tLoss: 0.5087847388928584\n",
      "Epoch number:  10 \tLoss: 0.385280942261131\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.298426820094373\n",
      "Epoch number:  4 \tLoss: 2.297956078805439\n",
      "Epoch number:  6 \tLoss: 2.2977352829756903\n",
      "Epoch number:  8 \tLoss: 2.297624028608502\n",
      "Epoch number:  10 \tLoss: 2.2975698384836405\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2985247819498564\n",
      "Epoch number:  4 \tLoss: 2.2969730422386956\n",
      "Epoch number:  6 \tLoss: 0.5888159919277607\n",
      "Epoch number:  8 \tLoss: 0.35666709331636914\n",
      "Epoch number:  10 \tLoss: 0.3298606285385219\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.296806125060739\n",
      "Epoch number:  4 \tLoss: 2.2964978313331534\n",
      "Epoch number:  6 \tLoss: 2.2964016247050933\n",
      "Epoch number:  8 \tLoss: 2.2963988076671185\n",
      "Epoch number:  10 \tLoss: 2.296436289499403\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3023889859336886\n",
      "Epoch number:  4 \tLoss: 2.3020978644763304\n",
      "Epoch number:  6 \tLoss: 2.3018401940074553\n",
      "Epoch number:  8 \tLoss: 2.301574477863758\n",
      "Epoch number:  10 \tLoss: 2.3012900348880434\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3024146520136557\n",
      "Epoch number:  4 \tLoss: 2.3021481802129817\n",
      "Epoch number:  6 \tLoss: 2.301916591452876\n",
      "Epoch number:  8 \tLoss: 2.301687564698834\n",
      "Epoch number:  10 \tLoss: 2.301460196633646\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301938822146858\n",
      "Epoch number:  4 \tLoss: 2.3011841870134955\n",
      "Epoch number:  6 \tLoss: 2.2965121634100822\n",
      "Epoch number:  8 \tLoss: 1.7115776720716114\n",
      "Epoch number:  10 \tLoss: 1.700596701393263\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3021471985820607\n",
      "Epoch number:  4 \tLoss: 2.3019612564677248\n",
      "Epoch number:  6 \tLoss: 2.3018599386815826\n",
      "Epoch number:  8 \tLoss: 2.3017667059009397\n",
      "Epoch number:  10 \tLoss: 2.30167506474188\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299980984818058\n",
      "Epoch number:  4 \tLoss: 2.29838374525621\n",
      "Epoch number:  6 \tLoss: 1.742832978025104\n",
      "Epoch number:  8 \tLoss: 1.734123650897577\n",
      "Epoch number:  10 \tLoss: 1.727720883281573\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299521190744877\n",
      "Epoch number:  4 \tLoss: 2.299424595983648\n",
      "Epoch number:  6 \tLoss: 2.299446219369808\n",
      "Epoch number:  8 \tLoss: 2.2994852718266587\n",
      "Epoch number:  10 \tLoss: 2.299532439602778\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010366846465202\n",
      "Epoch number:  4 \tLoss: 2.3007801720247985\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.30172115494672\n",
      "Epoch number:  4 \tLoss: 2.3015253725939537\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300739343439958\n",
      "Epoch number:  4 \tLoss: 2.300525094567749\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3011237603926387\n",
      "Epoch number:  4 \tLoss: 2.300751568810551\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3001200138374465\n",
      "Epoch number:  4 \tLoss: 2.29691074717875\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299893501364266\n",
      "Epoch number:  4 \tLoss: 2.299697101096742\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3008326908285928\n",
      "Epoch number:  4 \tLoss: 2.3005299444281393\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301718500950864\n",
      "Epoch number:  4 \tLoss: 2.3015245786531415\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3007391050164947\n",
      "Epoch number:  4 \tLoss: 2.300665464554962\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301109043619206\n",
      "Epoch number:  4 \tLoss: 2.3007355462911403\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3003047354564834\n",
      "Epoch number:  4 \tLoss: 2.29984881367429\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2998978759383513\n",
      "Epoch number:  4 \tLoss: 2.299701011312033\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3012771904247957\n",
      "Epoch number:  4 \tLoss: 2.301053089536244\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301664482533605\n",
      "Epoch number:  4 \tLoss: 2.301713718176687\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301398184904188\n",
      "Epoch number:  4 \tLoss: 2.3010363007927475\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301799632626472\n",
      "Epoch number:  4 \tLoss: 2.3016387723355893\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300555970985179\n",
      "Epoch number:  4 \tLoss: 2.300384808504481\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3005103763215304\n",
      "Epoch number:  4 \tLoss: 2.3003552193171393\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010983711991075\n",
      "Epoch number:  4 \tLoss: 2.3008368947889504\n",
      "Epoch number:  6 \tLoss: 2.300374661553576\n",
      "Epoch number:  8 \tLoss: 2.298051446352216\n",
      "Epoch number:  10 \tLoss: 0.856009898693276\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3017267253081233\n",
      "Epoch number:  4 \tLoss: 2.3015264215411264\n",
      "Epoch number:  6 \tLoss: 2.301413121224143\n",
      "Epoch number:  8 \tLoss: 2.3013181736761137\n",
      "Epoch number:  10 \tLoss: 2.301229904580877\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3009412377282032\n",
      "Epoch number:  4 \tLoss: 2.3005994860613947\n",
      "Epoch number:  6 \tLoss: 2.2997350229600473\n",
      "Epoch number:  8 \tLoss: 0.737363487376853\n",
      "Epoch number:  10 \tLoss: 0.40161203330315604\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301107810112504\n",
      "Epoch number:  4 \tLoss: 2.300732394458833\n",
      "Epoch number:  6 \tLoss: 2.3005471030985\n",
      "Epoch number:  8 \tLoss: 2.3004232587187796\n",
      "Epoch number:  10 \tLoss: 2.300329549474808\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3004628344506926\n",
      "Epoch number:  4 \tLoss: 2.300210873900667\n",
      "Epoch number:  6 \tLoss: 0.8178889817162864\n",
      "Epoch number:  8 \tLoss: 0.39943967454423385\n",
      "Epoch number:  10 \tLoss: 0.3311320996849159\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2998813911755134\n",
      "Epoch number:  4 \tLoss: 2.2996845122604306\n",
      "Epoch number:  6 \tLoss: 2.299538314522763\n",
      "Epoch number:  8 \tLoss: 2.2994369984456418\n",
      "Epoch number:  10 \tLoss: 2.2993638991979597\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010671042118087\n",
      "Epoch number:  4 \tLoss: 2.300775156145012\n",
      "Epoch number:  6 \tLoss: 2.300261977837274\n",
      "Epoch number:  8 \tLoss: 1.672993007590437\n",
      "Epoch number:  10 \tLoss: 0.6731513220530858\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.30171623075897\n",
      "Epoch number:  4 \tLoss: 2.3015234799539064\n",
      "Epoch number:  6 \tLoss: 2.3014164782720354\n",
      "Epoch number:  8 \tLoss: 2.3013263815065943\n",
      "Epoch number:  10 \tLoss: 2.3012417572103923\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3006164910451603\n",
      "Epoch number:  4 \tLoss: 2.300322004314313\n",
      "Epoch number:  6 \tLoss: 2.2995872760538063\n",
      "Epoch number:  8 \tLoss: 1.7398781501394451\n",
      "Epoch number:  10 \tLoss: 0.5028894402629538\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.30111202148744\n",
      "Epoch number:  4 \tLoss: 2.3007422321157334\n",
      "Epoch number:  6 \tLoss: 2.300558878667569\n",
      "Epoch number:  8 \tLoss: 2.3004360506295995\n",
      "Epoch number:  10 \tLoss: 2.3003430529235236\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3002583299386425\n",
      "Epoch number:  4 \tLoss: 2.300057693619294\n",
      "Epoch number:  6 \tLoss: 0.7130247086696702\n",
      "Epoch number:  8 \tLoss: 0.376319901968981\n",
      "Epoch number:  10 \tLoss: 0.3317893992646783\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2998747225142986\n",
      "Epoch number:  4 \tLoss: 2.2996817249013435\n",
      "Epoch number:  6 \tLoss: 2.2995359026864306\n",
      "Epoch number:  8 \tLoss: 2.299433980709774\n",
      "Epoch number:  10 \tLoss: 2.299359954684089\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301240321996154\n",
      "Epoch number:  4 \tLoss: 2.3009869271511976\n",
      "Epoch number:  6 \tLoss: 2.3005416496416924\n",
      "Epoch number:  8 \tLoss: 1.7979016420364582\n",
      "Epoch number:  10 \tLoss: 1.6825192518816763\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301665312838745\n",
      "Epoch number:  4 \tLoss: 2.3017130517158133\n",
      "Epoch number:  6 \tLoss: 2.301719913433776\n",
      "Epoch number:  8 \tLoss: 2.301715991592694\n",
      "Epoch number:  10 \tLoss: 2.301707733645032\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301132441809529\n",
      "Epoch number:  4 \tLoss: 2.30094174559824\n",
      "Epoch number:  6 \tLoss: 2.300334540366385\n",
      "Epoch number:  8 \tLoss: 1.8840843492843975\n",
      "Epoch number:  10 \tLoss: 1.6582751131447626\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3017976530116124\n",
      "Epoch number:  4 \tLoss: 2.301634793626691\n",
      "Epoch number:  6 \tLoss: 2.3015884484371023\n",
      "Epoch number:  8 \tLoss: 2.3015816360149404\n",
      "Epoch number:  10 \tLoss: 2.301593176551263\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3007513654109886\n",
      "Epoch number:  4 \tLoss: 2.3005552362432793\n",
      "Epoch number:  6 \tLoss: 2.3000431911129064\n",
      "Epoch number:  8 \tLoss: 2.2982249973792532\n",
      "Epoch number:  10 \tLoss: 1.6830679487561784\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300507016076492\n",
      "Epoch number:  4 \tLoss: 2.3003551942742666\n",
      "Epoch number:  6 \tLoss: 2.3003094233407504\n",
      "Epoch number:  8 \tLoss: 2.30030699490265\n",
      "Epoch number:  10 \tLoss: 2.300327133155802\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3012534755498573\n",
      "Epoch number:  4 \tLoss: 2.3006428592415884\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301680149089208\n",
      "Epoch number:  4 \tLoss: 2.3011924821643976\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3007737503101278\n",
      "Epoch number:  4 \tLoss: 2.3005634353955187\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3015620617495642\n",
      "Epoch number:  4 \tLoss: 2.3014032008898\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3014621845816325\n",
      "Epoch number:  4 \tLoss: 2.301168952248695\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301744057834699\n",
      "Epoch number:  4 \tLoss: 2.301603977318461\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301232204565372\n",
      "Epoch number:  4 \tLoss: 2.30062872255698\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3016702763183683\n",
      "Epoch number:  4 \tLoss: 2.3011938854488054\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3008378195924437\n",
      "Epoch number:  4 \tLoss: 2.300717941823578\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301555246892229\n",
      "Epoch number:  4 \tLoss: 2.3014048061286334\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010897851762624\n",
      "Epoch number:  4 \tLoss: 2.2919735221338624\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3017283803423423\n",
      "Epoch number:  4 \tLoss: 2.3015982733493976\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301655762578383\n",
      "Epoch number:  4 \tLoss: 2.3010560958084807\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.302052125649044\n",
      "Epoch number:  4 \tLoss: 2.301659376362016\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301267648942951\n",
      "Epoch number:  4 \tLoss: 2.300986923456976\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301959430209973\n",
      "Epoch number:  4 \tLoss: 2.301764087499357\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301905259671932\n",
      "Epoch number:  4 \tLoss: 2.301584056723053\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3022339506858613\n",
      "Epoch number:  4 \tLoss: 2.302010065609594\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3012623469430173\n",
      "Epoch number:  4 \tLoss: 2.300467251455228\n",
      "Epoch number:  6 \tLoss: 2.3002422397650695\n",
      "Epoch number:  8 \tLoss: 2.300082944755794\n",
      "Epoch number:  10 \tLoss: 2.2998010646346243\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3016866448925857\n",
      "Epoch number:  4 \tLoss: 2.301196576102785\n",
      "Epoch number:  6 \tLoss: 2.3011169807438576\n",
      "Epoch number:  8 \tLoss: 2.301150003081801\n",
      "Epoch number:  10 \tLoss: 2.3012062775870286\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301169342044154\n",
      "Epoch number:  4 \tLoss: 2.3009300435217908\n",
      "Epoch number:  6 \tLoss: 2.300785284091839\n",
      "Epoch number:  8 \tLoss: 0.8222330694922692\n",
      "Epoch number:  10 \tLoss: 0.4015037734833847\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3015560735097624\n",
      "Epoch number:  4 \tLoss: 2.301405466667431\n",
      "Epoch number:  6 \tLoss: 2.3015332331542617\n",
      "Epoch number:  8 \tLoss: 2.301650251125195\n",
      "Epoch number:  10 \tLoss: 2.3017248777655586\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3016550712802384\n",
      "Epoch number:  4 \tLoss: 2.301256569435779\n",
      "Epoch number:  6 \tLoss: 0.5835564428855113\n",
      "Epoch number:  8 \tLoss: 0.3584986587882803\n",
      "Epoch number:  10 \tLoss: 0.30515621180024777\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301750456002208\n",
      "Epoch number:  4 \tLoss: 2.3016110237155387\n",
      "Epoch number:  6 \tLoss: 2.301697915452015\n",
      "Epoch number:  8 \tLoss: 2.301750263319677\n",
      "Epoch number:  10 \tLoss: 2.301764193345643\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301360551386655\n",
      "Epoch number:  4 \tLoss: 2.3007450308943915\n",
      "Epoch number:  6 \tLoss: 2.3005787963056745\n",
      "Epoch number:  8 \tLoss: 2.3004351286610527\n",
      "Epoch number:  10 \tLoss: 2.300165293961722\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3016850475946695\n",
      "Epoch number:  4 \tLoss: 2.301195901324885\n",
      "Epoch number:  6 \tLoss: 2.3011181217803496\n",
      "Epoch number:  8 \tLoss: 2.301152442219849\n",
      "Epoch number:  10 \tLoss: 2.3012095029926907\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3008874869359603\n",
      "Epoch number:  4 \tLoss: 2.3006884128836917\n",
      "Epoch number:  6 \tLoss: 2.300648973041137\n",
      "Epoch number:  8 \tLoss: 2.300537856447791\n",
      "Epoch number:  10 \tLoss: 0.7009682791761933\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3015672976598625\n",
      "Epoch number:  4 \tLoss: 2.3014114337357765\n",
      "Epoch number:  6 \tLoss: 2.3015366110875917\n",
      "Epoch number:  8 \tLoss: 2.301652361891199\n",
      "Epoch number:  10 \tLoss: 2.3017263730984974\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301414250494788\n",
      "Epoch number:  4 \tLoss: 2.301126503995438\n",
      "Epoch number:  6 \tLoss: 0.5880496180628657\n",
      "Epoch number:  8 \tLoss: 0.3650598081890405\n",
      "Epoch number:  10 \tLoss: 0.29657646190039333\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301755441108137\n",
      "Epoch number:  4 \tLoss: 2.3016124689335262\n",
      "Epoch number:  6 \tLoss: 2.3016993891062763\n",
      "Epoch number:  8 \tLoss: 2.301752044384739\n",
      "Epoch number:  10 \tLoss: 2.30176614455118\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.30165345615977\n",
      "Epoch number:  4 \tLoss: 2.301005273400517\n",
      "Epoch number:  6 \tLoss: 2.3007541559476823\n",
      "Epoch number:  8 \tLoss: 2.3005263342121354\n",
      "Epoch number:  10 \tLoss: 2.300140789565133\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3020495669362835\n",
      "Epoch number:  4 \tLoss: 2.301658062035674\n",
      "Epoch number:  6 \tLoss: 2.3015228014246754\n",
      "Epoch number:  8 \tLoss: 2.3014703749638823\n",
      "Epoch number:  10 \tLoss: 2.301447376383194\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3011915688864315\n",
      "Epoch number:  4 \tLoss: 2.300882933927015\n",
      "Epoch number:  6 \tLoss: 2.3007319231571275\n",
      "Epoch number:  8 \tLoss: 2.30051146243045\n",
      "Epoch number:  10 \tLoss: 2.299965747830572\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3019616927979296\n",
      "Epoch number:  4 \tLoss: 2.301766210071553\n",
      "Epoch number:  6 \tLoss: 2.301763991603533\n",
      "Epoch number:  8 \tLoss: 2.3017895047521626\n",
      "Epoch number:  10 \tLoss: 2.301812939705076\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3015556789500966\n",
      "Epoch number:  4 \tLoss: 2.3013219814575105\n",
      "Epoch number:  6 \tLoss: 2.3009639468631797\n",
      "Epoch number:  8 \tLoss: 0.5443931300552458\n",
      "Epoch number:  10 \tLoss: 0.3697554588380955\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3022368855225843\n",
      "Epoch number:  4 \tLoss: 2.3020105039115393\n",
      "Epoch number:  6 \tLoss: 2.3019866485949034\n",
      "Epoch number:  8 \tLoss: 2.3019888489953386\n",
      "Epoch number:  10 \tLoss: 2.301991723751443\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3009706688062113\n",
      "Epoch number:  4 \tLoss: 2.3008853594039564\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300969616091234\n",
      "Epoch number:  4 \tLoss: 2.3008844552336263\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.302412752544076\n",
      "Epoch number:  4 \tLoss: 2.3020910707758078\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.302416476912142\n",
      "Epoch number:  4 \tLoss: 2.302094017799461\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3050055913848837\n",
      "Epoch number:  4 \tLoss: 2.3038639837113823\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3050028277184977\n",
      "Epoch number:  4 \tLoss: 2.3038619235055946\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3009677825158015\n",
      "Epoch number:  4 \tLoss: 2.300882619052574\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3009655005663383\n",
      "Epoch number:  4 \tLoss: 2.300880339768177\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3024134461982313\n",
      "Epoch number:  4 \tLoss: 2.3020914515514583\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.302414137690048\n",
      "Epoch number:  4 \tLoss: 2.30209203600308\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.304999031168582\n",
      "Epoch number:  4 \tLoss: 2.303859870252289\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.305000023411139\n",
      "Epoch number:  4 \tLoss: 2.3038602686444545\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2973335807892896\n",
      "Epoch number:  4 \tLoss: 2.2973299238208162\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.297333657558757\n",
      "Epoch number:  4 \tLoss: 2.297330052703349\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300362872173701\n",
      "Epoch number:  4 \tLoss: 2.3001811869290054\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3003671772293726\n",
      "Epoch number:  4 \tLoss: 2.300184207623361\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.30401190015513\n",
      "Epoch number:  4 \tLoss: 2.303223695824779\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3040080729361585\n",
      "Epoch number:  4 \tLoss: 2.3032210862133167\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300969511243378\n",
      "Epoch number:  4 \tLoss: 2.300884268171853\n",
      "Epoch number:  6 \tLoss: 2.3008040432345434\n",
      "Epoch number:  8 \tLoss: 2.3007285672870506\n",
      "Epoch number:  10 \tLoss: 2.30065758863982\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3009718416097917\n",
      "Epoch number:  4 \tLoss: 2.3008864136341773\n",
      "Epoch number:  6 \tLoss: 2.3008060102771126\n",
      "Epoch number:  8 \tLoss: 2.3007303651052107\n",
      "Epoch number:  10 \tLoss: 2.3006592270282766\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.302418824729134\n",
      "Epoch number:  4 \tLoss: 2.302096345846538\n",
      "Epoch number:  6 \tLoss: 2.3018110096967144\n",
      "Epoch number:  8 \tLoss: 2.3015591713780696\n",
      "Epoch number:  10 \tLoss: 2.301337538447204\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3024173567060187\n",
      "Epoch number:  4 \tLoss: 2.3020948055017807\n",
      "Epoch number:  6 \tLoss: 2.30180944063172\n",
      "Epoch number:  8 \tLoss: 2.301557615057325\n",
      "Epoch number:  10 \tLoss: 2.3013360490053865\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.305006339165412\n",
      "Epoch number:  4 \tLoss: 2.3038654381560355\n",
      "Epoch number:  6 \tLoss: 2.302970031621022\n",
      "Epoch number:  8 \tLoss: 2.3022772985271045\n",
      "Epoch number:  10 \tLoss: 2.301746140025155\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3050008398175152\n",
      "Epoch number:  4 \tLoss: 2.303859872194472\n",
      "Epoch number:  6 \tLoss: 2.3029649316678134\n",
      "Epoch number:  8 \tLoss: 2.3022736382187086\n",
      "Epoch number:  10 \tLoss: 2.3017462977012917\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3009661487822743\n",
      "Epoch number:  4 \tLoss: 2.3008810332191674\n",
      "Epoch number:  6 \tLoss: 2.300800946113595\n",
      "Epoch number:  8 \tLoss: 2.300725616572335\n",
      "Epoch number:  10 \tLoss: 2.3006547910199355\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300966956861824\n",
      "Epoch number:  4 \tLoss: 2.3008818155668127\n",
      "Epoch number:  6 \tLoss: 2.300801700954447\n",
      "Epoch number:  8 \tLoss: 2.3007263447601938\n",
      "Epoch number:  10 \tLoss: 2.3006554942873274\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3024170353667825\n",
      "Epoch number:  4 \tLoss: 2.302094806747361\n",
      "Epoch number:  6 \tLoss: 2.3018097273145535\n",
      "Epoch number:  8 \tLoss: 2.3015581244194756\n",
      "Epoch number:  10 \tLoss: 2.301336685522524\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3024142426897725\n",
      "Epoch number:  4 \tLoss: 2.302091991124308\n",
      "Epoch number:  6 \tLoss: 2.3018069668645387\n",
      "Epoch number:  8 \tLoss: 2.301555507528707\n",
      "Epoch number:  10 \tLoss: 2.3013343186240984\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.305000129383683\n",
      "Epoch number:  4 \tLoss: 2.3038599452768738\n",
      "Epoch number:  6 \tLoss: 2.3029655144484185\n",
      "Epoch number:  8 \tLoss: 2.3022737363911205\n",
      "Epoch number:  10 \tLoss: 2.301743333353394\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3049978619481872\n",
      "Epoch number:  4 \tLoss: 2.3038578512486816\n",
      "Epoch number:  6 \tLoss: 2.3029638962279066\n",
      "Epoch number:  8 \tLoss: 2.3022734985522906\n",
      "Epoch number:  10 \tLoss: 2.3017468955462275\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2973325209988005\n",
      "Epoch number:  4 \tLoss: 2.2973290915377023\n",
      "Epoch number:  6 \tLoss: 2.2973273381856307\n",
      "Epoch number:  8 \tLoss: 2.297326773134508\n",
      "Epoch number:  10 \tLoss: 2.2973270492865154\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2973352005236993\n",
      "Epoch number:  4 \tLoss: 2.297331147728496\n",
      "Epoch number:  6 \tLoss: 2.297328933710195\n",
      "Epoch number:  8 \tLoss: 2.2973280271449976\n",
      "Epoch number:  10 \tLoss: 2.2973280492406123\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3003658729629133\n",
      "Epoch number:  4 \tLoss: 2.300183185575043\n",
      "Epoch number:  6 \tLoss: 2.3000553550657483\n",
      "Epoch number:  8 \tLoss: 2.2999653027572258\n",
      "Epoch number:  10 \tLoss: 2.2999014816147483\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3003660020060965\n",
      "Epoch number:  4 \tLoss: 2.3001833184927336\n",
      "Epoch number:  6 \tLoss: 2.300055469745066\n",
      "Epoch number:  8 \tLoss: 2.2999653937549005\n",
      "Epoch number:  10 \tLoss: 2.299901550118883\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3040085237001082\n",
      "Epoch number:  4 \tLoss: 2.303221098197496\n",
      "Epoch number:  6 \tLoss: 2.302713399469952\n",
      "Epoch number:  8 \tLoss: 2.302382120992594\n",
      "Epoch number:  10 \tLoss: 2.302163059905096\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3040092811906034\n",
      "Epoch number:  4 \tLoss: 2.3032217788835845\n",
      "Epoch number:  6 \tLoss: 2.3027139675630726\n",
      "Epoch number:  8 \tLoss: 2.3023826066968804\n",
      "Epoch number:  10 \tLoss: 2.3021635226842445\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.30092315094139\n",
      "Epoch number:  4 \tLoss: 2.3008360683917415\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3009242473802014\n",
      "Epoch number:  4 \tLoss: 2.300837218151985\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3023656278629216\n",
      "Epoch number:  4 \tLoss: 2.3020286335770357\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3023711481875346\n",
      "Epoch number:  4 \tLoss: 2.3020339544784956\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3050438524248573\n",
      "Epoch number:  4 \tLoss: 2.3038006729850307\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.305052564435413\n",
      "Epoch number:  4 \tLoss: 2.3038080146358473\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3009234801593816\n",
      "Epoch number:  4 \tLoss: 2.3008364647717596\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300925410079642\n",
      "Epoch number:  4 \tLoss: 2.3008382582618623\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.302371083943867\n",
      "Epoch number:  4 \tLoss: 2.3020339269080297\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.302366498645611\n",
      "Epoch number:  4 \tLoss: 2.3020293546242394\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3050542726052257\n",
      "Epoch number:  4 \tLoss: 2.3038097101021977\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.305047338890983\n",
      "Epoch number:  4 \tLoss: 2.303802947443395\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2999752661389063\n",
      "Epoch number:  4 \tLoss: 2.299910014906765\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299972641385456\n",
      "Epoch number:  4 \tLoss: 2.299907702482654\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301826718991331\n",
      "Epoch number:  4 \tLoss: 2.301533154035792\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3018222603677603\n",
      "Epoch number:  4 \tLoss: 2.301529319035371\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3047772603074916\n",
      "Epoch number:  4 \tLoss: 2.3036472527633216\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.304767113702174\n",
      "Epoch number:  4 \tLoss: 2.303639494444685\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3009240011669507\n",
      "Epoch number:  4 \tLoss: 2.3008369528517254\n",
      "Epoch number:  6 \tLoss: 2.3007551220858407\n",
      "Epoch number:  8 \tLoss: 2.3006782065244664\n",
      "Epoch number:  10 \tLoss: 2.3006059264190957\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3009251680422764\n",
      "Epoch number:  4 \tLoss: 2.3008381134837914\n",
      "Epoch number:  6 \tLoss: 2.300756270006902\n",
      "Epoch number:  8 \tLoss: 2.3006793374645067\n",
      "Epoch number:  10 \tLoss: 2.3006070364911344\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.302371130769096\n",
      "Epoch number:  4 \tLoss: 2.302033693869208\n",
      "Epoch number:  6 \tLoss: 2.3017370553884575\n",
      "Epoch number:  8 \tLoss: 2.3014767563592935\n",
      "Epoch number:  10 \tLoss: 2.301248875135273\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3023723271238876\n",
      "Epoch number:  4 \tLoss: 2.3020347029191597\n",
      "Epoch number:  6 \tLoss: 2.301737889947302\n",
      "Epoch number:  8 \tLoss: 2.301477425564584\n",
      "Epoch number:  10 \tLoss: 2.3012494050370176\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.305043712624698\n",
      "Epoch number:  4 \tLoss: 2.3038010094198658\n",
      "Epoch number:  6 \tLoss: 2.3028487101737984\n",
      "Epoch number:  8 \tLoss: 2.302126464461169\n",
      "Epoch number:  10 \tLoss: 2.3015818025572643\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.305052600398993\n",
      "Epoch number:  4 \tLoss: 2.3038075667184748\n",
      "Epoch number:  6 \tLoss: 2.3028538259837914\n",
      "Epoch number:  8 \tLoss: 2.3021315619069327\n",
      "Epoch number:  10 \tLoss: 2.30158975567984\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3009220589786907\n",
      "Epoch number:  4 \tLoss: 2.300834920620786\n",
      "Epoch number:  6 \tLoss: 2.3007530266542906\n",
      "Epoch number:  8 \tLoss: 2.30067607233102\n",
      "Epoch number:  10 \tLoss: 2.3006037759275486\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300922803592798\n",
      "Epoch number:  4 \tLoss: 2.30083586710583\n",
      "Epoch number:  6 \tLoss: 2.300754141599748\n",
      "Epoch number:  8 \tLoss: 2.300677326648171\n",
      "Epoch number:  10 \tLoss: 2.3006051426196295\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3023686206637217\n",
      "Epoch number:  4 \tLoss: 2.3020313433535917\n",
      "Epoch number:  6 \tLoss: 2.301734874773949\n",
      "Epoch number:  8 \tLoss: 2.3014747510707765\n",
      "Epoch number:  10 \tLoss: 2.301247045597828\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3023687640231\n",
      "Epoch number:  4 \tLoss: 2.3020315633260093\n",
      "Epoch number:  6 \tLoss: 2.301735148359521\n",
      "Epoch number:  8 \tLoss: 2.301475057967644\n",
      "Epoch number:  10 \tLoss: 2.30124738656301\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3050530734618055\n",
      "Epoch number:  4 \tLoss: 2.3038074144288374\n",
      "Epoch number:  6 \tLoss: 2.3028532468521785\n",
      "Epoch number:  8 \tLoss: 2.302129939522807\n",
      "Epoch number:  10 \tLoss: 2.3015847573260935\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3050490659368323\n",
      "Epoch number:  4 \tLoss: 2.3038056216334155\n",
      "Epoch number:  6 \tLoss: 2.302852959477452\n",
      "Epoch number:  8 \tLoss: 2.3021313906687504\n",
      "Epoch number:  10 \tLoss: 2.301590007849046\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299975472363148\n",
      "Epoch number:  4 \tLoss: 2.2999102016329944\n",
      "Epoch number:  6 \tLoss: 2.299852930785795\n",
      "Epoch number:  8 \tLoss: 2.2998026214055063\n",
      "Epoch number:  10 \tLoss: 2.2997583703153537\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2999765014407743\n",
      "Epoch number:  4 \tLoss: 2.299911075613944\n",
      "Epoch number:  6 \tLoss: 2.2998536720234806\n",
      "Epoch number:  8 \tLoss: 2.299803250612718\n",
      "Epoch number:  10 \tLoss: 2.2997589056413728\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3018231280118915\n",
      "Epoch number:  4 \tLoss: 2.301530201298245\n",
      "Epoch number:  6 \tLoss: 2.30128800386292\n",
      "Epoch number:  8 \tLoss: 2.301087501097356\n",
      "Epoch number:  10 \tLoss: 2.300921247427149\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301822738716027\n",
      "Epoch number:  4 \tLoss: 2.3015297124740313\n",
      "Epoch number:  6 \tLoss: 2.3012874650488144\n",
      "Epoch number:  8 \tLoss: 2.3010869429499694\n",
      "Epoch number:  10 \tLoss: 2.300920690277997\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.304772025422836\n",
      "Epoch number:  4 \tLoss: 2.30364297986385\n",
      "Epoch number:  6 \tLoss: 2.302818349995502\n",
      "Epoch number:  8 \tLoss: 2.30221690032852\n",
      "Epoch number:  10 \tLoss: 2.301776813811343\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3047757205991743\n",
      "Epoch number:  4 \tLoss: 2.3036458333436487\n",
      "Epoch number:  6 \tLoss: 2.3028206427014153\n",
      "Epoch number:  8 \tLoss: 2.302219163576758\n",
      "Epoch number:  10 \tLoss: 2.3017801096351302\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300913597873027\n",
      "Epoch number:  4 \tLoss: 2.300824034083493\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3009136693351087\n",
      "Epoch number:  4 \tLoss: 2.3008240005711253\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3023722603706216\n",
      "Epoch number:  4 \tLoss: 2.3020091284108846\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3023724971882515\n",
      "Epoch number:  4 \tLoss: 2.3020100739354334\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.305198333469179\n",
      "Epoch number:  4 \tLoss: 2.30376292750828\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3052004742633057\n",
      "Epoch number:  4 \tLoss: 2.3037642534569134\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300914785042383\n",
      "Epoch number:  4 \tLoss: 2.300825021286611\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300913068220042\n",
      "Epoch number:  4 \tLoss: 2.3008235075475088\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3023741445237977\n",
      "Epoch number:  4 \tLoss: 2.3020108637746475\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.302378069360266\n",
      "Epoch number:  4 \tLoss: 2.302014612128273\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3052058934788326\n",
      "Epoch number:  4 \tLoss: 2.303769248209319\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.305204869015983\n",
      "Epoch number:  4 \tLoss: 2.3037680404171277\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3006962303085485\n",
      "Epoch number:  4 \tLoss: 2.3006127875072444\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300695548893716\n",
      "Epoch number:  4 \tLoss: 2.3006121890569124\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3022498777123586\n",
      "Epoch number:  4 \tLoss: 2.3019002874743304\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.302253285565243\n",
      "Epoch number:  4 \tLoss: 2.3019033732595164\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3051437026453296\n",
      "Epoch number:  4 \tLoss: 2.303743854530785\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3051510392543113\n",
      "Epoch number:  4 \tLoss: 2.3037492083612134\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300915455152602\n",
      "Epoch number:  4 \tLoss: 2.3008257986653438\n",
      "Epoch number:  6 \tLoss: 2.3007418607668875\n",
      "Epoch number:  8 \tLoss: 2.3006632741337927\n",
      "Epoch number:  10 \tLoss: 2.3005897039606564\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300912995480748\n",
      "Epoch number:  4 \tLoss: 2.3008234361757074\n",
      "Epoch number:  6 \tLoss: 2.300739601793985\n",
      "Epoch number:  8 \tLoss: 2.3006611224295903\n",
      "Epoch number:  10 \tLoss: 2.3005876598875443\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3023720506426675\n",
      "Epoch number:  4 \tLoss: 2.302009281614146\n",
      "Epoch number:  6 \tLoss: 2.301695708218742\n",
      "Epoch number:  8 \tLoss: 2.3014246773645346\n",
      "Epoch number:  10 \tLoss: 2.3011906234147808\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.302374081435735\n",
      "Epoch number:  4 \tLoss: 2.302011237090737\n",
      "Epoch number:  6 \tLoss: 2.301697552577849\n",
      "Epoch number:  8 \tLoss: 2.3014263773701944\n",
      "Epoch number:  10 \tLoss: 2.301192180047844\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.305204021865555\n",
      "Epoch number:  4 \tLoss: 2.303767258193019\n",
      "Epoch number:  6 \tLoss: 2.302728610911414\n",
      "Epoch number:  8 \tLoss: 2.3019769325145925\n",
      "Epoch number:  10 \tLoss: 2.3014300139459256\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3052010029581784\n",
      "Epoch number:  4 \tLoss: 2.3037652826856037\n",
      "Epoch number:  6 \tLoss: 2.3027273112946087\n",
      "Epoch number:  8 \tLoss: 2.3019771941418643\n",
      "Epoch number:  10 \tLoss: 2.301435119887644\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3009146256036845\n",
      "Epoch number:  4 \tLoss: 2.3008249099082443\n",
      "Epoch number:  6 \tLoss: 2.3007409361140843\n",
      "Epoch number:  8 \tLoss: 2.300662332038146\n",
      "Epoch number:  10 \tLoss: 2.3005887587388285\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300915132974776\n",
      "Epoch number:  4 \tLoss: 2.300825385986635\n",
      "Epoch number:  6 \tLoss: 2.3007413810673656\n",
      "Epoch number:  8 \tLoss: 2.3006627471443757\n",
      "Epoch number:  10 \tLoss: 2.3005891448008895\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3023750192700176\n",
      "Epoch number:  4 \tLoss: 2.3020120086164004\n",
      "Epoch number:  6 \tLoss: 2.301698206330735\n",
      "Epoch number:  8 \tLoss: 2.301426959306393\n",
      "Epoch number:  10 \tLoss: 2.3011927040216364\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3023768511836273\n",
      "Epoch number:  4 \tLoss: 2.302013750333919\n",
      "Epoch number:  6 \tLoss: 2.301699831225567\n",
      "Epoch number:  8 \tLoss: 2.301428439319449\n",
      "Epoch number:  10 \tLoss: 2.3011940408560134\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.305201900130508\n",
      "Epoch number:  4 \tLoss: 2.3037649153450115\n",
      "Epoch number:  6 \tLoss: 2.302726158260013\n",
      "Epoch number:  8 \tLoss: 2.301974306024248\n",
      "Epoch number:  10 \tLoss: 2.301427086529392\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3052017963370752\n",
      "Epoch number:  4 \tLoss: 2.303765835536998\n",
      "Epoch number:  6 \tLoss: 2.302727697247592\n",
      "Epoch number:  8 \tLoss: 2.301977469998093\n",
      "Epoch number:  10 \tLoss: 2.3014353283508178\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300695150434833\n",
      "Epoch number:  4 \tLoss: 2.3006117790155853\n",
      "Epoch number:  6 \tLoss: 2.3005352456568913\n",
      "Epoch number:  8 \tLoss: 2.3004649829306305\n",
      "Epoch number:  10 \tLoss: 2.300400472695404\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300696914634752\n",
      "Epoch number:  4 \tLoss: 2.300613488926508\n",
      "Epoch number:  6 \tLoss: 2.3005368958304913\n",
      "Epoch number:  8 \tLoss: 2.3004665698455784\n",
      "Epoch number:  10 \tLoss: 2.3004019932436917\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3022507113744206\n",
      "Epoch number:  4 \tLoss: 2.30190091769012\n",
      "Epoch number:  6 \tLoss: 2.30160345437252\n",
      "Epoch number:  8 \tLoss: 2.301350355757566\n",
      "Epoch number:  10 \tLoss: 2.301135007396558\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3022508069813545\n",
      "Epoch number:  4 \tLoss: 2.3019013085480737\n",
      "Epoch number:  6 \tLoss: 2.301604044676864\n",
      "Epoch number:  8 \tLoss: 2.301351076108206\n",
      "Epoch number:  10 \tLoss: 2.301135829315743\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.305146564078465\n",
      "Epoch number:  4 \tLoss: 2.3037450437463494\n",
      "Epoch number:  6 \tLoss: 2.302743454595805\n",
      "Epoch number:  8 \tLoss: 2.3020254355713883\n",
      "Epoch number:  10 \tLoss: 2.3015070705524088\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.305149297017295\n",
      "Epoch number:  4 \tLoss: 2.3037475910950143\n",
      "Epoch number:  6 \tLoss: 2.3027460036165546\n",
      "Epoch number:  8 \tLoss: 2.3020291174998624\n",
      "Epoch number:  10 \tLoss: 2.301514795007628\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2996599136365483\n",
      "Epoch number:  4 \tLoss: 2.2981056566756655\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.30002536547152\n",
      "Epoch number:  4 \tLoss: 2.299427420116063\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.298844429456849\n",
      "Epoch number:  4 \tLoss: 2.29652228678808\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.298233001415659\n",
      "Epoch number:  4 \tLoss: 2.297665358632985\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.298040079199592\n",
      "Epoch number:  4 \tLoss: 2.2958729109324083\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.296364550061686\n",
      "Epoch number:  4 \tLoss: 2.295717012821711\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2996447492961574\n",
      "Epoch number:  4 \tLoss: 2.2980557517853795\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3000277163087577\n",
      "Epoch number:  4 \tLoss: 2.2994324485050295\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2988369947441156\n",
      "Epoch number:  4 \tLoss: 2.296527500634239\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2982358284777606\n",
      "Epoch number:  4 \tLoss: 2.297671656089765\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.298057538524908\n",
      "Epoch number:  4 \tLoss: 2.295945345328106\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.296367219517155\n",
      "Epoch number:  4 \tLoss: 2.295723209629216\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.302659588241434\n",
      "Epoch number:  4 \tLoss: 2.3023407317094344\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3026953453989187\n",
      "Epoch number:  4 \tLoss: 2.30241194368481\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301846726951118\n",
      "Epoch number:  4 \tLoss: 2.300954644126694\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3020488210814594\n",
      "Epoch number:  4 \tLoss: 2.3018803544833726\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299495984722917\n",
      "Epoch number:  4 \tLoss: 2.298085415406419\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.29878418403968\n",
      "Epoch number:  4 \tLoss: 2.298715187397264\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299643070036664\n",
      "Epoch number:  4 \tLoss: 2.298096958083536\n",
      "Epoch number:  6 \tLoss: 1.7801425473928199\n",
      "Epoch number:  8 \tLoss: 1.6647125832354621\n",
      "Epoch number:  10 \tLoss: 1.6374667324548995\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3000240026810768\n",
      "Epoch number:  4 \tLoss: 2.299426473240455\n",
      "Epoch number:  6 \tLoss: 2.2991506934998713\n",
      "Epoch number:  8 \tLoss: 2.2989724362562725\n",
      "Epoch number:  10 \tLoss: 2.298845743879348\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.298849786060914\n",
      "Epoch number:  4 \tLoss: 2.2965399522116803\n",
      "Epoch number:  6 \tLoss: 1.8222245558463102\n",
      "Epoch number:  8 \tLoss: 1.621777007268591\n",
      "Epoch number:  10 \tLoss: 0.8132046620903465\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2982330425525426\n",
      "Epoch number:  4 \tLoss: 2.2976655286093166\n",
      "Epoch number:  6 \tLoss: 2.297331520654555\n",
      "Epoch number:  8 \tLoss: 2.297134666565275\n",
      "Epoch number:  10 \tLoss: 2.2970194979777654\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2980245537764126\n",
      "Epoch number:  4 \tLoss: 2.2958516354306044\n",
      "Epoch number:  6 \tLoss: 2.269315787486029\n",
      "Epoch number:  8 \tLoss: 1.6521420459519893\n",
      "Epoch number:  10 \tLoss: 0.8833227980929024\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2963632479550466\n",
      "Epoch number:  4 \tLoss: 2.2957157953255765\n",
      "Epoch number:  6 \tLoss: 2.2954334468901845\n",
      "Epoch number:  8 \tLoss: 2.295333227277024\n",
      "Epoch number:  10 \tLoss: 2.295320764983953\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2996523190149127\n",
      "Epoch number:  4 \tLoss: 2.2980848087335386\n",
      "Epoch number:  6 \tLoss: 1.7475684551203856\n",
      "Epoch number:  8 \tLoss: 1.665473507987262\n",
      "Epoch number:  10 \tLoss: 1.6511727860704115\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300025645992972\n",
      "Epoch number:  4 \tLoss: 2.2994295582808264\n",
      "Epoch number:  6 \tLoss: 2.299155542133793\n",
      "Epoch number:  8 \tLoss: 2.2989790885841836\n",
      "Epoch number:  10 \tLoss: 2.2988541045864577\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.298849263382102\n",
      "Epoch number:  4 \tLoss: 2.2965368751531714\n",
      "Epoch number:  6 \tLoss: 1.8089413447111442\n",
      "Epoch number:  8 \tLoss: 1.6237708898458554\n",
      "Epoch number:  10 \tLoss: 0.9679206746037847\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.298235776218625\n",
      "Epoch number:  4 \tLoss: 2.2976717922444436\n",
      "Epoch number:  6 \tLoss: 2.297340082810954\n",
      "Epoch number:  8 \tLoss: 2.2971442326332445\n",
      "Epoch number:  10 \tLoss: 2.2970292026236754\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2980291632245287\n",
      "Epoch number:  4 \tLoss: 2.295831521764032\n",
      "Epoch number:  6 \tLoss: 2.268588949305398\n",
      "Epoch number:  8 \tLoss: 1.5973395322496424\n",
      "Epoch number:  10 \tLoss: 1.0451103134031883\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.296366363076446\n",
      "Epoch number:  4 \tLoss: 2.295722472442408\n",
      "Epoch number:  6 \tLoss: 2.2954400057084\n",
      "Epoch number:  8 \tLoss: 2.295337672372384\n",
      "Epoch number:  10 \tLoss: 2.295322112955666\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3026596093017195\n",
      "Epoch number:  4 \tLoss: 2.3023407943487313\n",
      "Epoch number:  6 \tLoss: 2.3020454054292014\n",
      "Epoch number:  8 \tLoss: 2.301731135229135\n",
      "Epoch number:  10 \tLoss: 2.301375599523489\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.302696710829697\n",
      "Epoch number:  4 \tLoss: 2.302412918701723\n",
      "Epoch number:  6 \tLoss: 2.3021570797356707\n",
      "Epoch number:  8 \tLoss: 2.301902893485954\n",
      "Epoch number:  10 \tLoss: 2.301649729347274\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301846365904834\n",
      "Epoch number:  4 \tLoss: 2.3009591983884756\n",
      "Epoch number:  6 \tLoss: 2.294951901406069\n",
      "Epoch number:  8 \tLoss: 1.7116077180541525\n",
      "Epoch number:  10 \tLoss: 1.6986524115778772\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.302050657809919\n",
      "Epoch number:  4 \tLoss: 2.3018822045748966\n",
      "Epoch number:  6 \tLoss: 2.301809768436896\n",
      "Epoch number:  8 \tLoss: 2.301746243987534\n",
      "Epoch number:  10 \tLoss: 2.301683524879158\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299498239289444\n",
      "Epoch number:  4 \tLoss: 2.298108472662238\n",
      "Epoch number:  6 \tLoss: 2.006469156405953\n",
      "Epoch number:  8 \tLoss: 1.7432040897785748\n",
      "Epoch number:  10 \tLoss: 1.7355149388837543\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.298782854668952\n",
      "Epoch number:  4 \tLoss: 2.2987140792952205\n",
      "Epoch number:  6 \tLoss: 2.298782537407965\n",
      "Epoch number:  8 \tLoss: 2.2988755558664637\n",
      "Epoch number:  10 \tLoss: 2.298981604098239\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301551975856185\n",
      "Epoch number:  4 \tLoss: 2.301476920459991\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.302161890149169\n",
      "Epoch number:  4 \tLoss: 2.30221173113751\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.30142841748324\n",
      "Epoch number:  4 \tLoss: 2.3011861006286076\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3017715927794105\n",
      "Epoch number:  4 \tLoss: 2.3012006131624303\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300048318563446\n",
      "Epoch number:  4 \tLoss: 2.3001129402592215\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299393485649323\n",
      "Epoch number:  4 \tLoss: 2.2994899961114132\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3015513230457407\n",
      "Epoch number:  4 \tLoss: 2.3014627890717456\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3021613970793045\n",
      "Epoch number:  4 \tLoss: 2.3022114922272405\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301427399530482\n",
      "Epoch number:  4 \tLoss: 2.3011815659665062\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301771351283022\n",
      "Epoch number:  4 \tLoss: 2.301200861322085\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3000411662512623\n",
      "Epoch number:  4 \tLoss: 2.3001107856987595\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2993920936108094\n",
      "Epoch number:  4 \tLoss: 2.2994891093065113\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301504825847031\n",
      "Epoch number:  4 \tLoss: 2.30130809974361\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3019114690090494\n",
      "Epoch number:  4 \tLoss: 2.302069105337515\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301977283016606\n",
      "Epoch number:  4 \tLoss: 2.3017258455221166\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.302574650173494\n",
      "Epoch number:  4 \tLoss: 2.3023329232304492\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300325270231881\n",
      "Epoch number:  4 \tLoss: 2.3000061940614325\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.29977965749942\n",
      "Epoch number:  4 \tLoss: 2.299395892586019\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3015422671407633\n",
      "Epoch number:  4 \tLoss: 2.3014562025687595\n",
      "Epoch number:  6 \tLoss: 2.301212634683884\n",
      "Epoch number:  8 \tLoss: 2.2997734305060575\n",
      "Epoch number:  10 \tLoss: 2.1041577536499827\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3021620441763213\n",
      "Epoch number:  4 \tLoss: 2.3022125687791215\n",
      "Epoch number:  6 \tLoss: 2.302231222202771\n",
      "Epoch number:  8 \tLoss: 2.3021866555901247\n",
      "Epoch number:  10 \tLoss: 2.30210074843572\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3014260978072674\n",
      "Epoch number:  4 \tLoss: 2.301178182807073\n",
      "Epoch number:  6 \tLoss: 2.300406326044522\n",
      "Epoch number:  8 \tLoss: 2.297707580124548\n",
      "Epoch number:  10 \tLoss: 1.8654386603581066\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3017691327221192\n",
      "Epoch number:  4 \tLoss: 2.3011974888053346\n",
      "Epoch number:  6 \tLoss: 2.300884131598939\n",
      "Epoch number:  8 \tLoss: 2.3006908803768185\n",
      "Epoch number:  10 \tLoss: 2.3005663822421365\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300045853772456\n",
      "Epoch number:  4 \tLoss: 2.3001098343119026\n",
      "Epoch number:  6 \tLoss: 2.299580630171457\n",
      "Epoch number:  8 \tLoss: 2.2974516050446208\n",
      "Epoch number:  10 \tLoss: 2.288345376613756\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299393487216034\n",
      "Epoch number:  4 \tLoss: 2.2994897853803455\n",
      "Epoch number:  6 \tLoss: 2.2995673214614127\n",
      "Epoch number:  8 \tLoss: 2.299599219466703\n",
      "Epoch number:  10 \tLoss: 2.2996090444672794\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3015440560348726\n",
      "Epoch number:  4 \tLoss: 2.3014640580274643\n",
      "Epoch number:  6 \tLoss: 2.301219928799853\n",
      "Epoch number:  8 \tLoss: 2.2997691362792425\n",
      "Epoch number:  10 \tLoss: 2.2266718336986786\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3021626181234884\n",
      "Epoch number:  4 \tLoss: 2.3022126026129093\n",
      "Epoch number:  6 \tLoss: 2.302230660099001\n",
      "Epoch number:  8 \tLoss: 2.302186119271756\n",
      "Epoch number:  10 \tLoss: 2.3021007825854105\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301413007676564\n",
      "Epoch number:  4 \tLoss: 2.3011632705624234\n",
      "Epoch number:  6 \tLoss: 2.3003859407643907\n",
      "Epoch number:  8 \tLoss: 2.2976771670593297\n",
      "Epoch number:  10 \tLoss: 1.783893076353767\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3017704821816145\n",
      "Epoch number:  4 \tLoss: 2.301199415170218\n",
      "Epoch number:  6 \tLoss: 2.3008865436461496\n",
      "Epoch number:  8 \tLoss: 2.300693616138729\n",
      "Epoch number:  10 \tLoss: 2.300569324183619\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300030564620527\n",
      "Epoch number:  4 \tLoss: 2.3001041743624753\n",
      "Epoch number:  6 \tLoss: 2.299580240800293\n",
      "Epoch number:  8 \tLoss: 2.2974501955874578\n",
      "Epoch number:  10 \tLoss: 2.288315540860935\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299394002499459\n",
      "Epoch number:  4 \tLoss: 2.2994899612377093\n",
      "Epoch number:  6 \tLoss: 2.299567619654934\n",
      "Epoch number:  8 \tLoss: 2.299599764918836\n",
      "Epoch number:  10 \tLoss: 2.2996098447350937\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3015085972504243\n",
      "Epoch number:  4 \tLoss: 2.3013108057562484\n",
      "Epoch number:  6 \tLoss: 2.3009668375574632\n",
      "Epoch number:  8 \tLoss: 2.299945699361195\n",
      "Epoch number:  10 \tLoss: 1.709884831447005\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3019099512220698\n",
      "Epoch number:  4 \tLoss: 2.3020680461571867\n",
      "Epoch number:  6 \tLoss: 2.302113339435124\n",
      "Epoch number:  8 \tLoss: 2.3021224168320775\n",
      "Epoch number:  10 \tLoss: 2.3021151427047726\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3019744762023433\n",
      "Epoch number:  4 \tLoss: 2.301723993716934\n",
      "Epoch number:  6 \tLoss: 2.3010054346114677\n",
      "Epoch number:  8 \tLoss: 2.2981229114925945\n",
      "Epoch number:  10 \tLoss: 1.6713235346402142\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3025734990280142\n",
      "Epoch number:  4 \tLoss: 2.3023319100258224\n",
      "Epoch number:  6 \tLoss: 2.3022460778827876\n",
      "Epoch number:  8 \tLoss: 2.302235370835982\n",
      "Epoch number:  10 \tLoss: 2.3022630592324234\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3003169132566037\n",
      "Epoch number:  4 \tLoss: 2.3000072818663027\n",
      "Epoch number:  6 \tLoss: 2.2992921892462075\n",
      "Epoch number:  8 \tLoss: 2.296718559411416\n",
      "Epoch number:  10 \tLoss: 1.704573015806856\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2997772731704953\n",
      "Epoch number:  4 \tLoss: 2.299394806994613\n",
      "Epoch number:  6 \tLoss: 2.2992854984076314\n",
      "Epoch number:  8 \tLoss: 2.2992820882253855\n",
      "Epoch number:  10 \tLoss: 2.299333227742937\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3015230871787336\n",
      "Epoch number:  4 \tLoss: 2.3005092667532945\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.30172935090762\n",
      "Epoch number:  4 \tLoss: 2.3005994162360768\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.30070245161614\n",
      "Epoch number:  4 \tLoss: 2.3003642411226966\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3006241642373895\n",
      "Epoch number:  4 \tLoss: 2.3006431719549423\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3019295336139285\n",
      "Epoch number:  4 \tLoss: 2.302374649361556\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.302158844591136\n",
      "Epoch number:  4 \tLoss: 2.3027039736955524\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.30151796949223\n",
      "Epoch number:  4 \tLoss: 2.3005108507070178\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3017279404924444\n",
      "Epoch number:  4 \tLoss: 2.3006002881892678\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3007020559997984\n",
      "Epoch number:  4 \tLoss: 2.300355818018257\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300623831738394\n",
      "Epoch number:  4 \tLoss: 2.300643212413794\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301939539314219\n",
      "Epoch number:  4 \tLoss: 2.3023720439381576\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3021569700130837\n",
      "Epoch number:  4 \tLoss: 2.3027022865527216\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3022317939020494\n",
      "Epoch number:  4 \tLoss: 2.3010409231898037\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3023702156653383\n",
      "Epoch number:  4 \tLoss: 2.3014446641540864\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.30103074317597\n",
      "Epoch number:  4 \tLoss: 2.3004263034815358\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3011237613583875\n",
      "Epoch number:  4 \tLoss: 2.3009244724608284\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.302190374689374\n",
      "Epoch number:  4 \tLoss: 2.302184688375138\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3026378920432022\n",
      "Epoch number:  4 \tLoss: 2.302807179383101\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3015184649265965\n",
      "Epoch number:  4 \tLoss: 2.300511744217522\n",
      "Epoch number:  6 \tLoss: 2.299894314414119\n",
      "Epoch number:  8 \tLoss: 2.2996862527613415\n",
      "Epoch number:  10 \tLoss: 2.2996073594385127\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3017206383345763\n",
      "Epoch number:  4 \tLoss: 2.300598124496291\n",
      "Epoch number:  6 \tLoss: 2.300261834294354\n",
      "Epoch number:  8 \tLoss: 2.300219947543061\n",
      "Epoch number:  10 \tLoss: 2.3003045490301357\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300702373259698\n",
      "Epoch number:  4 \tLoss: 2.3003652557461334\n",
      "Epoch number:  6 \tLoss: 2.3007999823927965\n",
      "Epoch number:  8 \tLoss: 2.301310925165389\n",
      "Epoch number:  10 \tLoss: 2.30166210493453\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3006233489869756\n",
      "Epoch number:  4 \tLoss: 2.300641242123614\n",
      "Epoch number:  6 \tLoss: 2.3011886568074424\n",
      "Epoch number:  8 \tLoss: 2.3017422920915616\n",
      "Epoch number:  10 \tLoss: 2.3021783976357186\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301928103356136\n",
      "Epoch number:  4 \tLoss: 2.302360759185049\n",
      "Epoch number:  6 \tLoss: 2.3030429511928285\n",
      "Epoch number:  8 \tLoss: 2.3034134598889158\n",
      "Epoch number:  10 \tLoss: 2.303518554887419\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.302156553612283\n",
      "Epoch number:  4 \tLoss: 2.3027019629736403\n",
      "Epoch number:  6 \tLoss: 2.3033330230695253\n",
      "Epoch number:  8 \tLoss: 2.3037514464712463\n",
      "Epoch number:  10 \tLoss: 2.3040099309154285\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.30150988825251\n",
      "Epoch number:  4 \tLoss: 2.300521941801608\n",
      "Epoch number:  6 \tLoss: 2.2998978640629355\n",
      "Epoch number:  8 \tLoss: 2.299688044303929\n",
      "Epoch number:  10 \tLoss: 2.2996072212633503\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3017233508873582\n",
      "Epoch number:  4 \tLoss: 2.300598211073827\n",
      "Epoch number:  6 \tLoss: 2.3002617367125273\n",
      "Epoch number:  8 \tLoss: 2.300220152700163\n",
      "Epoch number:  10 \tLoss: 2.300305119887797\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3007002828276244\n",
      "Epoch number:  4 \tLoss: 2.3003507924066926\n",
      "Epoch number:  6 \tLoss: 2.3007900171036186\n",
      "Epoch number:  8 \tLoss: 2.301302262840373\n",
      "Epoch number:  10 \tLoss: 2.3016537976034135\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3006236824582946\n",
      "Epoch number:  4 \tLoss: 2.3006421193404876\n",
      "Epoch number:  6 \tLoss: 2.3011893560652017\n",
      "Epoch number:  8 \tLoss: 2.3017427146727276\n",
      "Epoch number:  10 \tLoss: 2.3021786798302846\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3019263499504006\n",
      "Epoch number:  4 \tLoss: 2.302360355061314\n",
      "Epoch number:  6 \tLoss: 2.3030428080908214\n",
      "Epoch number:  8 \tLoss: 2.303411177614006\n",
      "Epoch number:  10 \tLoss: 2.3035137215295896\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.302162022392163\n",
      "Epoch number:  4 \tLoss: 2.3027071282174414\n",
      "Epoch number:  6 \tLoss: 2.303337954252322\n",
      "Epoch number:  8 \tLoss: 2.3037563263136698\n",
      "Epoch number:  10 \tLoss: 2.3040149186016023\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.302230148109985\n",
      "Epoch number:  4 \tLoss: 2.3010404828818842\n",
      "Epoch number:  6 \tLoss: 2.300491078963753\n",
      "Epoch number:  8 \tLoss: 2.3001981496841144\n",
      "Epoch number:  10 \tLoss: 2.2999348587698853\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.302370852973631\n",
      "Epoch number:  4 \tLoss: 2.301443203236018\n",
      "Epoch number:  6 \tLoss: 2.3010048959534366\n",
      "Epoch number:  8 \tLoss: 2.3007813147847793\n",
      "Epoch number:  10 \tLoss: 2.3006547755921622\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010249475883198\n",
      "Epoch number:  4 \tLoss: 2.3004205009647123\n",
      "Epoch number:  6 \tLoss: 2.3004647854776197\n",
      "Epoch number:  8 \tLoss: 2.300423331103313\n",
      "Epoch number:  10 \tLoss: 2.300251533507657\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301124843793913\n",
      "Epoch number:  4 \tLoss: 2.300929349370356\n",
      "Epoch number:  6 \tLoss: 2.3011025042324222\n",
      "Epoch number:  8 \tLoss: 2.3013222391565002\n",
      "Epoch number:  10 \tLoss: 2.3015225387848974\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.302203388462413\n",
      "Epoch number:  4 \tLoss: 2.3022068638557647\n",
      "Epoch number:  6 \tLoss: 2.3022145928106132\n",
      "Epoch number:  8 \tLoss: 2.302069193830458\n",
      "Epoch number:  10 \tLoss: 2.3017556453383747\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3026311886153317\n",
      "Epoch number:  4 \tLoss: 2.3028024926554704\n",
      "Epoch number:  6 \tLoss: 2.303076145869387\n",
      "Epoch number:  8 \tLoss: 2.3033020439677574\n",
      "Epoch number:  10 \tLoss: 2.3034740661825275\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2996819496125727\n",
      "Epoch number:  4 \tLoss: 2.2981439046592365\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3000975610550065\n",
      "Epoch number:  4 \tLoss: 2.2995124796585866\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2989166132903707\n",
      "Epoch number:  4 \tLoss: 2.296583011080954\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2984489209579624\n",
      "Epoch number:  4 \tLoss: 2.2979744703521745\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2981171190237175\n",
      "Epoch number:  4 \tLoss: 2.295916351572398\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.296810479998269\n",
      "Epoch number:  4 \tLoss: 2.2964985950652377\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2996908514337564\n",
      "Epoch number:  4 \tLoss: 2.2981867307432817\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3000971907794185\n",
      "Epoch number:  4 \tLoss: 2.299514697187013\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2988906092812824\n",
      "Epoch number:  4 \tLoss: 2.296553613355278\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2984553363092646\n",
      "Epoch number:  4 \tLoss: 2.297983105428185\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.298111319625688\n",
      "Epoch number:  4 \tLoss: 2.2959373553417373\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.296816117554307\n",
      "Epoch number:  4 \tLoss: 2.29650527679893\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3023861279155216\n",
      "Epoch number:  4 \tLoss: 2.3020966887235113\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3024115098752014\n",
      "Epoch number:  4 \tLoss: 2.3021474675273215\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3019453436720494\n",
      "Epoch number:  4 \tLoss: 2.3011669746074506\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.302151011758951\n",
      "Epoch number:  4 \tLoss: 2.301966916277221\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3000477688409195\n",
      "Epoch number:  4 \tLoss: 2.2986610637738987\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299512369749508\n",
      "Epoch number:  4 \tLoss: 2.299420260135321\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2996918194842833\n",
      "Epoch number:  4 \tLoss: 2.29819083262643\n",
      "Epoch number:  6 \tLoss: 1.8062805940160491\n",
      "Epoch number:  8 \tLoss: 1.662170406432261\n",
      "Epoch number:  10 \tLoss: 1.6210160390322048\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3000935515086565\n",
      "Epoch number:  4 \tLoss: 2.299508372197849\n",
      "Epoch number:  6 \tLoss: 2.2992470286176783\n",
      "Epoch number:  8 \tLoss: 2.299088060025561\n",
      "Epoch number:  10 \tLoss: 2.2989816082441563\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2989113213853205\n",
      "Epoch number:  4 \tLoss: 2.296625569024507\n",
      "Epoch number:  6 \tLoss: 1.7747250637963117\n",
      "Epoch number:  8 \tLoss: 1.2845800089061636\n",
      "Epoch number:  10 \tLoss: 0.643484036601034\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.298451099856548\n",
      "Epoch number:  4 \tLoss: 2.2979765390377556\n",
      "Epoch number:  6 \tLoss: 2.2977539316180953\n",
      "Epoch number:  8 \tLoss: 2.2976424552878822\n",
      "Epoch number:  10 \tLoss: 2.2975891839085594\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2980964652310747\n",
      "Epoch number:  4 \tLoss: 2.295927682901327\n",
      "Epoch number:  6 \tLoss: 2.2671538039975676\n",
      "Epoch number:  8 \tLoss: 1.5081904886570514\n",
      "Epoch number:  10 \tLoss: 0.8284046763397706\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.296813601873616\n",
      "Epoch number:  4 \tLoss: 2.296501633195039\n",
      "Epoch number:  6 \tLoss: 2.296404131766415\n",
      "Epoch number:  8 \tLoss: 2.2964019298956804\n",
      "Epoch number:  10 \tLoss: 2.296441406448345\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2996959703227806\n",
      "Epoch number:  4 \tLoss: 2.2982133454482523\n",
      "Epoch number:  6 \tLoss: 1.8176576519881287\n",
      "Epoch number:  8 \tLoss: 1.662261087723182\n",
      "Epoch number:  10 \tLoss: 1.622170526594065\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300098733317894\n",
      "Epoch number:  4 \tLoss: 2.2995166435339245\n",
      "Epoch number:  6 \tLoss: 2.299257795107635\n",
      "Epoch number:  8 \tLoss: 2.2991007886826753\n",
      "Epoch number:  10 \tLoss: 2.2989958674241824\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2989198166567912\n",
      "Epoch number:  4 \tLoss: 2.2966107604511783\n",
      "Epoch number:  6 \tLoss: 1.752942635907265\n",
      "Epoch number:  8 \tLoss: 1.6154253431766052\n",
      "Epoch number:  10 \tLoss: 0.8614984550724235\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2984531307845013\n",
      "Epoch number:  4 \tLoss: 2.2979810109615326\n",
      "Epoch number:  6 \tLoss: 2.2977594277497504\n",
      "Epoch number:  8 \tLoss: 2.297647880101692\n",
      "Epoch number:  10 \tLoss: 2.29759390115338\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2981059710064353\n",
      "Epoch number:  4 \tLoss: 2.2958728267119684\n",
      "Epoch number:  6 \tLoss: 2.2641510916068555\n",
      "Epoch number:  8 \tLoss: 1.6307682384730575\n",
      "Epoch number:  10 \tLoss: 0.7067239728689875\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2968156585417256\n",
      "Epoch number:  4 \tLoss: 2.2965044535380543\n",
      "Epoch number:  6 \tLoss: 2.2964056746725907\n",
      "Epoch number:  8 \tLoss: 2.296401125127987\n",
      "Epoch number:  10 \tLoss: 2.2964377639394256\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3023851683275574\n",
      "Epoch number:  4 \tLoss: 2.302095972289989\n",
      "Epoch number:  6 \tLoss: 2.3018390215264075\n",
      "Epoch number:  8 \tLoss: 2.301574179705467\n",
      "Epoch number:  10 \tLoss: 2.3012913405008533\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.30241256615883\n",
      "Epoch number:  4 \tLoss: 2.302148325533011\n",
      "Epoch number:  6 \tLoss: 2.3019168180813847\n",
      "Epoch number:  8 \tLoss: 2.301687713686724\n",
      "Epoch number:  10 \tLoss: 2.301460260540753\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301948943581106\n",
      "Epoch number:  4 \tLoss: 2.3011734597890436\n",
      "Epoch number:  6 \tLoss: 2.29653727899523\n",
      "Epoch number:  8 \tLoss: 1.7151571155375744\n",
      "Epoch number:  10 \tLoss: 1.7038594788232526\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3021506552619604\n",
      "Epoch number:  4 \tLoss: 2.3019667338970984\n",
      "Epoch number:  6 \tLoss: 2.3018651959328147\n",
      "Epoch number:  8 \tLoss: 2.301771469296087\n",
      "Epoch number:  10 \tLoss: 2.301679310644232\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3000431007369344\n",
      "Epoch number:  4 \tLoss: 2.2986509746253674\n",
      "Epoch number:  6 \tLoss: 1.838027642793281\n",
      "Epoch number:  8 \tLoss: 1.7391050129592576\n",
      "Epoch number:  10 \tLoss: 1.7309917992825505\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2995109441595676\n",
      "Epoch number:  4 \tLoss: 2.29941927940299\n",
      "Epoch number:  6 \tLoss: 2.299441602684441\n",
      "Epoch number:  8 \tLoss: 2.299481084300808\n",
      "Epoch number:  10 \tLoss: 2.299528666013965\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010783847046037\n",
      "Epoch number:  4 \tLoss: 2.3008600488858297\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301716062340599\n",
      "Epoch number:  4 \tLoss: 2.30152161157666\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300862445659416\n",
      "Epoch number:  4 \tLoss: 2.3006337728993693\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301107955287992\n",
      "Epoch number:  4 \tLoss: 2.300734457925256\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300328527107258\n",
      "Epoch number:  4 \tLoss: 2.3001448912913007\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2998973644541247\n",
      "Epoch number:  4 \tLoss: 2.299699620023174\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010780451748754\n",
      "Epoch number:  4 \tLoss: 2.3008722279669227\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3017170739516084\n",
      "Epoch number:  4 \tLoss: 2.301523572001656\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300879840529089\n",
      "Epoch number:  4 \tLoss: 2.3006502574351\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301108491981853\n",
      "Epoch number:  4 \tLoss: 2.3007357570036797\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300330920990649\n",
      "Epoch number:  4 \tLoss: 2.3001558579127663\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2998987571707885\n",
      "Epoch number:  4 \tLoss: 2.299701465115496\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3012502008500824\n",
      "Epoch number:  4 \tLoss: 2.3010066248935463\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3016628817013602\n",
      "Epoch number:  4 \tLoss: 2.30171279515138\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3012711475704934\n",
      "Epoch number:  4 \tLoss: 2.30105128161245\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301784555801742\n",
      "Epoch number:  4 \tLoss: 2.3016324685380862\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3006976364867238\n",
      "Epoch number:  4 \tLoss: 2.3004814188688405\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300478127042822\n",
      "Epoch number:  4 \tLoss: 2.300344798386273\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.30108061057493\n",
      "Epoch number:  4 \tLoss: 2.30086743880207\n",
      "Epoch number:  6 \tLoss: 2.3004280383856472\n",
      "Epoch number:  8 \tLoss: 2.2986716968267484\n",
      "Epoch number:  10 \tLoss: 1.9252157544868325\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301716502542597\n",
      "Epoch number:  4 \tLoss: 2.3015224765332167\n",
      "Epoch number:  6 \tLoss: 2.3014141314700525\n",
      "Epoch number:  8 \tLoss: 2.301322790454224\n",
      "Epoch number:  10 \tLoss: 2.3012370466299825\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300862375678552\n",
      "Epoch number:  4 \tLoss: 2.3006324642118923\n",
      "Epoch number:  6 \tLoss: 2.2999414483815457\n",
      "Epoch number:  8 \tLoss: 2.297496441313522\n",
      "Epoch number:  10 \tLoss: 1.9077107197885403\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3011107673368443\n",
      "Epoch number:  4 \tLoss: 2.3007376823017043\n",
      "Epoch number:  6 \tLoss: 2.3005522707638746\n",
      "Epoch number:  8 \tLoss: 2.3004279829863434\n",
      "Epoch number:  10 \tLoss: 2.3003338582984076\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3003406163103017\n",
      "Epoch number:  4 \tLoss: 2.3001526807999655\n",
      "Epoch number:  6 \tLoss: 2.2994937887459663\n",
      "Epoch number:  8 \tLoss: 2.2972380945600346\n",
      "Epoch number:  10 \tLoss: 2.2873750157524975\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299897251836547\n",
      "Epoch number:  4 \tLoss: 2.2996995189080014\n",
      "Epoch number:  6 \tLoss: 2.2995529265334302\n",
      "Epoch number:  8 \tLoss: 2.299450902062917\n",
      "Epoch number:  10 \tLoss: 2.2993770205009705\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010827420030524\n",
      "Epoch number:  4 \tLoss: 2.3008651005025262\n",
      "Epoch number:  6 \tLoss: 2.300423437169826\n",
      "Epoch number:  8 \tLoss: 2.2986482135468784\n",
      "Epoch number:  10 \tLoss: 1.8300089334041514\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3017172250399813\n",
      "Epoch number:  4 \tLoss: 2.301523371007531\n",
      "Epoch number:  6 \tLoss: 2.3014152978696965\n",
      "Epoch number:  8 \tLoss: 2.3013244373626907\n",
      "Epoch number:  10 \tLoss: 2.301239298006782\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3008734822966095\n",
      "Epoch number:  4 \tLoss: 2.300639339727107\n",
      "Epoch number:  6 \tLoss: 2.29993961653708\n",
      "Epoch number:  8 \tLoss: 2.297474558327525\n",
      "Epoch number:  10 \tLoss: 1.8064644738602726\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301109189779558\n",
      "Epoch number:  4 \tLoss: 2.3007363432029306\n",
      "Epoch number:  6 \tLoss: 2.3005512881443932\n",
      "Epoch number:  8 \tLoss: 2.300427405965508\n",
      "Epoch number:  10 \tLoss: 2.300333706903998\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.30033153542716\n",
      "Epoch number:  4 \tLoss: 2.3001484063532738\n",
      "Epoch number:  6 \tLoss: 2.2994889568798014\n",
      "Epoch number:  8 \tLoss: 2.297219908535635\n",
      "Epoch number:  10 \tLoss: 2.2872777973555776\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2998986017871235\n",
      "Epoch number:  4 \tLoss: 2.2997013313314483\n",
      "Epoch number:  6 \tLoss: 2.299555106727167\n",
      "Epoch number:  8 \tLoss: 2.2994533229478065\n",
      "Epoch number:  10 \tLoss: 2.299379617650424\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3012483255426606\n",
      "Epoch number:  4 \tLoss: 2.30100363317764\n",
      "Epoch number:  6 \tLoss: 2.300621310258601\n",
      "Epoch number:  8 \tLoss: 2.299490530215584\n",
      "Epoch number:  10 \tLoss: 1.6998886900561452\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301663558865583\n",
      "Epoch number:  4 \tLoss: 2.301713461672071\n",
      "Epoch number:  6 \tLoss: 2.301721064616653\n",
      "Epoch number:  8 \tLoss: 2.301717310296899\n",
      "Epoch number:  10 \tLoss: 2.301709038348884\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3012742809854525\n",
      "Epoch number:  4 \tLoss: 2.3010554592298313\n",
      "Epoch number:  6 \tLoss: 2.3004078355498403\n",
      "Epoch number:  8 \tLoss: 2.2978652906657304\n",
      "Epoch number:  10 \tLoss: 1.6759167836785294\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3017846664097843\n",
      "Epoch number:  4 \tLoss: 2.301632529847439\n",
      "Epoch number:  6 \tLoss: 2.301589884346052\n",
      "Epoch number:  8 \tLoss: 2.301584229058283\n",
      "Epoch number:  10 \tLoss: 2.301596045037346\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300692591034558\n",
      "Epoch number:  4 \tLoss: 2.300476583753971\n",
      "Epoch number:  6 \tLoss: 2.2997841407548973\n",
      "Epoch number:  8 \tLoss: 2.2971940858467894\n",
      "Epoch number:  10 \tLoss: 1.7039110915014368\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300477004678454\n",
      "Epoch number:  4 \tLoss: 2.3003440756696554\n",
      "Epoch number:  6 \tLoss: 2.3003044422359653\n",
      "Epoch number:  8 \tLoss: 2.300303859686676\n",
      "Epoch number:  10 \tLoss: 2.300324448519729\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3013007897875717\n",
      "Epoch number:  4 \tLoss: 2.300654357570319\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301683529084699\n",
      "Epoch number:  4 \tLoss: 2.3011950186002483\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3009687176055844\n",
      "Epoch number:  4 \tLoss: 2.3007916543465394\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3015573952620194\n",
      "Epoch number:  4 \tLoss: 2.30140500479094\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301274721023549\n",
      "Epoch number:  4 \tLoss: 2.301154198471462\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301736553525778\n",
      "Epoch number:  4 \tLoss: 2.301602741126685\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301289260633438\n",
      "Epoch number:  4 \tLoss: 2.3006455716146053\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301683050444735\n",
      "Epoch number:  4 \tLoss: 2.3011955827602284\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3009600521929996\n",
      "Epoch number:  4 \tLoss: 2.3007885772989574\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301557627015751\n",
      "Epoch number:  4 \tLoss: 2.3014055378198837\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3012601663278045\n",
      "Epoch number:  4 \tLoss: 2.3011303830815013\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3017368774390667\n",
      "Epoch number:  4 \tLoss: 2.3016032710828176\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3016732999984924\n",
      "Epoch number:  4 \tLoss: 2.3010785503231936\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.302033650616956\n",
      "Epoch number:  4 \tLoss: 2.3016469853439085\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3013115375706787\n",
      "Epoch number:  4 \tLoss: 2.3010419069991657\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3019356170681577\n",
      "Epoch number:  4 \tLoss: 2.30174482787806\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3016560448992487\n",
      "Epoch number:  4 \tLoss: 2.3014209412448414\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.302175853747379\n",
      "Epoch number:  4 \tLoss: 2.3019648039801686\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301302891479414\n",
      "Epoch number:  4 \tLoss: 2.300653587934154\n",
      "Epoch number:  6 \tLoss: 2.3004914180779905\n",
      "Epoch number:  8 \tLoss: 2.300374607050392\n",
      "Epoch number:  10 \tLoss: 2.3001781750015686\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3016840052807526\n",
      "Epoch number:  4 \tLoss: 2.3011954631764544\n",
      "Epoch number:  6 \tLoss: 2.3011176221191465\n",
      "Epoch number:  8 \tLoss: 2.3011518489719816\n",
      "Epoch number:  10 \tLoss: 2.3012088427466395\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300978913412698\n",
      "Epoch number:  4 \tLoss: 2.3008058700432987\n",
      "Epoch number:  6 \tLoss: 2.3008186784338482\n",
      "Epoch number:  8 \tLoss: 2.3007852625154332\n",
      "Epoch number:  10 \tLoss: 2.3006012006510734\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3015569325666423\n",
      "Epoch number:  4 \tLoss: 2.301405231309432\n",
      "Epoch number:  6 \tLoss: 2.301533022312273\n",
      "Epoch number:  8 \tLoss: 2.3016501247706933\n",
      "Epoch number:  10 \tLoss: 2.301724823155108\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301256071474277\n",
      "Epoch number:  4 \tLoss: 2.301135151453526\n",
      "Epoch number:  6 \tLoss: 2.301196022722181\n",
      "Epoch number:  8 \tLoss: 2.3011576354644605\n",
      "Epoch number:  10 \tLoss: 2.300942786690356\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301736049915919\n",
      "Epoch number:  4 \tLoss: 2.3016026252702697\n",
      "Epoch number:  6 \tLoss: 2.301691819446294\n",
      "Epoch number:  8 \tLoss: 2.301745481404564\n",
      "Epoch number:  10 \tLoss: 2.301760266594295\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3012923050202896\n",
      "Epoch number:  4 \tLoss: 2.3006509382992415\n",
      "Epoch number:  6 \tLoss: 2.300493173538558\n",
      "Epoch number:  8 \tLoss: 2.3003770901628715\n",
      "Epoch number:  10 \tLoss: 2.300178706822593\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3016816459376654\n",
      "Epoch number:  4 \tLoss: 2.3011954145740883\n",
      "Epoch number:  6 \tLoss: 2.301118591513188\n",
      "Epoch number:  8 \tLoss: 2.3011532484118242\n",
      "Epoch number:  10 \tLoss: 2.3012104340769604\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3009628143237886\n",
      "Epoch number:  4 \tLoss: 2.300786114279977\n",
      "Epoch number:  6 \tLoss: 2.300800936656651\n",
      "Epoch number:  8 \tLoss: 2.3007693943673506\n",
      "Epoch number:  10 \tLoss: 2.3005860023621842\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301557627588658\n",
      "Epoch number:  4 \tLoss: 2.3014056498700644\n",
      "Epoch number:  6 \tLoss: 2.3015332827895185\n",
      "Epoch number:  8 \tLoss: 2.3016503383944795\n",
      "Epoch number:  10 \tLoss: 2.301725081665114\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3012534248254313\n",
      "Epoch number:  4 \tLoss: 2.301136329241332\n",
      "Epoch number:  6 \tLoss: 2.301199605286002\n",
      "Epoch number:  8 \tLoss: 2.3011620349975392\n",
      "Epoch number:  10 \tLoss: 2.300946561071903\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.30173645957981\n",
      "Epoch number:  4 \tLoss: 2.301603342766408\n",
      "Epoch number:  6 \tLoss: 2.3016924607114695\n",
      "Epoch number:  8 \tLoss: 2.3017461482635864\n",
      "Epoch number:  10 \tLoss: 2.3017610423637653\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3016776471141522\n",
      "Epoch number:  4 \tLoss: 2.301085457958496\n",
      "Epoch number:  6 \tLoss: 2.3008348595485866\n",
      "Epoch number:  8 \tLoss: 2.3006014373673844\n",
      "Epoch number:  10 \tLoss: 2.300259942784044\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3020351869257403\n",
      "Epoch number:  4 \tLoss: 2.301647681396648\n",
      "Epoch number:  6 \tLoss: 2.301514323465747\n",
      "Epoch number:  8 \tLoss: 2.301463180879101\n",
      "Epoch number:  10 \tLoss: 2.301441178636967\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301306081768233\n",
      "Epoch number:  4 \tLoss: 2.3010370469391996\n",
      "Epoch number:  6 \tLoss: 2.300872361911053\n",
      "Epoch number:  8 \tLoss: 2.3006625590736904\n",
      "Epoch number:  10 \tLoss: 2.3002912355456577\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301935807757389\n",
      "Epoch number:  4 \tLoss: 2.301744581163236\n",
      "Epoch number:  6 \tLoss: 2.3017469305472353\n",
      "Epoch number:  8 \tLoss: 2.301776104101805\n",
      "Epoch number:  10 \tLoss: 2.301802345481516\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3016550475097026\n",
      "Epoch number:  4 \tLoss: 2.301418033197835\n",
      "Epoch number:  6 \tLoss: 2.301312525750551\n",
      "Epoch number:  8 \tLoss: 2.3011409938900473\n",
      "Epoch number:  10 \tLoss: 2.300789069662112\n",
      "Training with params: learning_rate=0.001, activation=sigmoid, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.302175929041167\n",
      "Epoch number:  4 \tLoss: 2.3019648907912176\n",
      "Epoch number:  6 \tLoss: 2.301952622824552\n",
      "Epoch number:  8 \tLoss: 2.301963438369587\n",
      "Epoch number:  10 \tLoss: 2.3019728009751277\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.4793838587140904\n",
      "Epoch number:  4 \tLoss: 0.9975640223312328\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300438878830028\n",
      "Epoch number:  4 \tLoss: 2.2993260448115462\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.0574309208751085\n",
      "Epoch number:  4 \tLoss: 0.5810327897481804\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3003042131563016\n",
      "Epoch number:  4 \tLoss: 2.2993388622648077\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.6490824113883308\n",
      "Epoch number:  4 \tLoss: 0.4650099689525548\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300097773351549\n",
      "Epoch number:  4 \tLoss: 2.2992134380760056\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.375761910880649\n",
      "Epoch number:  4 \tLoss: 0.8725641048519067\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300442736856847\n",
      "Epoch number:  4 \tLoss: 2.2993179542053612\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.9014000846413643\n",
      "Epoch number:  4 \tLoss: 0.5763884589425481\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300390211387003\n",
      "Epoch number:  4 \tLoss: 2.299322050537969\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.6486537842627151\n",
      "Epoch number:  4 \tLoss: 0.47338389518278806\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3000787683558817\n",
      "Epoch number:  4 \tLoss: 2.2991641813226242\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.6582083493056168\n",
      "Epoch number:  4 \tLoss: 1.2180542939066528\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3006566571333615\n",
      "Epoch number:  4 \tLoss: 2.2995019649392665\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.08223340492229\n",
      "Epoch number:  4 \tLoss: 0.6472773655212396\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3007189465793823\n",
      "Epoch number:  4 \tLoss: 2.2996112380947062\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.6914257796764972\n",
      "Epoch number:  4 \tLoss: 0.5157680876507406\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300996471495392\n",
      "Epoch number:  4 \tLoss: 2.300027670821253\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.4094020533858294\n",
      "Epoch number:  4 \tLoss: 0.982621366900509\n",
      "Epoch number:  6 \tLoss: 0.6824119559205539\n",
      "Epoch number:  8 \tLoss: 0.568267380588512\n",
      "Epoch number:  10 \tLoss: 0.4666406387743694\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300404505174537\n",
      "Epoch number:  4 \tLoss: 2.299326357380426\n",
      "Epoch number:  6 \tLoss: 2.2993251209926338\n",
      "Epoch number:  8 \tLoss: 2.299325028664554\n",
      "Epoch number:  10 \tLoss: 2.299324938693083\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.039366758973308\n",
      "Epoch number:  4 \tLoss: 0.5891943969570381\n",
      "Epoch number:  6 \tLoss: 0.46808366715734295\n",
      "Epoch number:  8 \tLoss: 0.3819206722918912\n",
      "Epoch number:  10 \tLoss: 0.34208850102471544\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.30035392429896\n",
      "Epoch number:  4 \tLoss: 2.299330901889841\n",
      "Epoch number:  6 \tLoss: 2.299328686413931\n",
      "Epoch number:  8 \tLoss: 2.299327471829019\n",
      "Epoch number:  10 \tLoss: 2.2993262300855095\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.646523657224083\n",
      "Epoch number:  4 \tLoss: 0.4655568103133765\n",
      "Epoch number:  6 \tLoss: 0.36940521933218634\n",
      "Epoch number:  8 \tLoss: 0.3315339993207371\n",
      "Epoch number:  10 \tLoss: 0.30632520652924805\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300111197797921\n",
      "Epoch number:  4 \tLoss: 2.29922185737277\n",
      "Epoch number:  6 \tLoss: 2.2990606059510545\n",
      "Epoch number:  8 \tLoss: 2.2984239058914104\n",
      "Epoch number:  10 \tLoss: 1.714259547218343\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.3764265373520679\n",
      "Epoch number:  4 \tLoss: 1.012421665797025\n",
      "Epoch number:  6 \tLoss: 0.59606442560528\n",
      "Epoch number:  8 \tLoss: 0.5374844715930154\n",
      "Epoch number:  10 \tLoss: 0.45831558505829256\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300473354441267\n",
      "Epoch number:  4 \tLoss: 2.299336448326376\n",
      "Epoch number:  6 \tLoss: 2.2993350738975984\n",
      "Epoch number:  8 \tLoss: 2.299334987791889\n",
      "Epoch number:  10 \tLoss: 2.2993349032409633\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.9125133871001038\n",
      "Epoch number:  4 \tLoss: 0.5776109546647655\n",
      "Epoch number:  6 \tLoss: 0.46404875289817166\n",
      "Epoch number:  8 \tLoss: 0.37521448847334515\n",
      "Epoch number:  10 \tLoss: 0.33676504076967473\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3002960839447426\n",
      "Epoch number:  4 \tLoss: 2.2993233739591075\n",
      "Epoch number:  6 \tLoss: 2.2993208905155416\n",
      "Epoch number:  8 \tLoss: 2.2993192692182167\n",
      "Epoch number:  10 \tLoss: 2.2993175763691736\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.6462421996625609\n",
      "Epoch number:  4 \tLoss: 0.4599612530403414\n",
      "Epoch number:  6 \tLoss: 0.36837957483304357\n",
      "Epoch number:  8 \tLoss: 0.3277162014893485\n",
      "Epoch number:  10 \tLoss: 0.3033582668242972\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3001244408963526\n",
      "Epoch number:  4 \tLoss: 2.29924878780017\n",
      "Epoch number:  6 \tLoss: 2.299185415052659\n",
      "Epoch number:  8 \tLoss: 2.299066919960708\n",
      "Epoch number:  10 \tLoss: 2.298687480932301\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.6860144275642455\n",
      "Epoch number:  4 \tLoss: 1.2253895761664348\n",
      "Epoch number:  6 \tLoss: 0.8533805904420363\n",
      "Epoch number:  8 \tLoss: 0.6875423912161249\n",
      "Epoch number:  10 \tLoss: 0.6584261046260982\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300664363996732\n",
      "Epoch number:  4 \tLoss: 2.2995089307832206\n",
      "Epoch number:  6 \tLoss: 2.299471482733627\n",
      "Epoch number:  8 \tLoss: 2.29944261220796\n",
      "Epoch number:  10 \tLoss: 2.299419558769956\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.0960026230040878\n",
      "Epoch number:  4 \tLoss: 0.7292035952534283\n",
      "Epoch number:  6 \tLoss: 0.5625717985003116\n",
      "Epoch number:  8 \tLoss: 0.4793333636384349\n",
      "Epoch number:  10 \tLoss: 0.4174158787325509\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300765655412862\n",
      "Epoch number:  4 \tLoss: 2.2996690425049753\n",
      "Epoch number:  6 \tLoss: 2.299601542306154\n",
      "Epoch number:  8 \tLoss: 2.299548058501473\n",
      "Epoch number:  10 \tLoss: 2.299505065357987\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.6689046598836164\n",
      "Epoch number:  4 \tLoss: 0.5055006353922309\n",
      "Epoch number:  6 \tLoss: 0.4131108126778495\n",
      "Epoch number:  8 \tLoss: 0.37076093826956485\n",
      "Epoch number:  10 \tLoss: 0.3470393288959645\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3008863311696817\n",
      "Epoch number:  4 \tLoss: 2.2998646268882017\n",
      "Epoch number:  6 \tLoss: 2.2997637964289415\n",
      "Epoch number:  8 \tLoss: 2.2996815364419456\n",
      "Epoch number:  10 \tLoss: 2.2996144624966615\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.5189600256565017\n",
      "Epoch number:  4 \tLoss: 0.8760471778826929\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3314084932882113\n",
      "Epoch number:  4 \tLoss: 2.300189890867125\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.0351312052709403\n",
      "Epoch number:  4 \tLoss: 0.576186804048319\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3303969772850204\n",
      "Epoch number:  4 \tLoss: 2.3001220363918073\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.6598879519077913\n",
      "Epoch number:  4 \tLoss: 0.46291028768093956\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.325225099291861\n",
      "Epoch number:  4 \tLoss: 2.299700698910096\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.4007111710197369\n",
      "Epoch number:  4 \tLoss: 0.9653870238783488\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.332630204513375\n",
      "Epoch number:  4 \tLoss: 2.3003072605608534\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.9277103569002799\n",
      "Epoch number:  4 \tLoss: 0.5828124394027261\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3290951139696037\n",
      "Epoch number:  4 \tLoss: 2.3000133293378675\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.6587927282873612\n",
      "Epoch number:  4 \tLoss: 0.45728866989784067\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3232991158856646\n",
      "Epoch number:  4 \tLoss: 2.2995726188013905\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.3805375491127592\n",
      "Epoch number:  4 \tLoss: 0.9146935125968633\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3323051654545637\n",
      "Epoch number:  4 \tLoss: 2.300427859903603\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.0091932861961304\n",
      "Epoch number:  4 \tLoss: 0.6024364649902438\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.329893582113904\n",
      "Epoch number:  4 \tLoss: 2.3004425898426875\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.6699806763333054\n",
      "Epoch number:  4 \tLoss: 0.4851758474318643\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3245601599547383\n",
      "Epoch number:  4 \tLoss: 2.3004491084662737\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.37128114866357\n",
      "Epoch number:  4 \tLoss: 0.8766016839290212\n",
      "Epoch number:  6 \tLoss: 0.6050936102587736\n",
      "Epoch number:  8 \tLoss: 0.5400642935478908\n",
      "Epoch number:  10 \tLoss: 0.4854185786752446\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3316256872419747\n",
      "Epoch number:  4 \tLoss: 2.300220011683835\n",
      "Epoch number:  6 \tLoss: 2.2992960043198902\n",
      "Epoch number:  8 \tLoss: 2.2992685183142862\n",
      "Epoch number:  10 \tLoss: 2.2992677259453447\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.0558242294378388\n",
      "Epoch number:  4 \tLoss: 0.5870238588361991\n",
      "Epoch number:  6 \tLoss: 0.4697179709830431\n",
      "Epoch number:  8 \tLoss: 0.3748076678536357\n",
      "Epoch number:  10 \tLoss: 0.3390569065006786\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3286744748341253\n",
      "Epoch number:  4 \tLoss: 2.2999968131156496\n",
      "Epoch number:  6 \tLoss: 2.2992858828775495\n",
      "Epoch number:  8 \tLoss: 2.2992671226370853\n",
      "Epoch number:  10 \tLoss: 2.299265578009777\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.662077147687528\n",
      "Epoch number:  4 \tLoss: 0.4635533172434847\n",
      "Epoch number:  6 \tLoss: 0.36884519061900345\n",
      "Epoch number:  8 \tLoss: 0.32930180769689893\n",
      "Epoch number:  10 \tLoss: 0.3077631877539521\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.325149221068977\n",
      "Epoch number:  4 \tLoss: 2.299654707604753\n",
      "Epoch number:  6 \tLoss: 2.299037832021576\n",
      "Epoch number:  8 \tLoss: 2.298714314819836\n",
      "Epoch number:  10 \tLoss: 2.294018198905427\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.3858150420521915\n",
      "Epoch number:  4 \tLoss: 0.979522837756079\n",
      "Epoch number:  6 \tLoss: 0.684625375674938\n",
      "Epoch number:  8 \tLoss: 0.5557428547882935\n",
      "Epoch number:  10 \tLoss: 0.5132581830765613\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.33108306693627\n",
      "Epoch number:  4 \tLoss: 2.3001682871434403\n",
      "Epoch number:  6 \tLoss: 2.2992864018966244\n",
      "Epoch number:  8 \tLoss: 2.2992610076755686\n",
      "Epoch number:  10 \tLoss: 2.29926030812387\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.9149809200591372\n",
      "Epoch number:  4 \tLoss: 0.5785689968639857\n",
      "Epoch number:  6 \tLoss: 0.46216640225647215\n",
      "Epoch number:  8 \tLoss: 0.37387332018838926\n",
      "Epoch number:  10 \tLoss: 0.3344907990348546\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.328800130594378\n",
      "Epoch number:  4 \tLoss: 2.2999931991470253\n",
      "Epoch number:  6 \tLoss: 2.29927375747547\n",
      "Epoch number:  8 \tLoss: 2.2992547795474922\n",
      "Epoch number:  10 \tLoss: 2.2992533197966813\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.6572667026417097\n",
      "Epoch number:  4 \tLoss: 0.4713104857056187\n",
      "Epoch number:  6 \tLoss: 0.3747322846684851\n",
      "Epoch number:  8 \tLoss: 0.335551073909325\n",
      "Epoch number:  10 \tLoss: 0.31245883279990594\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3248298516263266\n",
      "Epoch number:  4 \tLoss: 2.299647251180302\n",
      "Epoch number:  6 \tLoss: 2.298970146287178\n",
      "Epoch number:  8 \tLoss: 2.2978433908126283\n",
      "Epoch number:  10 \tLoss: 1.7053008024953464\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.5120709258753444\n",
      "Epoch number:  4 \tLoss: 0.9940734768280047\n",
      "Epoch number:  6 \tLoss: 0.8014912079766285\n",
      "Epoch number:  8 \tLoss: 0.56694765077346\n",
      "Epoch number:  10 \tLoss: 0.5341287959876079\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3323408726609602\n",
      "Epoch number:  4 \tLoss: 2.300451605319263\n",
      "Epoch number:  6 \tLoss: 2.2994687081806817\n",
      "Epoch number:  8 \tLoss: 2.2994285743634455\n",
      "Epoch number:  10 \tLoss: 2.2994182870603854\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.9792967757136264\n",
      "Epoch number:  4 \tLoss: 0.5901064997979495\n",
      "Epoch number:  6 \tLoss: 0.48888792936084813\n",
      "Epoch number:  8 \tLoss: 0.40465552158431184\n",
      "Epoch number:  10 \tLoss: 0.3534682190967205\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.328891958803744\n",
      "Epoch number:  4 \tLoss: 2.300419062087373\n",
      "Epoch number:  6 \tLoss: 2.2996927808234022\n",
      "Epoch number:  8 \tLoss: 2.2996551757842045\n",
      "Epoch number:  10 \tLoss: 2.299635866139444\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.6675799196945397\n",
      "Epoch number:  4 \tLoss: 0.4668653924551956\n",
      "Epoch number:  6 \tLoss: 0.3800020223040289\n",
      "Epoch number:  8 \tLoss: 0.3412869121183816\n",
      "Epoch number:  10 \tLoss: 0.3173541697815732\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3248663364757496\n",
      "Epoch number:  4 \tLoss: 2.3004334955943704\n",
      "Epoch number:  6 \tLoss: 2.2999268171869223\n",
      "Epoch number:  8 \tLoss: 2.299881814700661\n",
      "Epoch number:  10 \tLoss: 2.2998490762644073\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.4811196359539383\n",
      "Epoch number:  4 \tLoss: 0.8910807187267608\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.4874179621035895\n",
      "Epoch number:  4 \tLoss: 2.323046725477455\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.9898443554018598\n",
      "Epoch number:  4 \tLoss: 0.5880270906936822\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.4787824456186285\n",
      "Epoch number:  4 \tLoss: 2.3187723420284425\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.6914636081002086\n",
      "Epoch number:  4 \tLoss: 0.4615013553406128\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.4611385633332747\n",
      "Epoch number:  4 \tLoss: 2.312169021178494\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.4397451618628787\n",
      "Epoch number:  4 \tLoss: 0.873566856993658\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.489459179713652\n",
      "Epoch number:  4 \tLoss: 2.324216749603578\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.0361821818839616\n",
      "Epoch number:  4 \tLoss: 0.5922997175704092\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.4857294801483034\n",
      "Epoch number:  4 \tLoss: 2.3221238398118507\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.6932732109827463\n",
      "Epoch number:  4 \tLoss: 0.46948028766141386\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.453413776980133\n",
      "Epoch number:  4 \tLoss: 2.310109863228196\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.5985919848145422\n",
      "Epoch number:  4 \tLoss: 0.9235551510743286\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.491801514864643\n",
      "Epoch number:  4 \tLoss: 2.325815107437837\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.9744243594036245\n",
      "Epoch number:  4 \tLoss: 0.5948184207417553\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.4829630400811045\n",
      "Epoch number:  4 \tLoss: 2.321033282785286\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.68618788388926\n",
      "Epoch number:  4 \tLoss: 0.4714654623499624\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.4507432473206157\n",
      "Epoch number:  4 \tLoss: 2.310144494359334\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.4521666672436067\n",
      "Epoch number:  4 \tLoss: 1.0107403452224508\n",
      "Epoch number:  6 \tLoss: 0.6301592930715604\n",
      "Epoch number:  8 \tLoss: 0.5399905920334159\n",
      "Epoch number:  10 \tLoss: 0.44988126001011935\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.494494689979764\n",
      "Epoch number:  4 \tLoss: 2.327343460303938\n",
      "Epoch number:  6 \tLoss: 2.3033329469169805\n",
      "Epoch number:  8 \tLoss: 2.2998450294887545\n",
      "Epoch number:  10 \tLoss: 2.299338828167617\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.9636431137391894\n",
      "Epoch number:  4 \tLoss: 0.5887395668314364\n",
      "Epoch number:  6 \tLoss: 0.471036628805642\n",
      "Epoch number:  8 \tLoss: 0.37606298169024543\n",
      "Epoch number:  10 \tLoss: 0.3348448174660677\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.480914590325844\n",
      "Epoch number:  4 \tLoss: 2.3197440385884436\n",
      "Epoch number:  6 \tLoss: 2.3014808972396343\n",
      "Epoch number:  8 \tLoss: 2.299504830855791\n",
      "Epoch number:  10 \tLoss: 2.2992910350918008\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.6897191327732537\n",
      "Epoch number:  4 \tLoss: 0.4739496151497594\n",
      "Epoch number:  6 \tLoss: 0.37764102213781026\n",
      "Epoch number:  8 \tLoss: 0.33878504920250124\n",
      "Epoch number:  10 \tLoss: 0.31470600515263125\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.4582697413015686\n",
      "Epoch number:  4 \tLoss: 2.3113809174933433\n",
      "Epoch number:  6 \tLoss: 2.299888340458963\n",
      "Epoch number:  8 \tLoss: 2.298630829555708\n",
      "Epoch number:  10 \tLoss: 2.115009297364292\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.4423305854319322\n",
      "Epoch number:  4 \tLoss: 0.905472118189238\n",
      "Epoch number:  6 \tLoss: 0.5937350441360247\n",
      "Epoch number:  8 \tLoss: 0.5296479276697932\n",
      "Epoch number:  10 \tLoss: 0.42926818004869227\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.48987270565391\n",
      "Epoch number:  4 \tLoss: 2.324441373365433\n",
      "Epoch number:  6 \tLoss: 2.3025274422702324\n",
      "Epoch number:  8 \tLoss: 2.2996609886478434\n",
      "Epoch number:  10 \tLoss: 2.29928741097345\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.0761484940208534\n",
      "Epoch number:  4 \tLoss: 0.5965813326331982\n",
      "Epoch number:  6 \tLoss: 0.47749782489939524\n",
      "Epoch number:  8 \tLoss: 0.38147779083862554\n",
      "Epoch number:  10 \tLoss: 0.34212901164338283\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.4816646521998096\n",
      "Epoch number:  4 \tLoss: 2.3200779656001904\n",
      "Epoch number:  6 \tLoss: 2.3015425168234827\n",
      "Epoch number:  8 \tLoss: 2.2995086356327583\n",
      "Epoch number:  10 \tLoss: 2.29928594123822\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.6990128715756577\n",
      "Epoch number:  4 \tLoss: 0.4750196902793727\n",
      "Epoch number:  6 \tLoss: 0.3784501618704407\n",
      "Epoch number:  8 \tLoss: 0.33711456794356465\n",
      "Epoch number:  10 \tLoss: 0.3126220056482967\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.449796952652887\n",
      "Epoch number:  4 \tLoss: 2.309279344763193\n",
      "Epoch number:  6 \tLoss: 2.2996541265239734\n",
      "Epoch number:  8 \tLoss: 2.298558911117996\n",
      "Epoch number:  10 \tLoss: 1.7686632795930137\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.4498959828982485\n",
      "Epoch number:  4 \tLoss: 0.9774940803773553\n",
      "Epoch number:  6 \tLoss: 0.7237789101289307\n",
      "Epoch number:  8 \tLoss: 0.5766364795910481\n",
      "Epoch number:  10 \tLoss: 0.5219915571012285\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.4926921274509635\n",
      "Epoch number:  4 \tLoss: 2.3263780662253493\n",
      "Epoch number:  6 \tLoss: 2.3032518105727804\n",
      "Epoch number:  8 \tLoss: 2.300019178801814\n",
      "Epoch number:  10 \tLoss: 2.2995647080726713\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.9923279915448511\n",
      "Epoch number:  4 \tLoss: 0.5885396849675263\n",
      "Epoch number:  6 \tLoss: 0.4618120776666561\n",
      "Epoch number:  8 \tLoss: 0.37302601623843806\n",
      "Epoch number:  10 \tLoss: 0.3367207608824933\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.4821876057974994\n",
      "Epoch number:  4 \tLoss: 2.3206707378365663\n",
      "Epoch number:  6 \tLoss: 2.3019797993547186\n",
      "Epoch number:  8 \tLoss: 2.299893132064687\n",
      "Epoch number:  10 \tLoss: 2.2996572889247267\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.6897508282865356\n",
      "Epoch number:  4 \tLoss: 0.4707225982580746\n",
      "Epoch number:  6 \tLoss: 0.3853970160296414\n",
      "Epoch number:  8 \tLoss: 0.34637683164300387\n",
      "Epoch number:  10 \tLoss: 0.32078074145639396\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.4540122804935836\n",
      "Epoch number:  4 \tLoss: 2.3109123164676526\n",
      "Epoch number:  6 \tLoss: 2.3005204941380017\n",
      "Epoch number:  8 \tLoss: 2.299691456082133\n",
      "Epoch number:  10 \tLoss: 2.29912126326\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.40321254319680583\n",
      "Epoch number:  4 \tLoss: 0.3684845762834858\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301081369205011\n",
      "Epoch number:  4 \tLoss: 2.3010746351382143\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.3582393087663715\n",
      "Epoch number:  4 \tLoss: 0.32590179458736757\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3011353991087136\n",
      "Epoch number:  4 \tLoss: 2.3009050353783076\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.3328588757297737\n",
      "Epoch number:  4 \tLoss: 0.27722296427299237\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.2021452444652174\n",
      "Epoch number:  4 \tLoss: 0.6411509553074071\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.39290288345878627\n",
      "Epoch number:  4 \tLoss: 0.3569172948422396\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010663079678024\n",
      "Epoch number:  4 \tLoss: 2.3010603026867438\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.3576189893740724\n",
      "Epoch number:  4 \tLoss: 0.32532893924205125\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010931513567416\n",
      "Epoch number:  4 \tLoss: 2.3008958554770365\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.33522884493103383\n",
      "Epoch number:  4 \tLoss: 0.30059017715148967\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.4474464324807186\n",
      "Epoch number:  4 \tLoss: 0.6349307148652993\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.511765675568452\n",
      "Epoch number:  4 \tLoss: 0.4011418882714833\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301079278784967\n",
      "Epoch number:  4 \tLoss: 2.301052772047544\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.3846217473264052\n",
      "Epoch number:  4 \tLoss: 0.3627018809725839\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301139818200115\n",
      "Epoch number:  4 \tLoss: 2.301064725629783\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.3672793279916356\n",
      "Epoch number:  4 \tLoss: 0.35583761579360845\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3011765634755506\n",
      "Epoch number:  4 \tLoss: 2.3010715458324\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.38096088690713953\n",
      "Epoch number:  4 \tLoss: 0.3470326166136004\n",
      "Epoch number:  6 \tLoss: 0.33889253851684575\n",
      "Epoch number:  8 \tLoss: 0.3324291801749938\n",
      "Epoch number:  10 \tLoss: 0.31430086154164294\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010391882178016\n",
      "Epoch number:  4 \tLoss: 2.3010318981931936\n",
      "Epoch number:  6 \tLoss: 2.3010276427150833\n",
      "Epoch number:  8 \tLoss: 2.3010250780894204\n",
      "Epoch number:  10 \tLoss: 2.3010233208191377\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.36124153559698285\n",
      "Epoch number:  4 \tLoss: 0.32669959096047196\n",
      "Epoch number:  6 \tLoss: 0.29484291358864234\n",
      "Epoch number:  8 \tLoss: 0.3009858296753845\n",
      "Epoch number:  10 \tLoss: 0.2783600972669881\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010462095609405\n",
      "Epoch number:  4 \tLoss: 1.6782421892521864\n",
      "Epoch number:  6 \tLoss: 1.447811210650529\n",
      "Epoch number:  8 \tLoss: 1.1786696864431756\n",
      "Epoch number:  10 \tLoss: 0.9705878452082114\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.34543595371814056\n",
      "Epoch number:  4 \tLoss: 0.30500161351923527\n",
      "Epoch number:  6 \tLoss: 0.27472977019878436\n",
      "Epoch number:  8 \tLoss: 0.23494033994844357\n",
      "Epoch number:  10 \tLoss: 0.23588875549650504\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.4424782564282952\n",
      "Epoch number:  4 \tLoss: 0.6437610414309277\n",
      "Epoch number:  6 \tLoss: 0.3361714888251149\n",
      "Epoch number:  8 \tLoss: 0.3073098654529678\n",
      "Epoch number:  10 \tLoss: 0.3061576183553892\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.4069044257448819\n",
      "Epoch number:  4 \tLoss: 0.3771305885171767\n",
      "Epoch number:  6 \tLoss: 0.34998601602499707\n",
      "Epoch number:  8 \tLoss: 0.35240728679244093\n",
      "Epoch number:  10 \tLoss: 0.3244745664940715\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010981929464616\n",
      "Epoch number:  4 \tLoss: 2.3010892913147756\n",
      "Epoch number:  6 \tLoss: 2.3010829701535345\n",
      "Epoch number:  8 \tLoss: 2.301078168400651\n",
      "Epoch number:  10 \tLoss: 2.301074139618541\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.3501415931083642\n",
      "Epoch number:  4 \tLoss: 0.3103606182551718\n",
      "Epoch number:  6 \tLoss: 0.2957148171229263\n",
      "Epoch number:  8 \tLoss: 0.299753136750198\n",
      "Epoch number:  10 \tLoss: 0.2727847522242487\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010196481662524\n",
      "Epoch number:  4 \tLoss: 2.2994586717375007\n",
      "Epoch number:  6 \tLoss: 1.3498809675865735\n",
      "Epoch number:  8 \tLoss: 1.1730228595051104\n",
      "Epoch number:  10 \tLoss: 0.9734990822281208\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.33849085365223447\n",
      "Epoch number:  4 \tLoss: 0.30703508264608365\n",
      "Epoch number:  6 \tLoss: 0.26709391381786074\n",
      "Epoch number:  8 \tLoss: 0.2654372874210974\n",
      "Epoch number:  10 \tLoss: 0.25873418608125087\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.432524096393561\n",
      "Epoch number:  4 \tLoss: 0.6396033984575222\n",
      "Epoch number:  6 \tLoss: 0.3249930647287237\n",
      "Epoch number:  8 \tLoss: 0.30557154990880747\n",
      "Epoch number:  10 \tLoss: 0.31551966488639993\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.4648775692887507\n",
      "Epoch number:  4 \tLoss: 0.3928888883805366\n",
      "Epoch number:  6 \tLoss: 0.38611533685274263\n",
      "Epoch number:  8 \tLoss: 0.3838198683149492\n",
      "Epoch number:  10 \tLoss: 0.38201537224490156\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301068393618885\n",
      "Epoch number:  4 \tLoss: 2.3010506892447915\n",
      "Epoch number:  6 \tLoss: 2.3010480378647844\n",
      "Epoch number:  8 \tLoss: 2.3010472113944376\n",
      "Epoch number:  10 \tLoss: 2.301046816384725\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.3908541426344479\n",
      "Epoch number:  4 \tLoss: 0.37425641944209037\n",
      "Epoch number:  6 \tLoss: 0.37065333265347655\n",
      "Epoch number:  8 \tLoss: 0.36655631019838386\n",
      "Epoch number:  10 \tLoss: 0.3609148419108403\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301109956473218\n",
      "Epoch number:  4 \tLoss: 2.3010587830467943\n",
      "Epoch number:  6 \tLoss: 2.3010511528420468\n",
      "Epoch number:  8 \tLoss: 2.30104879132829\n",
      "Epoch number:  10 \tLoss: 2.3010476662528796\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.37103185220976675\n",
      "Epoch number:  4 \tLoss: 0.3477732755467926\n",
      "Epoch number:  6 \tLoss: 0.3454246327359857\n",
      "Epoch number:  8 \tLoss: 0.34712696783437935\n",
      "Epoch number:  10 \tLoss: 0.3428644733283235\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3011666750129143\n",
      "Epoch number:  4 \tLoss: 2.3010700287645003\n",
      "Epoch number:  6 \tLoss: 2.3010554373813448\n",
      "Epoch number:  8 \tLoss: 2.301050955540514\n",
      "Epoch number:  10 \tLoss: 2.3010488294179976\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.40435494828772484\n",
      "Epoch number:  4 \tLoss: 0.3627313110582826\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300121181289455\n",
      "Epoch number:  4 \tLoss: 2.3001150804621115\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.36945622967009606\n",
      "Epoch number:  4 \tLoss: 0.3126560079979894\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.30026545473727\n",
      "Epoch number:  4 \tLoss: 1.6843535614261276\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.338058314319841\n",
      "Epoch number:  4 \tLoss: 0.29037353492876167\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.4539450224827408\n",
      "Epoch number:  4 \tLoss: 0.6344512815306966\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.41423780182652786\n",
      "Epoch number:  4 \tLoss: 0.36494788149717516\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300162367621002\n",
      "Epoch number:  4 \tLoss: 2.3001580652070097\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.3618031995714031\n",
      "Epoch number:  4 \tLoss: 0.3339329056040157\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3001374994555768\n",
      "Epoch number:  4 \tLoss: 2.2990240437660545\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.345329381851898\n",
      "Epoch number:  4 \tLoss: 0.3009205525981813\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.4441259503218065\n",
      "Epoch number:  4 \tLoss: 0.6434639203271109\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.40602123803294965\n",
      "Epoch number:  4 \tLoss: 0.37485295754943737\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.30022263325033\n",
      "Epoch number:  4 \tLoss: 2.300144719137788\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.37565526368621255\n",
      "Epoch number:  4 \tLoss: 0.36496636887321243\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300307097682744\n",
      "Epoch number:  4 \tLoss: 2.300195471849876\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.36831160663207974\n",
      "Epoch number:  4 \tLoss: 0.32737533684682163\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.70205915303765\n",
      "Epoch number:  4 \tLoss: 1.6867501309503026\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.4072191306666842\n",
      "Epoch number:  4 \tLoss: 0.3820631920373156\n",
      "Epoch number:  6 \tLoss: 0.3639973890108825\n",
      "Epoch number:  8 \tLoss: 0.3701286849195823\n",
      "Epoch number:  10 \tLoss: 0.3446871018512174\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3000794512956904\n",
      "Epoch number:  4 \tLoss: 2.300076075352623\n",
      "Epoch number:  6 \tLoss: 2.300073456386158\n",
      "Epoch number:  8 \tLoss: 2.300071306102355\n",
      "Epoch number:  10 \tLoss: 2.300069275134158\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.3788619043295544\n",
      "Epoch number:  4 \tLoss: 0.32582379597358113\n",
      "Epoch number:  6 \tLoss: 0.31859697712337803\n",
      "Epoch number:  8 \tLoss: 0.2901688759374767\n",
      "Epoch number:  10 \tLoss: 0.3001586986612153\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300130757993962\n",
      "Epoch number:  4 \tLoss: 1.684476375232971\n",
      "Epoch number:  6 \tLoss: 1.1872204620272981\n",
      "Epoch number:  8 \tLoss: 0.972530552030305\n",
      "Epoch number:  10 \tLoss: 0.8017525742061496\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.35642036908371744\n",
      "Epoch number:  4 \tLoss: 0.3032570871219122\n",
      "Epoch number:  6 \tLoss: 0.27717282500543894\n",
      "Epoch number:  8 \tLoss: 0.25560231189637994\n",
      "Epoch number:  10 \tLoss: 0.23289232438652754\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.4468271544355822\n",
      "Epoch number:  4 \tLoss: 0.6338119004853836\n",
      "Epoch number:  6 \tLoss: 0.3377822040372088\n",
      "Epoch number:  8 \tLoss: 0.3009131861917596\n",
      "Epoch number:  10 \tLoss: 0.2842106341056944\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.38674836617661396\n",
      "Epoch number:  4 \tLoss: 0.37967396679476145\n",
      "Epoch number:  6 \tLoss: 0.3590180772364711\n",
      "Epoch number:  8 \tLoss: 0.35022383554952535\n",
      "Epoch number:  10 \tLoss: 0.3361836244113554\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3000762517499442\n",
      "Epoch number:  4 \tLoss: 2.3000710549984076\n",
      "Epoch number:  6 \tLoss: 2.3000666001568315\n",
      "Epoch number:  8 \tLoss: 2.30006255782456\n",
      "Epoch number:  10 \tLoss: 2.3000586068455413\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.3614648849155799\n",
      "Epoch number:  4 \tLoss: 0.32081873624804785\n",
      "Epoch number:  6 \tLoss: 0.30080095379152827\n",
      "Epoch number:  8 \tLoss: 0.3077898891484352\n",
      "Epoch number:  10 \tLoss: 0.2902914584908885\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3001499159807928\n",
      "Epoch number:  4 \tLoss: 1.6815848787331904\n",
      "Epoch number:  6 \tLoss: 1.3846503950422813\n",
      "Epoch number:  8 \tLoss: 1.167714049517607\n",
      "Epoch number:  10 \tLoss: 0.8074053130940576\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.3629855883219892\n",
      "Epoch number:  4 \tLoss: 0.3069259238297475\n",
      "Epoch number:  6 \tLoss: 0.28442455032561326\n",
      "Epoch number:  8 \tLoss: 0.25337671935452066\n",
      "Epoch number:  10 \tLoss: 0.24260144481099014\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.4481701810562635\n",
      "Epoch number:  4 \tLoss: 0.6388805684449077\n",
      "Epoch number:  6 \tLoss: 0.40469982581342245\n",
      "Epoch number:  8 \tLoss: 0.2989876847670708\n",
      "Epoch number:  10 \tLoss: 0.2886627119541775\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.4251867894848398\n",
      "Epoch number:  4 \tLoss: 0.361750035338903\n",
      "Epoch number:  6 \tLoss: 0.36560369232905077\n",
      "Epoch number:  8 \tLoss: 0.37495639968150873\n",
      "Epoch number:  10 \tLoss: 0.3583879084480735\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300160803234935\n",
      "Epoch number:  4 \tLoss: 2.300111095317868\n",
      "Epoch number:  6 \tLoss: 2.3000860701354062\n",
      "Epoch number:  8 \tLoss: 2.3000732696945727\n",
      "Epoch number:  10 \tLoss: 2.3000665928681903\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.36713841887775067\n",
      "Epoch number:  4 \tLoss: 0.3457695371667125\n",
      "Epoch number:  6 \tLoss: 0.32900276081882157\n",
      "Epoch number:  8 \tLoss: 0.3385313187235325\n",
      "Epoch number:  10 \tLoss: 0.3178700392381492\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300397303835666\n",
      "Epoch number:  4 \tLoss: 2.300241930060771\n",
      "Epoch number:  6 \tLoss: 2.300158039520026\n",
      "Epoch number:  8 \tLoss: 2.300113226523125\n",
      "Epoch number:  10 \tLoss: 2.3000891936360004\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.37110935182256843\n",
      "Epoch number:  4 \tLoss: 0.31333392290106526\n",
      "Epoch number:  6 \tLoss: 0.3000862171775641\n",
      "Epoch number:  8 \tLoss: 0.2949998325043322\n",
      "Epoch number:  10 \tLoss: 0.28229463124043774\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3005803077891196\n",
      "Epoch number:  4 \tLoss: 2.30037124645237\n",
      "Epoch number:  6 \tLoss: 2.30023668314158\n",
      "Epoch number:  8 \tLoss: 2.3001588099752466\n",
      "Epoch number:  10 \tLoss: 2.300115600359667\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.4032444256785829\n",
      "Epoch number:  4 \tLoss: 0.39177949946678764\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299729317233582\n",
      "Epoch number:  4 \tLoss: 2.2997247064641786\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.3426482435949485\n",
      "Epoch number:  4 \tLoss: 0.30748972700337757\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2996755744784285\n",
      "Epoch number:  4 \tLoss: 2.2995565251054604\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.3503768136585995\n",
      "Epoch number:  4 \tLoss: 0.29506085554116734\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.4626241507999416\n",
      "Epoch number:  4 \tLoss: 0.6460031209337191\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.4162467038689237\n",
      "Epoch number:  4 \tLoss: 0.38060424365757745\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2996452252249204\n",
      "Epoch number:  4 \tLoss: 2.2996393299300832\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.3666520054339232\n",
      "Epoch number:  4 \tLoss: 0.3447475002771375\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299731088333491\n",
      "Epoch number:  4 \tLoss: 1.6686533514011583\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.3476841329759134\n",
      "Epoch number:  4 \tLoss: 0.28998803448460986\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.4600713269534848\n",
      "Epoch number:  4 \tLoss: 0.6632933817748262\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.4158080233889136\n",
      "Epoch number:  4 \tLoss: 0.36707904240822614\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299788764494996\n",
      "Epoch number:  4 \tLoss: 2.299758355491466\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.35678542052147294\n",
      "Epoch number:  4 \tLoss: 0.31065302999487304\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300114230907384\n",
      "Epoch number:  4 \tLoss: 2.3000407675857018\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.345401418546004\n",
      "Epoch number:  4 \tLoss: 0.30619789025956046\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.6699877595097337\n",
      "Epoch number:  4 \tLoss: 1.182177213728482\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.41504062712010975\n",
      "Epoch number:  4 \tLoss: 0.3990778945789538\n",
      "Epoch number:  6 \tLoss: 0.3702186614474695\n",
      "Epoch number:  8 \tLoss: 0.36154104577977575\n",
      "Epoch number:  10 \tLoss: 0.3380235792911681\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2996871755962514\n",
      "Epoch number:  4 \tLoss: 2.2996804572491083\n",
      "Epoch number:  6 \tLoss: 2.2996737910906533\n",
      "Epoch number:  8 \tLoss: 2.299666863092369\n",
      "Epoch number:  10 \tLoss: 2.2996589595982346\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.35665292969165097\n",
      "Epoch number:  4 \tLoss: 0.3168456484592045\n",
      "Epoch number:  6 \tLoss: 0.3011034143474808\n",
      "Epoch number:  8 \tLoss: 0.2913564099911533\n",
      "Epoch number:  10 \tLoss: 0.26973619349906675\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2997265232619637\n",
      "Epoch number:  4 \tLoss: 2.299292389041397\n",
      "Epoch number:  6 \tLoss: 1.6884562877205715\n",
      "Epoch number:  8 \tLoss: 1.1832662586541682\n",
      "Epoch number:  10 \tLoss: 0.9573131742808427\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.3305434622898153\n",
      "Epoch number:  4 \tLoss: 0.2846845926055627\n",
      "Epoch number:  6 \tLoss: 0.26493232369862146\n",
      "Epoch number:  8 \tLoss: 0.2475712254609294\n",
      "Epoch number:  10 \tLoss: 0.23054980383705492\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.4571142209417522\n",
      "Epoch number:  4 \tLoss: 0.6479628018605096\n",
      "Epoch number:  6 \tLoss: 0.4236711683164252\n",
      "Epoch number:  8 \tLoss: 0.31848356258204463\n",
      "Epoch number:  10 \tLoss: 0.32145724594539565\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.4138383319424531\n",
      "Epoch number:  4 \tLoss: 0.3786764911113875\n",
      "Epoch number:  6 \tLoss: 0.36207373033269863\n",
      "Epoch number:  8 \tLoss: 0.3643735707575838\n",
      "Epoch number:  10 \tLoss: 0.3270310191553745\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299721414040908\n",
      "Epoch number:  4 \tLoss: 2.299717551614366\n",
      "Epoch number:  6 \tLoss: 2.299713880826783\n",
      "Epoch number:  8 \tLoss: 2.299710304267288\n",
      "Epoch number:  10 \tLoss: 2.299706670462708\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.3488169234307811\n",
      "Epoch number:  4 \tLoss: 0.3061130551200978\n",
      "Epoch number:  6 \tLoss: 0.2912398108869867\n",
      "Epoch number:  8 \tLoss: 0.28739621200640897\n",
      "Epoch number:  10 \tLoss: 0.2602932016857961\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299624658648229\n",
      "Epoch number:  4 \tLoss: 2.2994621420148285\n",
      "Epoch number:  6 \tLoss: 1.4459593124933474\n",
      "Epoch number:  8 \tLoss: 1.1778094899214566\n",
      "Epoch number:  10 \tLoss: 0.6412461698874365\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.32671457443351104\n",
      "Epoch number:  4 \tLoss: 0.28186871486836834\n",
      "Epoch number:  6 \tLoss: 0.26744591459392264\n",
      "Epoch number:  8 \tLoss: 0.24804285348193575\n",
      "Epoch number:  10 \tLoss: 0.23880077118266826\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.3250265472512979\n",
      "Epoch number:  4 \tLoss: 0.6309962802684543\n",
      "Epoch number:  6 \tLoss: 0.35630082537972646\n",
      "Epoch number:  8 \tLoss: 0.2927783949125018\n",
      "Epoch number:  10 \tLoss: 0.29668828421888777\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.41689278712646854\n",
      "Epoch number:  4 \tLoss: 0.39840925130247573\n",
      "Epoch number:  6 \tLoss: 0.3554194746395426\n",
      "Epoch number:  8 \tLoss: 0.36360438508663756\n",
      "Epoch number:  10 \tLoss: 0.36182116495558225\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2997640767842484\n",
      "Epoch number:  4 \tLoss: 2.2997332451822263\n",
      "Epoch number:  6 \tLoss: 2.29970673360571\n",
      "Epoch number:  8 \tLoss: 2.299684082920167\n",
      "Epoch number:  10 \tLoss: 2.29966485871565\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.3493300743766903\n",
      "Epoch number:  4 \tLoss: 0.335100031777557\n",
      "Epoch number:  6 \tLoss: 0.3373589344539625\n",
      "Epoch number:  8 \tLoss: 0.33225133245997696\n",
      "Epoch number:  10 \tLoss: 0.341323674827692\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300163808106123\n",
      "Epoch number:  4 \tLoss: 2.3000793479493526\n",
      "Epoch number:  6 \tLoss: 2.2999952274909523\n",
      "Epoch number:  8 \tLoss: 2.299858766993867\n",
      "Epoch number:  10 \tLoss: 1.7006586814591695\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.3360790366912135\n",
      "Epoch number:  4 \tLoss: 0.3060512021305058\n",
      "Epoch number:  6 \tLoss: 0.27399035161874724\n",
      "Epoch number:  8 \tLoss: 0.26709346607817064\n",
      "Epoch number:  10 \tLoss: 0.2645837316424974\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.4147159685791209\n",
      "Epoch number:  4 \tLoss: 0.8119580974707494\n",
      "Epoch number:  6 \tLoss: 0.6130985575492973\n",
      "Epoch number:  8 \tLoss: 0.3416342883329405\n",
      "Epoch number:  10 \tLoss: 0.3111813899907603\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.3877108780885291\n",
      "Epoch number:  4 \tLoss: 0.3577400348530944\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301042649308741\n",
      "Epoch number:  4 \tLoss: 2.3010392624381875\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.3600983206379318\n",
      "Epoch number:  4 \tLoss: 0.3276752831342137\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010494668588977\n",
      "Epoch number:  4 \tLoss: 1.6848756423314089\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.3334855302656927\n",
      "Epoch number:  4 \tLoss: 0.2926759324344799\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.4442230498561837\n",
      "Epoch number:  4 \tLoss: 0.6410313959968046\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.3850962924699081\n",
      "Epoch number:  4 \tLoss: 0.3566891049636169\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010443945442423\n",
      "Epoch number:  4 \tLoss: 2.3010367514911567\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.35131946238787837\n",
      "Epoch number:  4 \tLoss: 0.323750682578583\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3009883729816676\n",
      "Epoch number:  4 \tLoss: 1.6880193316599268\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.3325238128675513\n",
      "Epoch number:  4 \tLoss: 0.2873453371012571\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.4448591849568546\n",
      "Epoch number:  4 \tLoss: 0.638804826108458\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.4849679586684436\n",
      "Epoch number:  4 \tLoss: 0.3880382982874127\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010709718432416\n",
      "Epoch number:  4 \tLoss: 2.301039060599465\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.38379429491828415\n",
      "Epoch number:  4 \tLoss: 0.3692141284164037\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010793059418293\n",
      "Epoch number:  4 \tLoss: 2.3010408074739526\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.37621831334713735\n",
      "Epoch number:  4 \tLoss: 0.35668714247897565\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.30118208297973\n",
      "Epoch number:  4 \tLoss: 2.301061051474443\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.400894382688921\n",
      "Epoch number:  4 \tLoss: 0.38598082013248186\n",
      "Epoch number:  6 \tLoss: 0.3696331409468417\n",
      "Epoch number:  8 \tLoss: 0.3571798176845071\n",
      "Epoch number:  10 \tLoss: 0.3356722878622719\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3011168937047954\n",
      "Epoch number:  4 \tLoss: 2.3011043410674774\n",
      "Epoch number:  6 \tLoss: 2.3010953522481645\n",
      "Epoch number:  8 \tLoss: 2.3010886662630914\n",
      "Epoch number:  10 \tLoss: 2.301083497035001\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.3435951223895523\n",
      "Epoch number:  4 \tLoss: 0.3203417786453627\n",
      "Epoch number:  6 \tLoss: 0.3066293537566749\n",
      "Epoch number:  8 \tLoss: 0.27343703823987264\n",
      "Epoch number:  10 \tLoss: 0.27005587729061686\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301181711093248\n",
      "Epoch number:  4 \tLoss: 2.3007929930104134\n",
      "Epoch number:  6 \tLoss: 1.6825877191685592\n",
      "Epoch number:  8 \tLoss: 1.1803916789433655\n",
      "Epoch number:  10 \tLoss: 0.9774493084733079\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.33833162110696086\n",
      "Epoch number:  4 \tLoss: 0.30036498321250577\n",
      "Epoch number:  6 \tLoss: 0.28555554030888425\n",
      "Epoch number:  8 \tLoss: 0.26092876562368505\n",
      "Epoch number:  10 \tLoss: 0.2534323362112017\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.2153069034208888\n",
      "Epoch number:  4 \tLoss: 0.6476965617588444\n",
      "Epoch number:  6 \tLoss: 0.4075108464771677\n",
      "Epoch number:  8 \tLoss: 0.295068191649863\n",
      "Epoch number:  10 \tLoss: 0.3051879877051644\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.4076775624491258\n",
      "Epoch number:  4 \tLoss: 0.39552999346852713\n",
      "Epoch number:  6 \tLoss: 0.3709101823655452\n",
      "Epoch number:  8 \tLoss: 0.3702254248485148\n",
      "Epoch number:  10 \tLoss: 0.34328714922559456\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3011612713565177\n",
      "Epoch number:  4 \tLoss: 2.3011477815718626\n",
      "Epoch number:  6 \tLoss: 2.301137834801327\n",
      "Epoch number:  8 \tLoss: 2.3011301589201483\n",
      "Epoch number:  10 \tLoss: 2.3011239378468678\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.3430640852296021\n",
      "Epoch number:  4 \tLoss: 0.3210881697098923\n",
      "Epoch number:  6 \tLoss: 0.31886779868701265\n",
      "Epoch number:  8 \tLoss: 0.3080136009242103\n",
      "Epoch number:  10 \tLoss: 0.2928634910464917\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3011094546850996\n",
      "Epoch number:  4 \tLoss: 1.680046122568582\n",
      "Epoch number:  6 \tLoss: 1.6813134998496329\n",
      "Epoch number:  8 \tLoss: 1.1807736039648413\n",
      "Epoch number:  10 \tLoss: 1.168055012502733\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.3251107228241644\n",
      "Epoch number:  4 \tLoss: 0.2859929104518775\n",
      "Epoch number:  6 \tLoss: 0.2711514473309902\n",
      "Epoch number:  8 \tLoss: 0.2532678808442042\n",
      "Epoch number:  10 \tLoss: 0.23768083311088073\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.4477400824314552\n",
      "Epoch number:  4 \tLoss: 0.6453806774978592\n",
      "Epoch number:  6 \tLoss: 0.40301239832069197\n",
      "Epoch number:  8 \tLoss: 0.2956939838860598\n",
      "Epoch number:  10 \tLoss: 0.28630201382656684\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.49472711785952916\n",
      "Epoch number:  4 \tLoss: 0.3948378882588877\n",
      "Epoch number:  6 \tLoss: 0.38599608216299036\n",
      "Epoch number:  8 \tLoss: 0.37484755817743354\n",
      "Epoch number:  10 \tLoss: 0.3734858378857008\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010707017040604\n",
      "Epoch number:  4 \tLoss: 2.3010391777933914\n",
      "Epoch number:  6 \tLoss: 2.3010343520190193\n",
      "Epoch number:  8 \tLoss: 2.3010328521050245\n",
      "Epoch number:  10 \tLoss: 2.3010321338895374\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.38570042411643957\n",
      "Epoch number:  4 \tLoss: 0.3610363926968487\n",
      "Epoch number:  6 \tLoss: 0.35534179513293723\n",
      "Epoch number:  8 \tLoss: 0.3517494379138623\n",
      "Epoch number:  10 \tLoss: 0.3486622270916968\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010904996888755\n",
      "Epoch number:  4 \tLoss: 2.301042742170553\n",
      "Epoch number:  6 \tLoss: 2.3010357131983437\n",
      "Epoch number:  8 \tLoss: 2.301033544870352\n",
      "Epoch number:  10 \tLoss: 2.301032508461593\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.3784288141828056\n",
      "Epoch number:  4 \tLoss: 0.3621627714737734\n",
      "Epoch number:  6 \tLoss: 0.3560725840859762\n",
      "Epoch number:  8 \tLoss: 0.35325049324468416\n",
      "Epoch number:  10 \tLoss: 0.3494490449860533\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3011373804176896\n",
      "Epoch number:  4 \tLoss: 2.3010516944114925\n",
      "Epoch number:  6 \tLoss: 2.3010391213700916\n",
      "Epoch number:  8 \tLoss: 2.301035274859587\n",
      "Epoch number:  10 \tLoss: 2.3010334433331745\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.4097418310913193\n",
      "Epoch number:  4 \tLoss: 0.36232250478848343\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300156893264746\n",
      "Epoch number:  4 \tLoss: 2.30015028758865\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.3521625432219644\n",
      "Epoch number:  4 \tLoss: 0.3187749756821923\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3001309873647124\n",
      "Epoch number:  4 \tLoss: 2.2996730111929513\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.32904172426601436\n",
      "Epoch number:  4 \tLoss: 0.2909238187196734\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.4463390266016478\n",
      "Epoch number:  4 \tLoss: 0.645736554474794\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.40023665606310393\n",
      "Epoch number:  4 \tLoss: 0.3595619975398549\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3000866600832466\n",
      "Epoch number:  4 \tLoss: 2.300083547775486\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.3450939953350466\n",
      "Epoch number:  4 \tLoss: 0.32011394614087013\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300121933053976\n",
      "Epoch number:  4 \tLoss: 1.6810921610799647\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.3337576527438171\n",
      "Epoch number:  4 \tLoss: 0.2792686434161091\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.2209327508717447\n",
      "Epoch number:  4 \tLoss: 0.6428637487738508\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.40793202072265133\n",
      "Epoch number:  4 \tLoss: 0.3684688487619074\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300259923461091\n",
      "Epoch number:  4 \tLoss: 2.3001628809756363\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.35581874843263206\n",
      "Epoch number:  4 \tLoss: 0.3337147293183565\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3003133433540937\n",
      "Epoch number:  4 \tLoss: 2.300192195473163\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.33344344286280075\n",
      "Epoch number:  4 \tLoss: 0.30217185093474025\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3004823446375053\n",
      "Epoch number:  4 \tLoss: 2.3002945028651003\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.39394849112668695\n",
      "Epoch number:  4 \tLoss: 0.3639112005289705\n",
      "Epoch number:  6 \tLoss: 0.34953065556798707\n",
      "Epoch number:  8 \tLoss: 0.36244857556527466\n",
      "Epoch number:  10 \tLoss: 0.3328882983435949\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3001407791386947\n",
      "Epoch number:  4 \tLoss: 2.3001360927290757\n",
      "Epoch number:  6 \tLoss: 2.300132329200807\n",
      "Epoch number:  8 \tLoss: 2.3001293066576647\n",
      "Epoch number:  10 \tLoss: 2.3001268909794192\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.3476331482080073\n",
      "Epoch number:  4 \tLoss: 0.3145070912797135\n",
      "Epoch number:  6 \tLoss: 0.2993920938008911\n",
      "Epoch number:  8 \tLoss: 0.2862667107648957\n",
      "Epoch number:  10 \tLoss: 0.2777566720426812\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3001514872805955\n",
      "Epoch number:  4 \tLoss: 2.300083278596767\n",
      "Epoch number:  6 \tLoss: 1.4106237187052595\n",
      "Epoch number:  8 \tLoss: 1.1755689754709135\n",
      "Epoch number:  10 \tLoss: 0.6466052291483871\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.3314632490857016\n",
      "Epoch number:  4 \tLoss: 0.29611838052866973\n",
      "Epoch number:  6 \tLoss: 0.27736373293146366\n",
      "Epoch number:  8 \tLoss: 0.26024891107158304\n",
      "Epoch number:  10 \tLoss: 0.2466441523154901\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.4458524752232245\n",
      "Epoch number:  4 \tLoss: 0.6477643493286065\n",
      "Epoch number:  6 \tLoss: 0.34387014415600176\n",
      "Epoch number:  8 \tLoss: 0.2993846092533194\n",
      "Epoch number:  10 \tLoss: 0.30743011435422124\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.3904194402931951\n",
      "Epoch number:  4 \tLoss: 0.39032099236918955\n",
      "Epoch number:  6 \tLoss: 0.36753827530106936\n",
      "Epoch number:  8 \tLoss: 0.3637106152693307\n",
      "Epoch number:  10 \tLoss: 0.34778817831503045\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3001307176839565\n",
      "Epoch number:  4 \tLoss: 2.300121049042844\n",
      "Epoch number:  6 \tLoss: 2.3001121587828237\n",
      "Epoch number:  8 \tLoss: 2.3001033082916114\n",
      "Epoch number:  10 \tLoss: 2.300093371278855\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.3544099640712704\n",
      "Epoch number:  4 \tLoss: 0.3270888359235024\n",
      "Epoch number:  6 \tLoss: 0.29500456690501203\n",
      "Epoch number:  8 \tLoss: 0.2858996600168014\n",
      "Epoch number:  10 \tLoss: 0.25379450119371005\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300127970097047\n",
      "Epoch number:  4 \tLoss: 2.0139431748819496\n",
      "Epoch number:  6 \tLoss: 1.6849092180782026\n",
      "Epoch number:  8 \tLoss: 1.1818661086784663\n",
      "Epoch number:  10 \tLoss: 0.9687091391328236\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.3382154270843263\n",
      "Epoch number:  4 \tLoss: 0.28290156442883246\n",
      "Epoch number:  6 \tLoss: 0.2627083086644253\n",
      "Epoch number:  8 \tLoss: 0.24858139005908633\n",
      "Epoch number:  10 \tLoss: 0.24422425667095926\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.4475922833881705\n",
      "Epoch number:  4 \tLoss: 0.6345076524971667\n",
      "Epoch number:  6 \tLoss: 0.3617709659120077\n",
      "Epoch number:  8 \tLoss: 0.2925628729849776\n",
      "Epoch number:  10 \tLoss: 0.2909450772728739\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.4154924710445918\n",
      "Epoch number:  4 \tLoss: 0.388346419679887\n",
      "Epoch number:  6 \tLoss: 0.3826901463033737\n",
      "Epoch number:  8 \tLoss: 0.37709219624056967\n",
      "Epoch number:  10 \tLoss: 0.3553850650033695\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300171304667736\n",
      "Epoch number:  4 \tLoss: 2.3001137362130106\n",
      "Epoch number:  6 \tLoss: 2.3000844407829852\n",
      "Epoch number:  8 \tLoss: 2.300069341668261\n",
      "Epoch number:  10 \tLoss: 2.300061423988837\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.36075851271615617\n",
      "Epoch number:  4 \tLoss: 0.33531691368515626\n",
      "Epoch number:  6 \tLoss: 0.3256442278354629\n",
      "Epoch number:  8 \tLoss: 0.3119612783629127\n",
      "Epoch number:  10 \tLoss: 0.3022720024766792\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300216607157238\n",
      "Epoch number:  4 \tLoss: 2.300138684031143\n",
      "Epoch number:  6 \tLoss: 2.300098078994893\n",
      "Epoch number:  8 \tLoss: 2.3000768838761414\n",
      "Epoch number:  10 \tLoss: 2.3000656821353\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.3420812822855643\n",
      "Epoch number:  4 \tLoss: 0.3128216736724906\n",
      "Epoch number:  6 \tLoss: 0.3043709417823237\n",
      "Epoch number:  8 \tLoss: 0.3077792673929038\n",
      "Epoch number:  10 \tLoss: 0.30005928924195824\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3005867057533225\n",
      "Epoch number:  4 \tLoss: 2.300373924354483\n",
      "Epoch number:  6 \tLoss: 2.3002397542229116\n",
      "Epoch number:  8 \tLoss: 2.300157702734624\n",
      "Epoch number:  10 \tLoss: 2.300111913525577\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.39273633281543063\n",
      "Epoch number:  4 \tLoss: 0.3571511970482231\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2997434617594976\n",
      "Epoch number:  4 \tLoss: 2.2997369266397856\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.3558721790793674\n",
      "Epoch number:  4 \tLoss: 0.3090503336823631\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2997386067456884\n",
      "Epoch number:  4 \tLoss: 2.2990538556550986\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.33471507284658814\n",
      "Epoch number:  4 \tLoss: 0.2973594672186829\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.4507825544978545\n",
      "Epoch number:  4 \tLoss: 0.6461746073495558\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.405683246706359\n",
      "Epoch number:  4 \tLoss: 0.3737748087854736\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299591576571432\n",
      "Epoch number:  4 \tLoss: 2.2995846186574256\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.34961955363467734\n",
      "Epoch number:  4 \tLoss: 0.31924408231210205\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299724745989418\n",
      "Epoch number:  4 \tLoss: 2.299300127150212\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.3354416548313377\n",
      "Epoch number:  4 \tLoss: 0.3001060621781155\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.4485438494576215\n",
      "Epoch number:  4 \tLoss: 0.6318370398381326\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.3859550917736473\n",
      "Epoch number:  4 \tLoss: 0.35966158855158886\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2998400685275446\n",
      "Epoch number:  4 \tLoss: 2.299803836914882\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.3535632917320276\n",
      "Epoch number:  4 \tLoss: 0.3223178813606915\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3000603657828433\n",
      "Epoch number:  4 \tLoss: 2.299994364378979\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.3319330085476181\n",
      "Epoch number:  4 \tLoss: 0.3032797084153661\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.3522127141405016\n",
      "Epoch number:  4 \tLoss: 0.807927752071027\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.39538752049015136\n",
      "Epoch number:  4 \tLoss: 0.36475261789380786\n",
      "Epoch number:  6 \tLoss: 0.3507303956205545\n",
      "Epoch number:  8 \tLoss: 0.33963266176378476\n",
      "Epoch number:  10 \tLoss: 0.32347379721041797\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2996889314161333\n",
      "Epoch number:  4 \tLoss: 2.299681981809362\n",
      "Epoch number:  6 \tLoss: 2.299675572191318\n",
      "Epoch number:  8 \tLoss: 2.299669604151127\n",
      "Epoch number:  10 \tLoss: 2.299663931705042\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.35534361515172175\n",
      "Epoch number:  4 \tLoss: 0.3331921915833838\n",
      "Epoch number:  6 \tLoss: 0.3131765548870452\n",
      "Epoch number:  8 \tLoss: 0.29221695491527033\n",
      "Epoch number:  10 \tLoss: 0.2688426689795876\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2997086378600136\n",
      "Epoch number:  4 \tLoss: 2.298988032136956\n",
      "Epoch number:  6 \tLoss: 1.680426669236856\n",
      "Epoch number:  8 \tLoss: 1.1592476185324088\n",
      "Epoch number:  10 \tLoss: 0.9697906365802335\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.3294968049046466\n",
      "Epoch number:  4 \tLoss: 0.279649928321973\n",
      "Epoch number:  6 \tLoss: 0.2690033420120145\n",
      "Epoch number:  8 \tLoss: 0.23904591285668014\n",
      "Epoch number:  10 \tLoss: 0.2505549629983888\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.442780364163774\n",
      "Epoch number:  4 \tLoss: 0.637526316458756\n",
      "Epoch number:  6 \tLoss: 0.34346499932487684\n",
      "Epoch number:  8 \tLoss: 0.3009772428467313\n",
      "Epoch number:  10 \tLoss: 0.283655751534911\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.4017341531044473\n",
      "Epoch number:  4 \tLoss: 0.3657229217459164\n",
      "Epoch number:  6 \tLoss: 0.34565977806602477\n",
      "Epoch number:  8 \tLoss: 0.3299982805848285\n",
      "Epoch number:  10 \tLoss: 0.32526840270218166\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2997098181497795\n",
      "Epoch number:  4 \tLoss: 2.2996994188812865\n",
      "Epoch number:  6 \tLoss: 2.299689466251461\n",
      "Epoch number:  8 \tLoss: 2.299679871578325\n",
      "Epoch number:  10 \tLoss: 2.29967054085056\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.3432104437943402\n",
      "Epoch number:  4 \tLoss: 0.32318032654745144\n",
      "Epoch number:  6 \tLoss: 0.29306948217512746\n",
      "Epoch number:  8 \tLoss: 0.28499250080558036\n",
      "Epoch number:  10 \tLoss: 0.26707588802501\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299707839170789\n",
      "Epoch number:  4 \tLoss: 1.6733306622214104\n",
      "Epoch number:  6 \tLoss: 1.430988987391858\n",
      "Epoch number:  8 \tLoss: 1.003302976024006\n",
      "Epoch number:  10 \tLoss: 0.632997322951139\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.3345239705512748\n",
      "Epoch number:  4 \tLoss: 0.2896315544133607\n",
      "Epoch number:  6 \tLoss: 0.2540014109749075\n",
      "Epoch number:  8 \tLoss: 0.2401932375174572\n",
      "Epoch number:  10 \tLoss: 0.2277015086499233\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.4496368330369085\n",
      "Epoch number:  4 \tLoss: 0.6278900512863553\n",
      "Epoch number:  6 \tLoss: 0.33325886458301085\n",
      "Epoch number:  8 \tLoss: 0.2977209045725708\n",
      "Epoch number:  10 \tLoss: 0.26750547156434\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.39632305957887526\n",
      "Epoch number:  4 \tLoss: 0.3663545606040535\n",
      "Epoch number:  6 \tLoss: 0.3566856925175426\n",
      "Epoch number:  8 \tLoss: 0.33750200064869007\n",
      "Epoch number:  10 \tLoss: 0.3355112905978183\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299921170389764\n",
      "Epoch number:  4 \tLoss: 2.299871009314012\n",
      "Epoch number:  6 \tLoss: 2.2998260090614564\n",
      "Epoch number:  8 \tLoss: 2.2997862118619494\n",
      "Epoch number:  10 \tLoss: 2.299751457994288\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.35329431337971395\n",
      "Epoch number:  4 \tLoss: 0.3093097578614521\n",
      "Epoch number:  6 \tLoss: 0.3074227472325441\n",
      "Epoch number:  8 \tLoss: 0.2944640986634787\n",
      "Epoch number:  10 \tLoss: 0.28543120394444593\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299934593060415\n",
      "Epoch number:  4 \tLoss: 2.2998839688539205\n",
      "Epoch number:  6 \tLoss: 2.299838905630036\n",
      "Epoch number:  8 \tLoss: 2.2997988059527894\n",
      "Epoch number:  10 \tLoss: 2.2997633740676022\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.33529134295208995\n",
      "Epoch number:  4 \tLoss: 0.29806619318826094\n",
      "Epoch number:  6 \tLoss: 0.2968718228767436\n",
      "Epoch number:  8 \tLoss: 0.27824072067813554\n",
      "Epoch number:  10 \tLoss: 0.2678041555798354\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=random, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.6881925755848493\n",
      "Epoch number:  4 \tLoss: 0.9834272314279119\n",
      "Epoch number:  6 \tLoss: 0.62392508482688\n",
      "Epoch number:  8 \tLoss: 0.4819808741334018\n",
      "Epoch number:  10 \tLoss: 0.3077776601705332\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300546611199009\n",
      "Epoch number:  4 \tLoss: 2.2993260775668016\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300544282218576\n",
      "Epoch number:  4 \tLoss: 2.299325271586961\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300544036073529\n",
      "Epoch number:  4 \tLoss: 2.2993257378738745\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300543480175563\n",
      "Epoch number:  4 \tLoss: 2.299326531024781\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3005425990354493\n",
      "Epoch number:  4 \tLoss: 2.299325995456086\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300545037313643\n",
      "Epoch number:  4 \tLoss: 2.299326868263354\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300548234243158\n",
      "Epoch number:  4 \tLoss: 2.299327981490527\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300543775102925\n",
      "Epoch number:  4 \tLoss: 2.2993253056549268\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3005439831565346\n",
      "Epoch number:  4 \tLoss: 2.2993262218418864\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300544461981528\n",
      "Epoch number:  4 \tLoss: 2.29932535272872\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300543940050855\n",
      "Epoch number:  4 \tLoss: 2.2993260989832507\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300543353277949\n",
      "Epoch number:  4 \tLoss: 2.299325653285059\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300553516370538\n",
      "Epoch number:  4 \tLoss: 2.2993306074614086\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3005562169252136\n",
      "Epoch number:  4 \tLoss: 2.29933551374087\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3005548639272875\n",
      "Epoch number:  4 \tLoss: 2.299333136181789\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3005542983751615\n",
      "Epoch number:  4 \tLoss: 2.2993322867485784\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3005561895333666\n",
      "Epoch number:  4 \tLoss: 2.2993355312983907\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300556977022987\n",
      "Epoch number:  4 \tLoss: 2.2993372679608197\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3005459472732808\n",
      "Epoch number:  4 \tLoss: 2.2993253657073875\n",
      "Epoch number:  6 \tLoss: 2.2993237707951057\n",
      "Epoch number:  8 \tLoss: 2.2993236797690697\n",
      "Epoch number:  10 \tLoss: 2.2993235744204674\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300547475257078\n",
      "Epoch number:  4 \tLoss: 2.2993265700942533\n",
      "Epoch number:  6 \tLoss: 2.2993250525999747\n",
      "Epoch number:  8 \tLoss: 2.2993250500803226\n",
      "Epoch number:  10 \tLoss: 2.299325049446644\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300543458892832\n",
      "Epoch number:  4 \tLoss: 2.2993258994409174\n",
      "Epoch number:  6 \tLoss: 2.299324312743719\n",
      "Epoch number:  8 \tLoss: 2.299324218256497\n",
      "Epoch number:  10 \tLoss: 2.2993241069637915\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3005447216026833\n",
      "Epoch number:  4 \tLoss: 2.29932592363055\n",
      "Epoch number:  6 \tLoss: 2.299324411544431\n",
      "Epoch number:  8 \tLoss: 2.2993244084775215\n",
      "Epoch number:  10 \tLoss: 2.299324407286765\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300542884608922\n",
      "Epoch number:  4 \tLoss: 2.2993257240504774\n",
      "Epoch number:  6 \tLoss: 2.2993241416853376\n",
      "Epoch number:  8 \tLoss: 2.2993240559009402\n",
      "Epoch number:  10 \tLoss: 2.2993239608240392\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300543054324722\n",
      "Epoch number:  4 \tLoss: 2.299325704521037\n",
      "Epoch number:  6 \tLoss: 2.2993241966245996\n",
      "Epoch number:  8 \tLoss: 2.2993241936649462\n",
      "Epoch number:  10 \tLoss: 2.29932419257438\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3005463024532227\n",
      "Epoch number:  4 \tLoss: 2.299326042672917\n",
      "Epoch number:  6 \tLoss: 2.299324494075545\n",
      "Epoch number:  8 \tLoss: 2.2993244562613944\n",
      "Epoch number:  10 \tLoss: 2.2993244172035747\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300542685925519\n",
      "Epoch number:  4 \tLoss: 2.2993251399479293\n",
      "Epoch number:  6 \tLoss: 2.2993236320556956\n",
      "Epoch number:  8 \tLoss: 2.2993236296551585\n",
      "Epoch number:  10 \tLoss: 2.299323629124806\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300543507948141\n",
      "Epoch number:  4 \tLoss: 2.2993258569495736\n",
      "Epoch number:  6 \tLoss: 2.299324277080993\n",
      "Epoch number:  8 \tLoss: 2.2993241946019802\n",
      "Epoch number:  10 \tLoss: 2.2993241024220863\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3005441132404516\n",
      "Epoch number:  4 \tLoss: 2.2993262797891414\n",
      "Epoch number:  6 \tLoss: 2.29932477009826\n",
      "Epoch number:  8 \tLoss: 2.299324766711712\n",
      "Epoch number:  10 \tLoss: 2.299324765196877\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300543352952448\n",
      "Epoch number:  4 \tLoss: 2.299325010197225\n",
      "Epoch number:  6 \tLoss: 2.2993234138507828\n",
      "Epoch number:  8 \tLoss: 2.2993233164824103\n",
      "Epoch number:  10 \tLoss: 2.2993232082360064\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3005433397339305\n",
      "Epoch number:  4 \tLoss: 2.2993262223540785\n",
      "Epoch number:  6 \tLoss: 2.299324714741269\n",
      "Epoch number:  8 \tLoss: 2.299324711414283\n",
      "Epoch number:  10 \tLoss: 2.299324709955566\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300553931576576\n",
      "Epoch number:  4 \tLoss: 2.299331404008513\n",
      "Epoch number:  6 \tLoss: 2.2993287718373807\n",
      "Epoch number:  8 \tLoss: 2.299327877345349\n",
      "Epoch number:  10 \tLoss: 2.299327160910644\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3005539944564775\n",
      "Epoch number:  4 \tLoss: 2.2993319139004367\n",
      "Epoch number:  6 \tLoss: 2.2993292170869077\n",
      "Epoch number:  8 \tLoss: 2.2993282588938437\n",
      "Epoch number:  10 \tLoss: 2.299327484194531\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300556873769964\n",
      "Epoch number:  4 \tLoss: 2.2993373222895124\n",
      "Epoch number:  6 \tLoss: 2.299333614056515\n",
      "Epoch number:  8 \tLoss: 2.29933182611254\n",
      "Epoch number:  10 \tLoss: 2.2993303778072174\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3005539176172487\n",
      "Epoch number:  4 \tLoss: 2.2993314420315825\n",
      "Epoch number:  6 \tLoss: 2.2993288065076047\n",
      "Epoch number:  8 \tLoss: 2.2993279082854676\n",
      "Epoch number:  10 \tLoss: 2.299327188264988\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3005553814712454\n",
      "Epoch number:  4 \tLoss: 2.2993341056165604\n",
      "Epoch number:  6 \tLoss: 2.299330965527952\n",
      "Epoch number:  8 \tLoss: 2.2993296560995424\n",
      "Epoch number:  10 \tLoss: 2.299328603408492\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3005566249224123\n",
      "Epoch number:  4 \tLoss: 2.2993363287184727\n",
      "Epoch number:  6 \tLoss: 2.299332759550873\n",
      "Epoch number:  8 \tLoss: 2.2993311027981154\n",
      "Epoch number:  10 \tLoss: 2.299329771776455\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3344479210099762\n",
      "Epoch number:  4 \tLoss: 2.3004700483045997\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.334390975474782\n",
      "Epoch number:  4 \tLoss: 2.300463482545819\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.334390001674391\n",
      "Epoch number:  4 \tLoss: 2.3004642934843362\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3344130675276884\n",
      "Epoch number:  4 \tLoss: 2.300466273941113\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3343399750351614\n",
      "Epoch number:  4 \tLoss: 2.3004589529397657\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3343706370770927\n",
      "Epoch number:  4 \tLoss: 2.3004626965591717\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.334415329376658\n",
      "Epoch number:  4 \tLoss: 2.300467732695849\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3344268252115468\n",
      "Epoch number:  4 \tLoss: 2.3004685528602806\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3343694062236304\n",
      "Epoch number:  4 \tLoss: 2.3004618109146313\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3344208713637333\n",
      "Epoch number:  4 \tLoss: 2.300467522218532\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3343619095736834\n",
      "Epoch number:  4 \tLoss: 2.3004617519758694\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.334389760072714\n",
      "Epoch number:  4 \tLoss: 2.300464203908866\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3344168171363426\n",
      "Epoch number:  4 \tLoss: 2.3004768263202844\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.33441368643886\n",
      "Epoch number:  4 \tLoss: 2.300477137248434\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3343964899764407\n",
      "Epoch number:  4 \tLoss: 2.3004758202733133\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3343238296534903\n",
      "Epoch number:  4 \tLoss: 2.300474265319511\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3343664164878017\n",
      "Epoch number:  4 \tLoss: 2.3004751100471412\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3343770723282753\n",
      "Epoch number:  4 \tLoss: 2.3004759802696286\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.334349786991857\n",
      "Epoch number:  4 \tLoss: 2.3004597207478357\n",
      "Epoch number:  6 \tLoss: 2.2992980710081246\n",
      "Epoch number:  8 \tLoss: 2.299257529408534\n",
      "Epoch number:  10 \tLoss: 2.2992560749024094\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3344496779044346\n",
      "Epoch number:  4 \tLoss: 2.3004709178672793\n",
      "Epoch number:  6 \tLoss: 2.2992999397185296\n",
      "Epoch number:  8 \tLoss: 2.2992588799383884\n",
      "Epoch number:  10 \tLoss: 2.2992574416262297\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3343687119305474\n",
      "Epoch number:  4 \tLoss: 2.3004621196487034\n",
      "Epoch number:  6 \tLoss: 2.2992987060582655\n",
      "Epoch number:  8 \tLoss: 2.299258034822299\n",
      "Epoch number:  10 \tLoss: 2.2992565467732184\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.334345186330004\n",
      "Epoch number:  4 \tLoss: 2.3004599091019866\n",
      "Epoch number:  6 \tLoss: 2.2992987945661767\n",
      "Epoch number:  8 \tLoss: 2.299258324555752\n",
      "Epoch number:  10 \tLoss: 2.2992569178420736\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3343352806262376\n",
      "Epoch number:  4 \tLoss: 2.30045969760441\n",
      "Epoch number:  6 \tLoss: 2.2992995141951047\n",
      "Epoch number:  8 \tLoss: 2.299259005026627\n",
      "Epoch number:  10 \tLoss: 2.2992574876633496\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.334396445866656\n",
      "Epoch number:  4 \tLoss: 2.3004649943755164\n",
      "Epoch number:  6 \tLoss: 2.2992990186823774\n",
      "Epoch number:  8 \tLoss: 2.2992582585072086\n",
      "Epoch number:  10 \tLoss: 2.299256836259336\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.334456892251759\n",
      "Epoch number:  4 \tLoss: 2.300470415544191\n",
      "Epoch number:  6 \tLoss: 2.29929858807326\n",
      "Epoch number:  8 \tLoss: 2.299257435832916\n",
      "Epoch number:  10 \tLoss: 2.299255948069686\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.334411796582153\n",
      "Epoch number:  4 \tLoss: 2.3004672425629966\n",
      "Epoch number:  6 \tLoss: 2.2992998786535916\n",
      "Epoch number:  8 \tLoss: 2.2992590349766036\n",
      "Epoch number:  10 \tLoss: 2.2992576078803677\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.33436611318756\n",
      "Epoch number:  4 \tLoss: 2.3004620119950316\n",
      "Epoch number:  6 \tLoss: 2.2992988670167827\n",
      "Epoch number:  8 \tLoss: 2.2992582201255676\n",
      "Epoch number:  10 \tLoss: 2.299256741376752\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.33437584636264\n",
      "Epoch number:  4 \tLoss: 2.3004634336406427\n",
      "Epoch number:  6 \tLoss: 2.2992994614901385\n",
      "Epoch number:  8 \tLoss: 2.2992588210236873\n",
      "Epoch number:  10 \tLoss: 2.299257405259559\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.334344289074555\n",
      "Epoch number:  4 \tLoss: 2.3004595256162057\n",
      "Epoch number:  6 \tLoss: 2.299298377638143\n",
      "Epoch number:  8 \tLoss: 2.2992578043125187\n",
      "Epoch number:  10 \tLoss: 2.299256276358318\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.334379103013894\n",
      "Epoch number:  4 \tLoss: 2.300463121124224\n",
      "Epoch number:  6 \tLoss: 2.2992987771676465\n",
      "Epoch number:  8 \tLoss: 2.299258114591516\n",
      "Epoch number:  10 \tLoss: 2.299256697668586\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3344703874137305\n",
      "Epoch number:  4 \tLoss: 2.3004778688170227\n",
      "Epoch number:  6 \tLoss: 2.2993048471056587\n",
      "Epoch number:  8 \tLoss: 2.2992633717029918\n",
      "Epoch number:  10 \tLoss: 2.2992616462453777\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3343804039692366\n",
      "Epoch number:  4 \tLoss: 2.3004759141480933\n",
      "Epoch number:  6 \tLoss: 2.2993114937426435\n",
      "Epoch number:  8 \tLoss: 2.2992701496934282\n",
      "Epoch number:  10 \tLoss: 2.2992680767375626\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.334415461383684\n",
      "Epoch number:  4 \tLoss: 2.3004767285817764\n",
      "Epoch number:  6 \tLoss: 2.2993089520540257\n",
      "Epoch number:  8 \tLoss: 2.299267545823061\n",
      "Epoch number:  10 \tLoss: 2.299265600700276\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3344217688121085\n",
      "Epoch number:  4 \tLoss: 2.3004762841343007\n",
      "Epoch number:  6 \tLoss: 2.299307921912045\n",
      "Epoch number:  8 \tLoss: 2.2992665690640224\n",
      "Epoch number:  10 \tLoss: 2.2992647027999493\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.334324268971084\n",
      "Epoch number:  4 \tLoss: 2.300473690910172\n",
      "Epoch number:  6 \tLoss: 2.299314578816764\n",
      "Epoch number:  8 \tLoss: 2.2992733884713674\n",
      "Epoch number:  10 \tLoss: 2.2992711802231414\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3343696901518447\n",
      "Epoch number:  4 \tLoss: 2.3004752275355935\n",
      "Epoch number:  6 \tLoss: 2.2993118263513153\n",
      "Epoch number:  8 \tLoss: 2.299270537851325\n",
      "Epoch number:  10 \tLoss: 2.299268462669968\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.503400752572473\n",
      "Epoch number:  4 \tLoss: 2.3340604788508\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.5031555680252717\n",
      "Epoch number:  4 \tLoss: 2.333852204688456\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.5031768254828304\n",
      "Epoch number:  4 \tLoss: 2.3338696555126957\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.5032189276040655\n",
      "Epoch number:  4 \tLoss: 2.3339057332257735\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.5032090531174673\n",
      "Epoch number:  4 \tLoss: 2.3338974337632883\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.5032603260670845\n",
      "Epoch number:  4 \tLoss: 2.333940597543678\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.503323950361272\n",
      "Epoch number:  4 \tLoss: 2.3339943136488692\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.5032343294192922\n",
      "Epoch number:  4 \tLoss: 2.333918873605929\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.503257647371151\n",
      "Epoch number:  4 \tLoss: 2.333938609560771\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.5032504152761494\n",
      "Epoch number:  4 \tLoss: 2.333932725943912\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.5031167461173864\n",
      "Epoch number:  4 \tLoss: 2.3338190292322882\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.5032387280298667\n",
      "Epoch number:  4 \tLoss: 2.3339226770217825\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.5033471326450085\n",
      "Epoch number:  4 \tLoss: 2.334026947668947\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.503219034728001\n",
      "Epoch number:  4 \tLoss: 2.3339206627135485\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.502986139812407\n",
      "Epoch number:  4 \tLoss: 2.3337278952793095\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.503013477694404\n",
      "Epoch number:  4 \tLoss: 2.3337510942240156\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.503175967791405\n",
      "Epoch number:  4 \tLoss: 2.333884863013849\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.5030871696916837\n",
      "Epoch number:  4 \tLoss: 2.333811568674041\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.503425827656571\n",
      "Epoch number:  4 \tLoss: 2.334081609937954\n",
      "Epoch number:  6 \tLoss: 2.3055933458300886\n",
      "Epoch number:  8 \tLoss: 2.3004087072424784\n",
      "Epoch number:  10 \tLoss: 2.299452662483558\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.5030848377275454\n",
      "Epoch number:  4 \tLoss: 2.333792437686165\n",
      "Epoch number:  6 \tLoss: 2.3054833869895006\n",
      "Epoch number:  8 \tLoss: 2.3003786160795805\n",
      "Epoch number:  10 \tLoss: 2.29944676506574\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.5032880879612214\n",
      "Epoch number:  4 \tLoss: 2.333964183703614\n",
      "Epoch number:  6 \tLoss: 2.305548059853064\n",
      "Epoch number:  8 \tLoss: 2.3003956904549905\n",
      "Epoch number:  10 \tLoss: 2.299449413764768\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.503200343238537\n",
      "Epoch number:  4 \tLoss: 2.3338897147605056\n",
      "Epoch number:  6 \tLoss: 2.3055197559086937\n",
      "Epoch number:  8 \tLoss: 2.3003880119002154\n",
      "Epoch number:  10 \tLoss: 2.2994480499256076\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.5030266201002136\n",
      "Epoch number:  4 \tLoss: 2.333742748743719\n",
      "Epoch number:  6 \tLoss: 2.305463644640559\n",
      "Epoch number:  8 \tLoss: 2.300372067001953\n",
      "Epoch number:  10 \tLoss: 2.299444093894679\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.5032406804359586\n",
      "Epoch number:  4 \tLoss: 2.3339240848550213\n",
      "Epoch number:  6 \tLoss: 2.3055330494606006\n",
      "Epoch number:  8 \tLoss: 2.3003919204409806\n",
      "Epoch number:  10 \tLoss: 2.299449126256456\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.5032806736629616\n",
      "Epoch number:  4 \tLoss: 2.3339580330716116\n",
      "Epoch number:  6 \tLoss: 2.3055460169679884\n",
      "Epoch number:  8 \tLoss: 2.300395504994544\n",
      "Epoch number:  10 \tLoss: 2.2994498321493957\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.503499961146998\n",
      "Epoch number:  4 \tLoss: 2.3341453734897453\n",
      "Epoch number:  6 \tLoss: 2.3056186619312427\n",
      "Epoch number:  8 \tLoss: 2.3004167698170024\n",
      "Epoch number:  10 \tLoss: 2.2994555412611093\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.5030389574892116\n",
      "Epoch number:  4 \tLoss: 2.3337532994371917\n",
      "Epoch number:  6 \tLoss: 2.3054678865271967\n",
      "Epoch number:  8 \tLoss: 2.300373544612986\n",
      "Epoch number:  10 \tLoss: 2.299444783359849\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.5032022676053343\n",
      "Epoch number:  4 \tLoss: 2.333891427426728\n",
      "Epoch number:  6 \tLoss: 2.305520473034385\n",
      "Epoch number:  8 \tLoss: 2.3003882769261175\n",
      "Epoch number:  10 \tLoss: 2.29944817698721\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.5032067207660087\n",
      "Epoch number:  4 \tLoss: 2.3338953865906826\n",
      "Epoch number:  6 \tLoss: 2.3055222903794963\n",
      "Epoch number:  8 \tLoss: 2.3003890688143858\n",
      "Epoch number:  10 \tLoss: 2.2994485711224284\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.503106260754712\n",
      "Epoch number:  4 \tLoss: 2.3338101479550275\n",
      "Epoch number:  6 \tLoss: 2.3054895646450695\n",
      "Epoch number:  8 \tLoss: 2.3003797142714277\n",
      "Epoch number:  10 \tLoss: 2.2994463401699923\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.5033408156182517\n",
      "Epoch number:  4 \tLoss: 2.334021197381576\n",
      "Epoch number:  6 \tLoss: 2.3055817478404625\n",
      "Epoch number:  8 \tLoss: 2.3004170637581542\n",
      "Epoch number:  10 \tLoss: 2.2994666034444804\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.5031525031953605\n",
      "Epoch number:  4 \tLoss: 2.3338653726765726\n",
      "Epoch number:  6 \tLoss: 2.305526252885784\n",
      "Epoch number:  8 \tLoss: 2.3004056551646563\n",
      "Epoch number:  10 \tLoss: 2.2994683649494263\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.5033054303088758\n",
      "Epoch number:  4 \tLoss: 2.3339924491363275\n",
      "Epoch number:  6 \tLoss: 2.305572414076099\n",
      "Epoch number:  8 \tLoss: 2.3004162318502352\n",
      "Epoch number:  10 \tLoss: 2.2994682835037623\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.5034047005022675\n",
      "Epoch number:  4 \tLoss: 2.3340751547468868\n",
      "Epoch number:  6 \tLoss: 2.305602564998431\n",
      "Epoch number:  8 \tLoss: 2.300423199450422\n",
      "Epoch number:  10 \tLoss: 2.29946832949035\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.503189618259057\n",
      "Epoch number:  4 \tLoss: 2.3338961615857907\n",
      "Epoch number:  6 \tLoss: 2.30553747609692\n",
      "Epoch number:  8 \tLoss: 2.300408260996029\n",
      "Epoch number:  10 \tLoss: 2.2994683512337692\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.5030677399519976\n",
      "Epoch number:  4 \tLoss: 2.3337953282085095\n",
      "Epoch number:  6 \tLoss: 2.3055012627894196\n",
      "Epoch number:  8 \tLoss: 2.300400354338646\n",
      "Epoch number:  10 \tLoss: 2.299468909745601\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301045170100869\n",
      "Epoch number:  4 \tLoss: 1.6823564711887913\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010503325584506\n",
      "Epoch number:  4 \tLoss: 2.301049930077234\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301045763054263\n",
      "Epoch number:  4 \tLoss: 1.2112473687638445\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010471357479747\n",
      "Epoch number:  4 \tLoss: 2.3010467620354347\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010448568781308\n",
      "Epoch number:  4 \tLoss: 1.705482035387272\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301046786691078\n",
      "Epoch number:  4 \tLoss: 2.3010463843777926\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301045646053534\n",
      "Epoch number:  4 \tLoss: 1.6760362659179635\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301048175906278\n",
      "Epoch number:  4 \tLoss: 2.301047726070258\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301048929921257\n",
      "Epoch number:  4 \tLoss: 1.6896468261572875\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010479947950175\n",
      "Epoch number:  4 \tLoss: 2.301047507011008\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010484039530907\n",
      "Epoch number:  4 \tLoss: 1.6838354191117721\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010499674223226\n",
      "Epoch number:  4 \tLoss: 2.3010494115554114\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301047638947538\n",
      "Epoch number:  4 \tLoss: 2.3010466090197386\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010473568293928\n",
      "Epoch number:  4 \tLoss: 2.3010465566077807\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010478146801026\n",
      "Epoch number:  4 \tLoss: 2.3010466477794562\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010477570540244\n",
      "Epoch number:  4 \tLoss: 2.3010466354979977\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010480322150224\n",
      "Epoch number:  4 \tLoss: 2.3010466866387635\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010484917337286\n",
      "Epoch number:  4 \tLoss: 2.3010467797401533\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010490235004992\n",
      "Epoch number:  4 \tLoss: 1.6879624487497031\n",
      "Epoch number:  6 \tLoss: 0.5580654400712217\n",
      "Epoch number:  8 \tLoss: 0.3733854559940418\n",
      "Epoch number:  10 \tLoss: 0.3599879385718347\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301045822771644\n",
      "Epoch number:  4 \tLoss: 2.3010453903393606\n",
      "Epoch number:  6 \tLoss: 2.301045115543524\n",
      "Epoch number:  8 \tLoss: 2.301044939129119\n",
      "Epoch number:  10 \tLoss: 2.30104482577169\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010450258429613\n",
      "Epoch number:  4 \tLoss: 1.6878087037743414\n",
      "Epoch number:  6 \tLoss: 0.39268369998315206\n",
      "Epoch number:  8 \tLoss: 0.354127511533123\n",
      "Epoch number:  10 \tLoss: 0.3226169019787752\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301048945881656\n",
      "Epoch number:  4 \tLoss: 2.3010485323114143\n",
      "Epoch number:  6 \tLoss: 2.3010482424260372\n",
      "Epoch number:  8 \tLoss: 2.3010480302914798\n",
      "Epoch number:  10 \tLoss: 2.3010478685194258\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010471910186814\n",
      "Epoch number:  4 \tLoss: 1.6981183072579564\n",
      "Epoch number:  6 \tLoss: 0.37908968913474045\n",
      "Epoch number:  8 \tLoss: 0.30647616046472337\n",
      "Epoch number:  10 \tLoss: 0.27062804719226974\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301048643575415\n",
      "Epoch number:  4 \tLoss: 2.3010481407883625\n",
      "Epoch number:  6 \tLoss: 2.3010478049797474\n",
      "Epoch number:  8 \tLoss: 2.3010475732655764\n",
      "Epoch number:  10 \tLoss: 2.3010474081801537\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010467322411627\n",
      "Epoch number:  4 \tLoss: 1.6836568073018823\n",
      "Epoch number:  6 \tLoss: 0.5618601107175115\n",
      "Epoch number:  8 \tLoss: 0.3749280635648914\n",
      "Epoch number:  10 \tLoss: 0.34722343402101113\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010485435024317\n",
      "Epoch number:  4 \tLoss: 2.301048132208671\n",
      "Epoch number:  6 \tLoss: 2.301047850133329\n",
      "Epoch number:  8 \tLoss: 2.3010476489510454\n",
      "Epoch number:  10 \tLoss: 2.30104749984608\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010465754075513\n",
      "Epoch number:  4 \tLoss: 1.6866267518221556\n",
      "Epoch number:  6 \tLoss: 0.3688860940405044\n",
      "Epoch number:  8 \tLoss: 0.3405887782525459\n",
      "Epoch number:  10 \tLoss: 0.3303357134870845\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010464794830194\n",
      "Epoch number:  4 \tLoss: 2.3010460277543707\n",
      "Epoch number:  6 \tLoss: 2.3010457541503837\n",
      "Epoch number:  8 \tLoss: 2.3010455905513245\n",
      "Epoch number:  10 \tLoss: 2.3010454963525633\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301046822900228\n",
      "Epoch number:  4 \tLoss: 1.223694494114257\n",
      "Epoch number:  6 \tLoss: 0.3570149489369428\n",
      "Epoch number:  8 \tLoss: 0.318131620504153\n",
      "Epoch number:  10 \tLoss: 0.2891941925432732\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010488625924608\n",
      "Epoch number:  4 \tLoss: 2.3010483138337428\n",
      "Epoch number:  6 \tLoss: 2.301047950399322\n",
      "Epoch number:  8 \tLoss: 2.3010477023653655\n",
      "Epoch number:  10 \tLoss: 2.30104752802878\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301048382399954\n",
      "Epoch number:  4 \tLoss: 2.30104675441739\n",
      "Epoch number:  6 \tLoss: 2.3010465111566827\n",
      "Epoch number:  8 \tLoss: 2.3010464350025694\n",
      "Epoch number:  10 \tLoss: 2.301046398533575\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301047454906871\n",
      "Epoch number:  4 \tLoss: 2.3010465737907406\n",
      "Epoch number:  6 \tLoss: 2.3010464408687716\n",
      "Epoch number:  8 \tLoss: 2.301046399217014\n",
      "Epoch number:  10 \tLoss: 2.301046379269415\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010478423955316\n",
      "Epoch number:  4 \tLoss: 2.3010466534409124\n",
      "Epoch number:  6 \tLoss: 2.301046471939948\n",
      "Epoch number:  8 \tLoss: 2.30104641503208\n",
      "Epoch number:  10 \tLoss: 2.3010463877824185\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010482811517288\n",
      "Epoch number:  4 \tLoss: 2.301046731950456\n",
      "Epoch number:  6 \tLoss: 2.3010465022297297\n",
      "Epoch number:  8 \tLoss: 2.301046430442337\n",
      "Epoch number:  10 \tLoss: 2.3010463960774206\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010478931740774\n",
      "Epoch number:  4 \tLoss: 2.301046665853742\n",
      "Epoch number:  6 \tLoss: 2.301046476927009\n",
      "Epoch number:  8 \tLoss: 2.301046417583783\n",
      "Epoch number:  10 \tLoss: 2.301046389157141\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.30104806256438\n",
      "Epoch number:  4 \tLoss: 2.301046694891597\n",
      "Epoch number:  6 \tLoss: 2.3010464881262798\n",
      "Epoch number:  8 \tLoss: 2.3010464232856855\n",
      "Epoch number:  10 \tLoss: 2.301046392226789\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3000583187308585\n",
      "Epoch number:  4 \tLoss: 1.4665113281159639\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300057541949791\n",
      "Epoch number:  4 \tLoss: 2.3000573502649004\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3000585383541634\n",
      "Epoch number:  4 \tLoss: 1.6821379540035837\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3000598752121144\n",
      "Epoch number:  4 \tLoss: 2.3000596278554104\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300059324139534\n",
      "Epoch number:  4 \tLoss: 1.472071196909639\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300061517544686\n",
      "Epoch number:  4 \tLoss: 2.300061233313132\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3000627037226606\n",
      "Epoch number:  4 \tLoss: 2.299850057753316\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3000584755369196\n",
      "Epoch number:  4 \tLoss: 2.3000583391792486\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300060342709044\n",
      "Epoch number:  4 \tLoss: 2.299976391277861\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300062505370824\n",
      "Epoch number:  4 \tLoss: 2.300062215693581\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300061851093515\n",
      "Epoch number:  4 \tLoss: 1.2836011762522659\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3000612412772283\n",
      "Epoch number:  4 \tLoss: 2.3000609884392045\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3000650646667853\n",
      "Epoch number:  4 \tLoss: 2.300061822172223\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300064378885968\n",
      "Epoch number:  4 \tLoss: 2.3000615878575363\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3000661034961984\n",
      "Epoch number:  4 \tLoss: 2.3000623012355215\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3000636548834805\n",
      "Epoch number:  4 \tLoss: 2.3000611935351234\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3000672846556394\n",
      "Epoch number:  4 \tLoss: 2.3000631439674986\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300064192147167\n",
      "Epoch number:  4 \tLoss: 2.300061533653499\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300060200788937\n",
      "Epoch number:  4 \tLoss: 1.4626631533436305\n",
      "Epoch number:  6 \tLoss: 0.41810194264596856\n",
      "Epoch number:  8 \tLoss: 0.3891318601576321\n",
      "Epoch number:  10 \tLoss: 0.3666755856915597\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300058236214061\n",
      "Epoch number:  4 \tLoss: 2.3000581133246034\n",
      "Epoch number:  6 \tLoss: 2.300058021736666\n",
      "Epoch number:  8 \tLoss: 2.3000579533159704\n",
      "Epoch number:  10 \tLoss: 2.300057902083455\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3000594518510638\n",
      "Epoch number:  4 \tLoss: 1.6925074762451415\n",
      "Epoch number:  6 \tLoss: 0.39561199094970767\n",
      "Epoch number:  8 \tLoss: 0.34036020239704556\n",
      "Epoch number:  10 \tLoss: 0.3011482137061594\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3000590454393706\n",
      "Epoch number:  4 \tLoss: 2.3000588513411158\n",
      "Epoch number:  6 \tLoss: 2.3000587033181867\n",
      "Epoch number:  8 \tLoss: 2.3000585893693994\n",
      "Epoch number:  10 \tLoss: 2.300058500679417\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3000599313781365\n",
      "Epoch number:  4 \tLoss: 1.69392187068343\n",
      "Epoch number:  6 \tLoss: 0.38800740362153185\n",
      "Epoch number:  8 \tLoss: 0.3303973251665094\n",
      "Epoch number:  10 \tLoss: 0.3066655486426396\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300060881828841\n",
      "Epoch number:  4 \tLoss: 2.300060575251738\n",
      "Epoch number:  6 \tLoss: 2.300060333972685\n",
      "Epoch number:  8 \tLoss: 2.3000601410459187\n",
      "Epoch number:  10 \tLoss: 2.3000599840259164\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300061046723381\n",
      "Epoch number:  4 \tLoss: 2.300045313788437\n",
      "Epoch number:  6 \tLoss: 0.6789889775522177\n",
      "Epoch number:  8 \tLoss: 0.3702615473371087\n",
      "Epoch number:  10 \tLoss: 0.35731867148322055\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300061186057581\n",
      "Epoch number:  4 \tLoss: 2.3000609267919963\n",
      "Epoch number:  6 \tLoss: 2.3000607241905966\n",
      "Epoch number:  8 \tLoss: 2.3000605634418907\n",
      "Epoch number:  10 \tLoss: 2.3000604336700268\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3000583124790137\n",
      "Epoch number:  4 \tLoss: 1.6841005866748815\n",
      "Epoch number:  6 \tLoss: 0.4559094904239842\n",
      "Epoch number:  8 \tLoss: 0.3559888583802176\n",
      "Epoch number:  10 \tLoss: 0.3245071174895949\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3000605715151097\n",
      "Epoch number:  4 \tLoss: 2.300060377524867\n",
      "Epoch number:  6 \tLoss: 2.300060235349878\n",
      "Epoch number:  8 \tLoss: 2.3000601309572293\n",
      "Epoch number:  10 \tLoss: 2.300060054054769\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300059033805462\n",
      "Epoch number:  4 \tLoss: 1.0501992442887829\n",
      "Epoch number:  6 \tLoss: 0.35268296455290193\n",
      "Epoch number:  8 \tLoss: 0.31618915200320696\n",
      "Epoch number:  10 \tLoss: 0.2911039135137161\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3000606562884904\n",
      "Epoch number:  4 \tLoss: 2.3000604113520264\n",
      "Epoch number:  6 \tLoss: 2.300060218694979\n",
      "Epoch number:  8 \tLoss: 2.300060064700878\n",
      "Epoch number:  10 \tLoss: 2.300059939372409\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300062860086116\n",
      "Epoch number:  4 \tLoss: 2.3000608716516497\n",
      "Epoch number:  6 \tLoss: 2.300059706112957\n",
      "Epoch number:  8 \tLoss: 2.300059048382672\n",
      "Epoch number:  10 \tLoss: 2.3000586826360276\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3000611769343653\n",
      "Epoch number:  4 \tLoss: 2.300060037036582\n",
      "Epoch number:  6 \tLoss: 2.300059278613528\n",
      "Epoch number:  8 \tLoss: 2.3000588219795604\n",
      "Epoch number:  10 \tLoss: 2.300058558262242\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3000645806863194\n",
      "Epoch number:  4 \tLoss: 2.3000615572746397\n",
      "Epoch number:  6 \tLoss: 2.300059991196087\n",
      "Epoch number:  8 \tLoss: 2.3000591734810345\n",
      "Epoch number:  10 \tLoss: 2.3000587412211067\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3000652803849144\n",
      "Epoch number:  4 \tLoss: 2.300061993917108\n",
      "Epoch number:  6 \tLoss: 2.300060251435451\n",
      "Epoch number:  8 \tLoss: 2.3000593263580806\n",
      "Epoch number:  10 \tLoss: 2.300058831173691\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3000661179402746\n",
      "Epoch number:  4 \tLoss: 2.3000624297780448\n",
      "Epoch number:  6 \tLoss: 2.300060480807311\n",
      "Epoch number:  8 \tLoss: 2.300059449283324\n",
      "Epoch number:  10 \tLoss: 2.300058899126188\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300064135398484\n",
      "Epoch number:  4 \tLoss: 2.3000612818485284\n",
      "Epoch number:  6 \tLoss: 2.3000598321761134\n",
      "Epoch number:  8 \tLoss: 2.3000590835447445\n",
      "Epoch number:  10 \tLoss: 2.3000586899200006\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2995727193472697\n",
      "Epoch number:  4 \tLoss: 2.2995062102991484\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2995759331277963\n",
      "Epoch number:  4 \tLoss: 2.2995754029890243\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2995751205071375\n",
      "Epoch number:  4 \tLoss: 1.9424241666780178\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299575117316103\n",
      "Epoch number:  4 \tLoss: 2.2995746941381414\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2995764954359315\n",
      "Epoch number:  4 \tLoss: 1.4754744450442865\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2995749340257925\n",
      "Epoch number:  4 \tLoss: 2.2995744457372864\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299575215365071\n",
      "Epoch number:  4 \tLoss: 2.299562241891953\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299576772715555\n",
      "Epoch number:  4 \tLoss: 2.299576216238994\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2995690679101277\n",
      "Epoch number:  4 \tLoss: 1.3919615186019543\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299580478243214\n",
      "Epoch number:  4 \tLoss: 2.2995798022608103\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299578164965622\n",
      "Epoch number:  4 \tLoss: 1.6913375608366414\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2995716984885344\n",
      "Epoch number:  4 \tLoss: 2.299571141439006\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2995843383472834\n",
      "Epoch number:  4 \tLoss: 2.2995769779779556\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299583200138816\n",
      "Epoch number:  4 \tLoss: 2.2995809521453565\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299581109857167\n",
      "Epoch number:  4 \tLoss: 2.299575035946638\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299584773203422\n",
      "Epoch number:  4 \tLoss: 2.2995822328332842\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299583154801273\n",
      "Epoch number:  4 \tLoss: 2.299576594126367\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2995864382458735\n",
      "Epoch number:  4 \tLoss: 2.2995835296739187\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2995746186320636\n",
      "Epoch number:  4 \tLoss: 2.2995600726406056\n",
      "Epoch number:  6 \tLoss: 0.688410810861589\n",
      "Epoch number:  8 \tLoss: 0.39029334104521696\n",
      "Epoch number:  10 \tLoss: 0.3716737237195381\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299572631875599\n",
      "Epoch number:  4 \tLoss: 2.29957209934576\n",
      "Epoch number:  6 \tLoss: 2.2995716336004013\n",
      "Epoch number:  8 \tLoss: 2.299571220232589\n",
      "Epoch number:  10 \tLoss: 2.299570852944728\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2995710956914173\n",
      "Epoch number:  4 \tLoss: 1.5954417303293245\n",
      "Epoch number:  6 \tLoss: 0.38710223048778464\n",
      "Epoch number:  8 \tLoss: 0.3471848307772026\n",
      "Epoch number:  10 \tLoss: 0.314881991677681\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299579145569743\n",
      "Epoch number:  4 \tLoss: 2.2995785351365683\n",
      "Epoch number:  6 \tLoss: 2.2995779884175245\n",
      "Epoch number:  8 \tLoss: 2.299577491462688\n",
      "Epoch number:  10 \tLoss: 2.299577038420273\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299573771361633\n",
      "Epoch number:  4 \tLoss: 1.6961731264436275\n",
      "Epoch number:  6 \tLoss: 0.540615825439334\n",
      "Epoch number:  8 \tLoss: 0.32879215574351434\n",
      "Epoch number:  10 \tLoss: 0.30383571383550817\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299578651036814\n",
      "Epoch number:  4 \tLoss: 2.299578039679797\n",
      "Epoch number:  6 \tLoss: 2.29957749564988\n",
      "Epoch number:  8 \tLoss: 2.299577004533744\n",
      "Epoch number:  10 \tLoss: 2.29957655999786\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299570487372304\n",
      "Epoch number:  4 \tLoss: 2.2995489735374988\n",
      "Epoch number:  6 \tLoss: 0.7611039905127904\n",
      "Epoch number:  8 \tLoss: 0.39374124144932526\n",
      "Epoch number:  10 \tLoss: 0.36663903643928775\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2995737804984926\n",
      "Epoch number:  4 \tLoss: 2.2995734264624526\n",
      "Epoch number:  6 \tLoss: 2.2995731108918287\n",
      "Epoch number:  8 \tLoss: 2.2995728223034315\n",
      "Epoch number:  10 \tLoss: 2.299572557541223\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2995735073488515\n",
      "Epoch number:  4 \tLoss: 1.4773822824082228\n",
      "Epoch number:  6 \tLoss: 0.38228351475312883\n",
      "Epoch number:  8 \tLoss: 0.3455562563977326\n",
      "Epoch number:  10 \tLoss: 0.3133972801971728\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2995791081855845\n",
      "Epoch number:  4 \tLoss: 2.2995785876410233\n",
      "Epoch number:  6 \tLoss: 2.2995781222160114\n",
      "Epoch number:  8 \tLoss: 2.299577698737176\n",
      "Epoch number:  10 \tLoss: 2.2995773122041476\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299578529725254\n",
      "Epoch number:  4 \tLoss: 1.4609234038637655\n",
      "Epoch number:  6 \tLoss: 0.36413432838962684\n",
      "Epoch number:  8 \tLoss: 0.30722730831109185\n",
      "Epoch number:  10 \tLoss: 0.2710000763931225\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2995738501005554\n",
      "Epoch number:  4 \tLoss: 2.2995732731268057\n",
      "Epoch number:  6 \tLoss: 2.29957276113817\n",
      "Epoch number:  8 \tLoss: 2.2995723000023327\n",
      "Epoch number:  10 \tLoss: 2.299571883709253\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299576951463574\n",
      "Epoch number:  4 \tLoss: 2.2994569662476714\n",
      "Epoch number:  6 \tLoss: 0.7017379724368633\n",
      "Epoch number:  8 \tLoss: 0.4115608960874711\n",
      "Epoch number:  10 \tLoss: 0.3518113640981704\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2995838269974804\n",
      "Epoch number:  4 \tLoss: 2.29958143104583\n",
      "Epoch number:  6 \tLoss: 2.2995794211955114\n",
      "Epoch number:  8 \tLoss: 2.2995777363611594\n",
      "Epoch number:  10 \tLoss: 2.2995763284246404\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2995874666328473\n",
      "Epoch number:  4 \tLoss: 2.2995800060500584\n",
      "Epoch number:  6 \tLoss: 1.1715375300794648\n",
      "Epoch number:  8 \tLoss: 0.3858666646250289\n",
      "Epoch number:  10 \tLoss: 0.3494649517872238\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2995854213850224\n",
      "Epoch number:  4 \tLoss: 2.2995824834900636\n",
      "Epoch number:  6 \tLoss: 2.299580091323839\n",
      "Epoch number:  8 \tLoss: 2.299578139890716\n",
      "Epoch number:  10 \tLoss: 2.299576548841588\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299586769650858\n",
      "Epoch number:  4 \tLoss: 2.2995772367720724\n",
      "Epoch number:  6 \tLoss: 0.7989650495696061\n",
      "Epoch number:  8 \tLoss: 0.3472875735840782\n",
      "Epoch number:  10 \tLoss: 0.30401291164171584\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299589724227576\n",
      "Epoch number:  4 \tLoss: 2.2995861444975843\n",
      "Epoch number:  6 \tLoss: 2.299583189984828\n",
      "Epoch number:  8 \tLoss: 2.2995807514714315\n",
      "Epoch number:  10 \tLoss: 2.2995787421285354\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010305558719946\n",
      "Epoch number:  4 \tLoss: 2.301004561816677\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010322256336813\n",
      "Epoch number:  4 \tLoss: 2.301031878420064\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301032664664709\n",
      "Epoch number:  4 \tLoss: 1.6951131132298207\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010331382060825\n",
      "Epoch number:  4 \tLoss: 2.3010326774724907\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301030260777937\n",
      "Epoch number:  4 \tLoss: 1.4575510018438513\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301034521997275\n",
      "Epoch number:  4 \tLoss: 2.3010339955829298\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301030964058524\n",
      "Epoch number:  4 \tLoss: 1.680679826406363\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301031169446403\n",
      "Epoch number:  4 \tLoss: 2.301030855582318\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301029480124808\n",
      "Epoch number:  4 \tLoss: 1.6884187034739009\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010329821911784\n",
      "Epoch number:  4 \tLoss: 2.301032540143181\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010330217870347\n",
      "Epoch number:  4 \tLoss: 1.280841242827461\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010345853256022\n",
      "Epoch number:  4 \tLoss: 2.3010339633203456\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010331951154526\n",
      "Epoch number:  4 \tLoss: 2.3010316629675267\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010326392964977\n",
      "Epoch number:  4 \tLoss: 2.3010315579063616\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301032606479773\n",
      "Epoch number:  4 \tLoss: 2.3010315444074663\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010331157688406\n",
      "Epoch number:  4 \tLoss: 2.3010316523684717\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301032838050947\n",
      "Epoch number:  4 \tLoss: 2.3010315967732446\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301033120309418\n",
      "Epoch number:  4 \tLoss: 2.301031643851053\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010324530548485\n",
      "Epoch number:  4 \tLoss: 1.676231095452873\n",
      "Epoch number:  6 \tLoss: 0.42888059153574326\n",
      "Epoch number:  8 \tLoss: 0.37254216441430904\n",
      "Epoch number:  10 \tLoss: 0.3674628188726276\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010345806988606\n",
      "Epoch number:  4 \tLoss: 2.3010341511144534\n",
      "Epoch number:  6 \tLoss: 2.30103384386335\n",
      "Epoch number:  8 \tLoss: 2.3010336138886562\n",
      "Epoch number:  10 \tLoss: 2.3010334341992813\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301031865405292\n",
      "Epoch number:  4 \tLoss: 2.301019864957369\n",
      "Epoch number:  6 \tLoss: 0.5638160830755141\n",
      "Epoch number:  8 \tLoss: 0.33601708429941324\n",
      "Epoch number:  10 \tLoss: 0.3213671803627309\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301032904930904\n",
      "Epoch number:  4 \tLoss: 2.301032453523594\n",
      "Epoch number:  6 \tLoss: 2.3010321660720727\n",
      "Epoch number:  8 \tLoss: 2.301031980353588\n",
      "Epoch number:  10 \tLoss: 2.301031859146085\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010316988331048\n",
      "Epoch number:  4 \tLoss: 1.463689025183584\n",
      "Epoch number:  6 \tLoss: 0.3455526108027736\n",
      "Epoch number:  8 \tLoss: 0.29899890931982936\n",
      "Epoch number:  10 \tLoss: 0.27400786545118966\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010341140827064\n",
      "Epoch number:  4 \tLoss: 2.3010336208411344\n",
      "Epoch number:  6 \tLoss: 2.301033284228434\n",
      "Epoch number:  8 \tLoss: 2.3010330457044996\n",
      "Epoch number:  10 \tLoss: 2.3010328702311185\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301030102508102\n",
      "Epoch number:  4 \tLoss: 1.1810819706398001\n",
      "Epoch number:  6 \tLoss: 0.4062175165212794\n",
      "Epoch number:  8 \tLoss: 0.3682057651030769\n",
      "Epoch number:  10 \tLoss: 0.3534647096941695\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.30103314981758\n",
      "Epoch number:  4 \tLoss: 2.3010328044984694\n",
      "Epoch number:  6 \tLoss: 2.3010325851319764\n",
      "Epoch number:  8 \tLoss: 2.3010324434126552\n",
      "Epoch number:  10 \tLoss: 2.301032350460241\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010307237433456\n",
      "Epoch number:  4 \tLoss: 1.6919422770976051\n",
      "Epoch number:  6 \tLoss: 0.39759513343154385\n",
      "Epoch number:  8 \tLoss: 0.331253282951819\n",
      "Epoch number:  10 \tLoss: 0.3059802392503669\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301035656017718\n",
      "Epoch number:  4 \tLoss: 2.3010351049681215\n",
      "Epoch number:  6 \tLoss: 2.301034713960547\n",
      "Epoch number:  8 \tLoss: 2.301034423845794\n",
      "Epoch number:  10 \tLoss: 2.301034199162417\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010316150898387\n",
      "Epoch number:  4 \tLoss: 1.4658928105701248\n",
      "Epoch number:  6 \tLoss: 0.356625692956918\n",
      "Epoch number:  8 \tLoss: 0.312177242352846\n",
      "Epoch number:  10 \tLoss: 0.2991410126445602\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301032117132175\n",
      "Epoch number:  4 \tLoss: 2.3010317610395936\n",
      "Epoch number:  6 \tLoss: 2.301031538439313\n",
      "Epoch number:  8 \tLoss: 2.3010313987274995\n",
      "Epoch number:  10 \tLoss: 2.3010313115919643\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301032103085805\n",
      "Epoch number:  4 \tLoss: 2.3010314447248055\n",
      "Epoch number:  6 \tLoss: 2.3010313485615685\n",
      "Epoch number:  8 \tLoss: 2.3010313186456433\n",
      "Epoch number:  10 \tLoss: 2.3010313042839936\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301032832782975\n",
      "Epoch number:  4 \tLoss: 2.3010315774249057\n",
      "Epoch number:  6 \tLoss: 2.3010313999861984\n",
      "Epoch number:  8 \tLoss: 2.3010313449419884\n",
      "Epoch number:  10 \tLoss: 2.301031318515535\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301032906658409\n",
      "Epoch number:  4 \tLoss: 2.3010316080450695\n",
      "Epoch number:  6 \tLoss: 2.3010314126380647\n",
      "Epoch number:  8 \tLoss: 2.301031351454248\n",
      "Epoch number:  10 \tLoss: 2.3010313220430803\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010332468137813\n",
      "Epoch number:  4 \tLoss: 2.3010316701407167\n",
      "Epoch number:  6 \tLoss: 2.3010314364358173\n",
      "Epoch number:  8 \tLoss: 2.3010313635859587\n",
      "Epoch number:  10 \tLoss: 2.3010313286051627\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010328924085073\n",
      "Epoch number:  4 \tLoss: 2.3010316071095227\n",
      "Epoch number:  6 \tLoss: 2.301031412186944\n",
      "Epoch number:  8 \tLoss: 2.301031351205297\n",
      "Epoch number:  10 \tLoss: 2.301031321906517\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010329805492415\n",
      "Epoch number:  4 \tLoss: 2.3010316196777363\n",
      "Epoch number:  6 \tLoss: 2.301031416860239\n",
      "Epoch number:  8 \tLoss: 2.3010313535809805\n",
      "Epoch number:  10 \tLoss: 2.3010313231910833\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3000532779605334\n",
      "Epoch number:  4 \tLoss: 2.3000370365138987\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300056588878339\n",
      "Epoch number:  4 \tLoss: 2.300056301378908\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3000500116769516\n",
      "Epoch number:  4 \tLoss: 1.6814418195835301\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.30005731267713\n",
      "Epoch number:  4 \tLoss: 2.3000569410689127\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300052404801903\n",
      "Epoch number:  4 \tLoss: 1.2006713200160635\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300052586291143\n",
      "Epoch number:  4 \tLoss: 2.3000523588516772\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3000559742478184\n",
      "Epoch number:  4 \tLoss: 2.3000534375359907\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3000542947722713\n",
      "Epoch number:  4 \tLoss: 2.300054024902263\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300054948659441\n",
      "Epoch number:  4 \tLoss: 1.470460278046525\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300054212513505\n",
      "Epoch number:  4 \tLoss: 2.300053873993699\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3000526419986613\n",
      "Epoch number:  4 \tLoss: 1.6790522184161676\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300053915314348\n",
      "Epoch number:  4 \tLoss: 2.300053623550523\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3000597782584076\n",
      "Epoch number:  4 \tLoss: 2.300055723405126\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300058026875464\n",
      "Epoch number:  4 \tLoss: 2.3000549521630194\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3000587696975696\n",
      "Epoch number:  4 \tLoss: 2.3000554771502406\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300056893831677\n",
      "Epoch number:  4 \tLoss: 2.3000543142813554\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3000587652776088\n",
      "Epoch number:  4 \tLoss: 2.3000553221611204\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3000583417868934\n",
      "Epoch number:  4 \tLoss: 2.3000549744666707\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3000547968479244\n",
      "Epoch number:  4 \tLoss: 2.3000432000281927\n",
      "Epoch number:  6 \tLoss: 0.6876785420019105\n",
      "Epoch number:  8 \tLoss: 0.40268660441448\n",
      "Epoch number:  10 \tLoss: 0.36361889432520883\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.30005350940425\n",
      "Epoch number:  4 \tLoss: 2.300053258403728\n",
      "Epoch number:  6 \tLoss: 2.300053065139218\n",
      "Epoch number:  8 \tLoss: 2.300052914666391\n",
      "Epoch number:  10 \tLoss: 2.3000527959679617\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300050861039051\n",
      "Epoch number:  4 \tLoss: 2.300032256068026\n",
      "Epoch number:  6 \tLoss: 0.5594479222680271\n",
      "Epoch number:  8 \tLoss: 0.334058815449173\n",
      "Epoch number:  10 \tLoss: 0.30759477357984427\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300055159100627\n",
      "Epoch number:  4 \tLoss: 2.3000548885241185\n",
      "Epoch number:  6 \tLoss: 2.3000546752619373\n",
      "Epoch number:  8 \tLoss: 2.300054504523008\n",
      "Epoch number:  10 \tLoss: 2.300054365409584\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300051933873771\n",
      "Epoch number:  4 \tLoss: 1.6912617268294998\n",
      "Epoch number:  6 \tLoss: 0.37871270842970123\n",
      "Epoch number:  8 \tLoss: 0.30431378037576706\n",
      "Epoch number:  10 \tLoss: 0.2799386813643349\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3000533844204165\n",
      "Epoch number:  4 \tLoss: 2.3000530651317717\n",
      "Epoch number:  6 \tLoss: 2.300052820039698\n",
      "Epoch number:  8 \tLoss: 2.300052629942855\n",
      "Epoch number:  10 \tLoss: 2.3000524806883624\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3000515926328657\n",
      "Epoch number:  4 \tLoss: 1.6783902664292807\n",
      "Epoch number:  6 \tLoss: 0.40987840714765705\n",
      "Epoch number:  8 \tLoss: 0.36867864193978844\n",
      "Epoch number:  10 \tLoss: 0.3619648696273896\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3000546153141266\n",
      "Epoch number:  4 \tLoss: 2.300054423266511\n",
      "Epoch number:  6 \tLoss: 2.300054272840993\n",
      "Epoch number:  8 \tLoss: 2.3000541531191883\n",
      "Epoch number:  10 \tLoss: 2.300054056060324\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3000538234048724\n",
      "Epoch number:  4 \tLoss: 2.300030547517974\n",
      "Epoch number:  6 \tLoss: 0.5665379184927668\n",
      "Epoch number:  8 \tLoss: 0.34126500061004134\n",
      "Epoch number:  10 \tLoss: 0.3090046450010022\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3000530216766215\n",
      "Epoch number:  4 \tLoss: 2.3000527990130606\n",
      "Epoch number:  6 \tLoss: 2.3000526263805208\n",
      "Epoch number:  8 \tLoss: 2.300052491023443\n",
      "Epoch number:  10 \tLoss: 2.3000523835326647\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3000535078344853\n",
      "Epoch number:  4 \tLoss: 1.1800639164168647\n",
      "Epoch number:  6 \tLoss: 0.3406018477839233\n",
      "Epoch number:  8 \tLoss: 0.31207644774761506\n",
      "Epoch number:  10 \tLoss: 0.2867680213609275\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300051639002795\n",
      "Epoch number:  4 \tLoss: 2.3000513543773864\n",
      "Epoch number:  6 \tLoss: 2.3000511378939676\n",
      "Epoch number:  8 \tLoss: 2.3000509720487834\n",
      "Epoch number:  10 \tLoss: 2.3000508439320844\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3000605170564934\n",
      "Epoch number:  4 \tLoss: 2.3000562738161636\n",
      "Epoch number:  6 \tLoss: 2.3000540269758396\n",
      "Epoch number:  8 \tLoss: 2.300052840020337\n",
      "Epoch number:  10 \tLoss: 2.300052207705811\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300056200197029\n",
      "Epoch number:  4 \tLoss: 2.3000537852069662\n",
      "Epoch number:  6 \tLoss: 2.300052624564029\n",
      "Epoch number:  8 \tLoss: 2.3000520488086167\n",
      "Epoch number:  10 \tLoss: 2.3000517545274444\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300060770549882\n",
      "Epoch number:  4 \tLoss: 2.30005639819726\n",
      "Epoch number:  6 \tLoss: 2.300054091698423\n",
      "Epoch number:  8 \tLoss: 2.3000528740229953\n",
      "Epoch number:  10 \tLoss: 2.3000522261901835\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3000585185414115\n",
      "Epoch number:  4 \tLoss: 2.3000552431929697\n",
      "Epoch number:  6 \tLoss: 2.3000534904906007\n",
      "Epoch number:  8 \tLoss: 2.3000525536730616\n",
      "Epoch number:  10 \tLoss: 2.300052049717111\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3000584372072406\n",
      "Epoch number:  4 \tLoss: 2.3000553093344713\n",
      "Epoch number:  6 \tLoss: 2.3000535620939213\n",
      "Epoch number:  8 \tLoss: 2.3000526046518663\n",
      "Epoch number:  10 \tLoss: 2.300052082469341\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300058846555419\n",
      "Epoch number:  4 \tLoss: 2.3000554613582174\n",
      "Epoch number:  6 \tLoss: 2.300053623661687\n",
      "Epoch number:  8 \tLoss: 2.3000526325268704\n",
      "Epoch number:  10 \tLoss: 2.3000520962679754\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299570456956502\n",
      "Epoch number:  4 \tLoss: 2.2995453767304395\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299569555010579\n",
      "Epoch number:  4 \tLoss: 2.2995690126087385\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2995711850801124\n",
      "Epoch number:  4 \tLoss: 1.6871397002233885\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2995746416270366\n",
      "Epoch number:  4 \tLoss: 2.299573984112365\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2995729476528552\n",
      "Epoch number:  4 \tLoss: 1.6955239735488425\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2995748850751294\n",
      "Epoch number:  4 \tLoss: 2.299574309054929\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2995699530126634\n",
      "Epoch number:  4 \tLoss: 2.2995611546323427\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.29957419793042\n",
      "Epoch number:  4 \tLoss: 2.299573733571369\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299570439763969\n",
      "Epoch number:  4 \tLoss: 1.444465207380357\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299573584773602\n",
      "Epoch number:  4 \tLoss: 2.299573108599898\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.29957294453767\n",
      "Epoch number:  4 \tLoss: 1.6984687376578813\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299570994058942\n",
      "Epoch number:  4 \tLoss: 2.2995704779976514\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2995725514001606\n",
      "Epoch number:  4 \tLoss: 2.2995708039807283\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2995760064813306\n",
      "Epoch number:  4 \tLoss: 2.2995744703280314\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2995864192461055\n",
      "Epoch number:  4 \tLoss: 2.2995794090288575\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299582567816684\n",
      "Epoch number:  4 \tLoss: 2.2995796018292105\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299584263281396\n",
      "Epoch number:  4 \tLoss: 2.2995762351177413\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299587097792454\n",
      "Epoch number:  4 \tLoss: 2.2995834775949553\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299571421942528\n",
      "Epoch number:  4 \tLoss: 1.6723553830253233\n",
      "Epoch number:  6 \tLoss: 0.50486881229639\n",
      "Epoch number:  8 \tLoss: 0.36561765602510343\n",
      "Epoch number:  10 \tLoss: 0.3619654733838369\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299567163940301\n",
      "Epoch number:  4 \tLoss: 2.2995668227199424\n",
      "Epoch number:  6 \tLoss: 2.2995665299619206\n",
      "Epoch number:  8 \tLoss: 2.2995662715269014\n",
      "Epoch number:  10 \tLoss: 2.2995660432823\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2995629921233083\n",
      "Epoch number:  4 \tLoss: 1.4234997163941854\n",
      "Epoch number:  6 \tLoss: 0.3584271112787925\n",
      "Epoch number:  8 \tLoss: 0.3331368418896732\n",
      "Epoch number:  10 \tLoss: 0.3116571018804033\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2995707640493275\n",
      "Epoch number:  4 \tLoss: 2.299570177923115\n",
      "Epoch number:  6 \tLoss: 2.2995696548788667\n",
      "Epoch number:  8 \tLoss: 2.2995691795778566\n",
      "Epoch number:  10 \tLoss: 2.2995687465020525\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.29957580830096\n",
      "Epoch number:  4 \tLoss: 1.7046702187125298\n",
      "Epoch number:  6 \tLoss: 0.4616197989616514\n",
      "Epoch number:  8 \tLoss: 0.30447859106840136\n",
      "Epoch number:  10 \tLoss: 0.28030328491459844\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299572464656004\n",
      "Epoch number:  4 \tLoss: 2.2995718723668315\n",
      "Epoch number:  6 \tLoss: 2.299571343160442\n",
      "Epoch number:  8 \tLoss: 2.2995708617217825\n",
      "Epoch number:  10 \tLoss: 2.299570422549692\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299565885607523\n",
      "Epoch number:  4 \tLoss: 2.1801532321427812\n",
      "Epoch number:  6 \tLoss: 0.5688048100782624\n",
      "Epoch number:  8 \tLoss: 0.37990677771371806\n",
      "Epoch number:  10 \tLoss: 0.35869756222792953\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2995731436427356\n",
      "Epoch number:  4 \tLoss: 2.2995726922282826\n",
      "Epoch number:  6 \tLoss: 2.2995722886200713\n",
      "Epoch number:  8 \tLoss: 2.2995719188826405\n",
      "Epoch number:  10 \tLoss: 2.2995715790794318\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2995719956357115\n",
      "Epoch number:  4 \tLoss: 1.4739387155123729\n",
      "Epoch number:  6 \tLoss: 0.37287456818991627\n",
      "Epoch number:  8 \tLoss: 0.32937317761920476\n",
      "Epoch number:  10 \tLoss: 0.2962950111383759\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299572832004495\n",
      "Epoch number:  4 \tLoss: 2.2995721478797524\n",
      "Epoch number:  6 \tLoss: 2.2995715412827065\n",
      "Epoch number:  8 \tLoss: 2.2995709954592543\n",
      "Epoch number:  10 \tLoss: 2.299570503275099\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2995704230489302\n",
      "Epoch number:  4 \tLoss: 1.6911476201639895\n",
      "Epoch number:  6 \tLoss: 0.3685973198668958\n",
      "Epoch number:  8 \tLoss: 0.30389067606719716\n",
      "Epoch number:  10 \tLoss: 0.26891450182599763\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299571461790993\n",
      "Epoch number:  4 \tLoss: 2.299570917391643\n",
      "Epoch number:  6 \tLoss: 2.299570434402203\n",
      "Epoch number:  8 \tLoss: 2.299569997615953\n",
      "Epoch number:  10 \tLoss: 2.299569601657889\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2995761539229402\n",
      "Epoch number:  4 \tLoss: 2.299573724205631\n",
      "Epoch number:  6 \tLoss: 2.29952141140702\n",
      "Epoch number:  8 \tLoss: 0.677973137189417\n",
      "Epoch number:  10 \tLoss: 0.3693738552109085\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299585449657825\n",
      "Epoch number:  4 \tLoss: 2.2995818522273894\n",
      "Epoch number:  6 \tLoss: 2.2995789135545057\n",
      "Epoch number:  8 \tLoss: 2.2995765098213576\n",
      "Epoch number:  10 \tLoss: 2.299574545524676\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2995875321751584\n",
      "Epoch number:  4 \tLoss: 2.29958338087876\n",
      "Epoch number:  6 \tLoss: 2.2995710176350883\n",
      "Epoch number:  8 \tLoss: 0.731049924529014\n",
      "Epoch number:  10 \tLoss: 0.34169053418332956\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2995859208077016\n",
      "Epoch number:  4 \tLoss: 2.299582432473618\n",
      "Epoch number:  6 \tLoss: 2.29957953731149\n",
      "Epoch number:  8 \tLoss: 2.299577134509123\n",
      "Epoch number:  10 \tLoss: 2.299575144501735\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299584933688549\n",
      "Epoch number:  4 \tLoss: 2.2995767345827605\n",
      "Epoch number:  6 \tLoss: 1.0963838793192706\n",
      "Epoch number:  8 \tLoss: 0.3531496706745183\n",
      "Epoch number:  10 \tLoss: 0.30127187626540236\n",
      "Training with params: learning_rate=0.001, activation=tanh, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299585927610318\n",
      "Epoch number:  4 \tLoss: 2.2995822301657753\n",
      "Epoch number:  6 \tLoss: 2.2995792143957376\n",
      "Epoch number:  8 \tLoss: 2.2995767508702074\n",
      "Epoch number:  10 \tLoss: 2.2995747398752906\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.006062350328966\n",
      "Epoch number:  4 \tLoss: 2.596791012785101\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.0060605881554827\n",
      "Epoch number:  4 \tLoss: 2.5968138689066196\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.005390718996115\n",
      "Epoch number:  4 \tLoss: 2.5958467547445827\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.0047870057564787\n",
      "Epoch number:  4 \tLoss: 2.595545833822957\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.998120832421452\n",
      "Epoch number:  4 \tLoss: 2.5848659873012263\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.0028453119045415\n",
      "Epoch number:  4 \tLoss: 2.593601128596834\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.0056980472231722\n",
      "Epoch number:  4 \tLoss: 2.5963656352683575\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.005926556040023\n",
      "Epoch number:  4 \tLoss: 2.596680478391659\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.0057047829130084\n",
      "Epoch number:  4 \tLoss: 2.5961488298779236\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.00496174264873\n",
      "Epoch number:  4 \tLoss: 2.59571918373348\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.9972167194834807\n",
      "Epoch number:  4 \tLoss: 2.5816392775550283\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.0016450287678427\n",
      "Epoch number:  4 \tLoss: 2.592426317210881\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.0055717044196526\n",
      "Epoch number:  4 \tLoss: 2.5963136119192414\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.006292699731888\n",
      "Epoch number:  4 \tLoss: 2.5970783604210186\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.0052007621331023\n",
      "Epoch number:  4 \tLoss: 2.595759676748702\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.0050366053857975\n",
      "Epoch number:  4 \tLoss: 2.595868723248323\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.998801349463335\n",
      "Epoch number:  4 \tLoss: 2.5867890303657557\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.0023317273852146\n",
      "Epoch number:  4 \tLoss: 2.593266843795776\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.0063771643742316\n",
      "Epoch number:  4 \tLoss: 2.597107610861198\n",
      "Epoch number:  6 \tLoss: 2.4393634038333514\n",
      "Epoch number:  8 \tLoss: 2.367831118966978\n",
      "Epoch number:  10 \tLoss: 2.3333802885952473\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.006828881552931\n",
      "Epoch number:  4 \tLoss: 2.5975816979722293\n",
      "Epoch number:  6 \tLoss: 2.439796029424176\n",
      "Epoch number:  8 \tLoss: 2.3681898555726444\n",
      "Epoch number:  10 \tLoss: 2.3336694767370005\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.004311571063745\n",
      "Epoch number:  4 \tLoss: 2.5948183534660867\n",
      "Epoch number:  6 \tLoss: 2.4370907310504277\n",
      "Epoch number:  8 \tLoss: 2.36567719982532\n",
      "Epoch number:  10 \tLoss: 2.3311943081510957\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.005290789740889\n",
      "Epoch number:  4 \tLoss: 2.596046194359805\n",
      "Epoch number:  6 \tLoss: 2.438461116800477\n",
      "Epoch number:  8 \tLoss: 2.3671663991932896\n",
      "Epoch number:  10 \tLoss: 2.3329534816242816\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.9965749580527103\n",
      "Epoch number:  4 \tLoss: 2.580462149901722\n",
      "Epoch number:  6 \tLoss: 2.3991523557693433\n",
      "Epoch number:  8 \tLoss: 1.9969365130235184\n",
      "Epoch number:  10 \tLoss: 1.8292876603121497\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.002372634598438\n",
      "Epoch number:  4 \tLoss: 2.5931419462957064\n",
      "Epoch number:  6 \tLoss: 2.4359525417794012\n",
      "Epoch number:  8 \tLoss: 2.365259052180203\n",
      "Epoch number:  10 \tLoss: 2.33163069978365\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.0061440586640176\n",
      "Epoch number:  4 \tLoss: 2.596859045086495\n",
      "Epoch number:  6 \tLoss: 2.439133804318977\n",
      "Epoch number:  8 \tLoss: 2.367637367574621\n",
      "Epoch number:  10 \tLoss: 2.3332219074320983\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.0057180171816387\n",
      "Epoch number:  4 \tLoss: 2.596472646007639\n",
      "Epoch number:  6 \tLoss: 2.4388305205139007\n",
      "Epoch number:  8 \tLoss: 2.367448318194669\n",
      "Epoch number:  10 \tLoss: 2.3331497603156213\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.0040141866141394\n",
      "Epoch number:  4 \tLoss: 2.5943613149035323\n",
      "Epoch number:  6 \tLoss: 2.436404983579653\n",
      "Epoch number:  8 \tLoss: 2.3645981875453876\n",
      "Epoch number:  10 \tLoss: 2.329252639433225\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.005366994010673\n",
      "Epoch number:  4 \tLoss: 2.5961115679566555\n",
      "Epoch number:  6 \tLoss: 2.4385162313037254\n",
      "Epoch number:  8 \tLoss: 2.367207897771978\n",
      "Epoch number:  10 \tLoss: 2.3329820451358247\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.996844056028068\n",
      "Epoch number:  4 \tLoss: 2.583949283538551\n",
      "Epoch number:  6 \tLoss: 2.4212312083264056\n",
      "Epoch number:  8 \tLoss: 2.331122604101034\n",
      "Epoch number:  10 \tLoss: 2.0083529716854045\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.002771003242871\n",
      "Epoch number:  4 \tLoss: 2.593533309410512\n",
      "Epoch number:  6 \tLoss: 2.436289887743435\n",
      "Epoch number:  8 \tLoss: 2.3655145018126613\n",
      "Epoch number:  10 \tLoss: 2.3318068392740883\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.0046165152097473\n",
      "Epoch number:  4 \tLoss: 2.595384443780998\n",
      "Epoch number:  6 \tLoss: 2.4379263013461943\n",
      "Epoch number:  8 \tLoss: 2.3668066162162114\n",
      "Epoch number:  10 \tLoss: 2.3327381029012577\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.0060420790454625\n",
      "Epoch number:  4 \tLoss: 2.596835366024924\n",
      "Epoch number:  6 \tLoss: 2.439189907752671\n",
      "Epoch number:  8 \tLoss: 2.367767860101814\n",
      "Epoch number:  10 \tLoss: 2.3334167733726443\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.003730785027929\n",
      "Epoch number:  4 \tLoss: 2.5944052613145754\n",
      "Epoch number:  6 \tLoss: 2.43687011251347\n",
      "Epoch number:  8 \tLoss: 2.3656246991129395\n",
      "Epoch number:  10 \tLoss: 2.3313145305296565\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.0059455919486227\n",
      "Epoch number:  4 \tLoss: 2.596744406046141\n",
      "Epoch number:  6 \tLoss: 2.439114667562016\n",
      "Epoch number:  8 \tLoss: 2.3677154286150275\n",
      "Epoch number:  10 \tLoss: 2.33338610740497\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.993256402261498\n",
      "Epoch number:  4 \tLoss: 2.5791225055637343\n",
      "Epoch number:  6 \tLoss: 2.411801559091272\n",
      "Epoch number:  8 \tLoss: 2.270613197955364\n",
      "Epoch number:  10 \tLoss: 1.703557498127767\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.0015336488170785\n",
      "Epoch number:  4 \tLoss: 2.5924935899984938\n",
      "Epoch number:  6 \tLoss: 2.435592090560922\n",
      "Epoch number:  8 \tLoss: 2.365182213635067\n",
      "Epoch number:  10 \tLoss: 2.3317706900984714\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.5391429364310056\n",
      "Epoch number:  4 \tLoss: 3.005037076283786\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.5398644625415185\n",
      "Epoch number:  4 \tLoss: 3.005976310442996\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.537670315014158\n",
      "Epoch number:  4 \tLoss: 3.002769088958138\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.5386964765853173\n",
      "Epoch number:  4 \tLoss: 3.0045797975021076\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.51959109360974\n",
      "Epoch number:  4 \tLoss: 2.9651096098222056\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.5340656100842107\n",
      "Epoch number:  4 \tLoss: 2.998982149576782\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.5387474835075987\n",
      "Epoch number:  4 \tLoss: 3.0044196132579417\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.538796451203908\n",
      "Epoch number:  4 \tLoss: 3.0047019283665466\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.537552312977521\n",
      "Epoch number:  4 \tLoss: 3.002757392794212\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.537991143424625\n",
      "Epoch number:  4 \tLoss: 3.0037455884582593\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.5204175796163053\n",
      "Epoch number:  4 \tLoss: 2.9617635868829852\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.5354635770710705\n",
      "Epoch number:  4 \tLoss: 3.000683449101701\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.5374325712204944\n",
      "Epoch number:  4 \tLoss: 3.0030928812072886\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.539914575724552\n",
      "Epoch number:  4 \tLoss: 3.0060420565901844\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.537390492992272\n",
      "Epoch number:  4 \tLoss: 3.0027054700932885\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.5380545355171473\n",
      "Epoch number:  4 \tLoss: 3.0038189793556773\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.51081305957603\n",
      "Epoch number:  4 \tLoss: 2.932493614558704\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.5364702982902427\n",
      "Epoch number:  4 \tLoss: 3.001954148576276\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.5395934580602786\n",
      "Epoch number:  4 \tLoss: 3.0055849543360718\n",
      "Epoch number:  6 \tLoss: 2.746674590414725\n",
      "Epoch number:  8 \tLoss: 2.5957569523546717\n",
      "Epoch number:  10 \tLoss: 2.5004640144045602\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.538861099380364\n",
      "Epoch number:  4 \tLoss: 3.0047818303876976\n",
      "Epoch number:  6 \tLoss: 2.745785723506562\n",
      "Epoch number:  8 \tLoss: 2.5948435812043504\n",
      "Epoch number:  10 \tLoss: 2.499614343227625\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.538967343293861\n",
      "Epoch number:  4 \tLoss: 3.0045570457320343\n",
      "Epoch number:  6 \tLoss: 2.74506353201245\n",
      "Epoch number:  8 \tLoss: 2.593178600928832\n",
      "Epoch number:  10 \tLoss: 2.4959250034690617\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.537515943603856\n",
      "Epoch number:  4 \tLoss: 3.0031700553743668\n",
      "Epoch number:  6 \tLoss: 2.743888709458722\n",
      "Epoch number:  8 \tLoss: 2.592744232840677\n",
      "Epoch number:  10 \tLoss: 2.4974419292782755\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.5256662536038688\n",
      "Epoch number:  4 \tLoss: 2.976372577450487\n",
      "Epoch number:  6 \tLoss: 2.6554806450580073\n",
      "Epoch number:  8 \tLoss: 2.147347135930577\n",
      "Epoch number:  10 \tLoss: 1.8930360945782154\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.5348297051376067\n",
      "Epoch number:  4 \tLoss: 2.9999475552192596\n",
      "Epoch number:  6 \tLoss: 2.740110460234794\n",
      "Epoch number:  8 \tLoss: 2.5885895028696995\n",
      "Epoch number:  10 \tLoss: 2.4931781879504262\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.5396493872632355\n",
      "Epoch number:  4 \tLoss: 3.0056649918321665\n",
      "Epoch number:  6 \tLoss: 2.7467882806252972\n",
      "Epoch number:  8 \tLoss: 2.595902899352644\n",
      "Epoch number:  10 \tLoss: 2.5006438063452108\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.5392828812870847\n",
      "Epoch number:  4 \tLoss: 3.005283921820261\n",
      "Epoch number:  6 \tLoss: 2.7463770356533734\n",
      "Epoch number:  8 \tLoss: 2.5954996835178465\n",
      "Epoch number:  10 \tLoss: 2.5002959183980757\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.5337328394009875\n",
      "Epoch number:  4 \tLoss: 2.9974244298487243\n",
      "Epoch number:  6 \tLoss: 2.735335652039097\n",
      "Epoch number:  8 \tLoss: 2.580029253506214\n",
      "Epoch number:  10 \tLoss: 2.47552313390633\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.538184400487211\n",
      "Epoch number:  4 \tLoss: 3.0039757252559887\n",
      "Epoch number:  6 \tLoss: 2.7448381109004982\n",
      "Epoch number:  8 \tLoss: 2.5937949306182726\n",
      "Epoch number:  10 \tLoss: 2.4985285702511453\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.5195176427168953\n",
      "Epoch number:  4 \tLoss: 2.961054556054212\n",
      "Epoch number:  6 \tLoss: 2.594528536136658\n",
      "Epoch number:  8 \tLoss: 1.9553831516471478\n",
      "Epoch number:  10 \tLoss: 1.7471493942703538\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.5363754488605643\n",
      "Epoch number:  4 \tLoss: 3.001804276543331\n",
      "Epoch number:  6 \tLoss: 2.742281059832505\n",
      "Epoch number:  8 \tLoss: 2.5909679546418207\n",
      "Epoch number:  10 \tLoss: 2.4956094159290343\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.540255432929817\n",
      "Epoch number:  4 \tLoss: 3.0064109120632008\n",
      "Epoch number:  6 \tLoss: 2.747703976332586\n",
      "Epoch number:  8 \tLoss: 2.596974953089252\n",
      "Epoch number:  10 \tLoss: 2.5018250997733955\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.537950485877646\n",
      "Epoch number:  4 \tLoss: 3.003721839791361\n",
      "Epoch number:  6 \tLoss: 2.744567003352043\n",
      "Epoch number:  8 \tLoss: 2.5935266592747093\n",
      "Epoch number:  10 \tLoss: 2.498286501732754\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.5386171280732213\n",
      "Epoch number:  4 \tLoss: 3.0041196122533895\n",
      "Epoch number:  6 \tLoss: 2.7446834770486634\n",
      "Epoch number:  8 \tLoss: 2.593116480921556\n",
      "Epoch number:  10 \tLoss: 2.496853054126574\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.5391037584621055\n",
      "Epoch number:  4 \tLoss: 3.005078487986859\n",
      "Epoch number:  6 \tLoss: 2.7461460775583957\n",
      "Epoch number:  8 \tLoss: 2.5952588670649233\n",
      "Epoch number:  10 \tLoss: 2.5000655373064182\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.516820565319896\n",
      "Epoch number:  4 \tLoss: 2.953531604839558\n",
      "Epoch number:  6 \tLoss: 2.506086735679051\n",
      "Epoch number:  8 \tLoss: 2.097695863092359\n",
      "Epoch number:  10 \tLoss: 1.7550470565196572\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.53722787614799\n",
      "Epoch number:  4 \tLoss: 3.002803649740965\n",
      "Epoch number:  6 \tLoss: 2.743479110156256\n",
      "Epoch number:  8 \tLoss: 2.592326945558808\n",
      "Epoch number:  10 \tLoss: 2.4970557096410384\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 4.141702187657242\n",
      "Epoch number:  4 \tLoss: 3.5387836923336202\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 4.141269576873934\n",
      "Epoch number:  4 \tLoss: 3.538292235570537\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 4.136963567265286\n",
      "Epoch number:  4 \tLoss: 3.5321740047189687\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 4.141417757017018\n",
      "Epoch number:  4 \tLoss: 3.538407733123661\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 4.117149135655471\n",
      "Epoch number:  4 \tLoss: 3.483228525467646\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 4.1374346485642235\n",
      "Epoch number:  4 \tLoss: 3.5337433537622007\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 4.142324996348198\n",
      "Epoch number:  4 \tLoss: 3.5394837511530723\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 4.140566871145671\n",
      "Epoch number:  4 \tLoss: 3.5374525142766102\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 4.1387864421967695\n",
      "Epoch number:  4 \tLoss: 3.5343223748009835\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 4.142128086551471\n",
      "Epoch number:  4 \tLoss: 3.539309365100186\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 4.103656658313847\n",
      "Epoch number:  4 \tLoss: 3.4287003415379864\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 4.137515010217912\n",
      "Epoch number:  4 \tLoss: 3.5337706371273883\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 4.142480613682734\n",
      "Epoch number:  4 \tLoss: 3.539559786057712\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 4.1418757156867105\n",
      "Epoch number:  4 \tLoss: 3.5390110311320986\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 4.138972747421541\n",
      "Epoch number:  4 \tLoss: 3.534046388772983\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 4.141221973654222\n",
      "Epoch number:  4 \tLoss: 3.5382329785836912\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 4.116317481242778\n",
      "Epoch number:  4 \tLoss: 3.4803579291132714\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 4.137255517554014\n",
      "Epoch number:  4 \tLoss: 3.5333372561616434\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 4.142972309535721\n",
      "Epoch number:  4 \tLoss: 3.5401811134980607\n",
      "Epoch number:  6 \tLoss: 3.216216509710583\n",
      "Epoch number:  8 \tLoss: 3.0060060478319026\n",
      "Epoch number:  10 \tLoss: 2.857310390172837\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 4.142523280714334\n",
      "Epoch number:  4 \tLoss: 3.5397784347703882\n",
      "Epoch number:  6 \tLoss: 3.215760706832162\n",
      "Epoch number:  8 \tLoss: 3.0054689170064086\n",
      "Epoch number:  10 \tLoss: 2.8566861219450734\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 4.1382401632835295\n",
      "Epoch number:  4 \tLoss: 3.5332959157054504\n",
      "Epoch number:  6 \tLoss: 3.205351336449655\n",
      "Epoch number:  8 \tLoss: 2.987359975969463\n",
      "Epoch number:  10 \tLoss: 2.8197733248807446\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 4.14132370133265\n",
      "Epoch number:  4 \tLoss: 3.5383519612068572\n",
      "Epoch number:  6 \tLoss: 3.2139807060711285\n",
      "Epoch number:  8 \tLoss: 3.0032361691874976\n",
      "Epoch number:  10 \tLoss: 2.8539356655811585\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 4.115577459555706\n",
      "Epoch number:  4 \tLoss: 3.474531541439028\n",
      "Epoch number:  6 \tLoss: 2.6962482239843526\n",
      "Epoch number:  8 \tLoss: 2.3344800344097862\n",
      "Epoch number:  10 \tLoss: 1.9075957852653087\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 4.138069890292113\n",
      "Epoch number:  4 \tLoss: 3.5343828896062472\n",
      "Epoch number:  6 \tLoss: 3.2090174677855314\n",
      "Epoch number:  8 \tLoss: 2.9970275808988363\n",
      "Epoch number:  10 \tLoss: 2.8463275534761903\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 4.1426376372467555\n",
      "Epoch number:  4 \tLoss: 3.5399004827449465\n",
      "Epoch number:  6 \tLoss: 3.2158700077140994\n",
      "Epoch number:  8 \tLoss: 3.005536259135083\n",
      "Epoch number:  10 \tLoss: 2.8566838617194827\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 4.141315419574154\n",
      "Epoch number:  4 \tLoss: 3.5383448327612124\n",
      "Epoch number:  6 \tLoss: 3.213972633799902\n",
      "Epoch number:  8 \tLoss: 3.0032265062210723\n",
      "Epoch number:  10 \tLoss: 2.8539241519620266\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 4.139584161196898\n",
      "Epoch number:  4 \tLoss: 3.5347574352006546\n",
      "Epoch number:  6 \tLoss: 3.207862894091838\n",
      "Epoch number:  8 \tLoss: 2.9926260421338227\n",
      "Epoch number:  10 \tLoss: 2.833982397076667\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 4.139505300209523\n",
      "Epoch number:  4 \tLoss: 3.5361902408126147\n",
      "Epoch number:  6 \tLoss: 3.211288201105204\n",
      "Epoch number:  8 \tLoss: 2.99986837578311\n",
      "Epoch number:  10 \tLoss: 2.8498038214976353\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 4.107237507992741\n",
      "Epoch number:  4 \tLoss: 3.4482894833986824\n",
      "Epoch number:  6 \tLoss: 2.5369061067857626\n",
      "Epoch number:  8 \tLoss: 1.6993291556821202\n",
      "Epoch number:  10 \tLoss: 1.368216765677521\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 4.137776053317783\n",
      "Epoch number:  4 \tLoss: 3.5341138818713227\n",
      "Epoch number:  6 \tLoss: 3.208701346180525\n",
      "Epoch number:  8 \tLoss: 2.9966396555157413\n",
      "Epoch number:  10 \tLoss: 2.845857241794322\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 4.141770044581565\n",
      "Epoch number:  4 \tLoss: 3.5385672597448257\n",
      "Epoch number:  6 \tLoss: 3.2140892129206464\n",
      "Epoch number:  8 \tLoss: 3.0032345728915626\n",
      "Epoch number:  10 \tLoss: 2.853784399484006\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 4.142244283462338\n",
      "Epoch number:  4 \tLoss: 3.5394325540253835\n",
      "Epoch number:  6 \tLoss: 3.2153264735835085\n",
      "Epoch number:  8 \tLoss: 3.0049240070259637\n",
      "Epoch number:  10 \tLoss: 2.856015998838868\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 4.136947352990494\n",
      "Epoch number:  4 \tLoss: 3.5319728671951824\n",
      "Epoch number:  6 \tLoss: 3.2036995510647888\n",
      "Epoch number:  8 \tLoss: 2.9853294405726465\n",
      "Epoch number:  10 \tLoss: 2.817972953191936\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 4.140104548420306\n",
      "Epoch number:  4 \tLoss: 3.5368914668319293\n",
      "Epoch number:  6 \tLoss: 3.2121620895055734\n",
      "Epoch number:  8 \tLoss: 3.000965475100336\n",
      "Epoch number:  10 \tLoss: 2.8511563793118735\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 4.107004566490705\n",
      "Epoch number:  4 \tLoss: 3.4437060976714844\n",
      "Epoch number:  6 \tLoss: 2.5368238666216616\n",
      "Epoch number:  8 \tLoss: 1.8386614879426633\n",
      "Epoch number:  10 \tLoss: 1.5062326169985245\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 4.139049837940558\n",
      "Epoch number:  4 \tLoss: 3.5356126895914066\n",
      "Epoch number:  6 \tLoss: 3.2105536701459725\n",
      "Epoch number:  8 \tLoss: 2.998940975393285\n",
      "Epoch number:  10 \tLoss: 2.8486637247569973\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3000828427487288\n",
      "Epoch number:  4 \tLoss: 1.8384315180020356\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300403379685825\n",
      "Epoch number:  4 \tLoss: 2.29932458016818\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.9890465806203295\n",
      "Epoch number:  4 \tLoss: 0.6582941910870994\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300349501160863\n",
      "Epoch number:  4 \tLoss: 2.299327778760616\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.8252256276688255\n",
      "Epoch number:  4 \tLoss: 0.526864324739003\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3001720579470613\n",
      "Epoch number:  4 \tLoss: 2.299308350429199\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2994315574685436\n",
      "Epoch number:  4 \tLoss: 1.44754931889411\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3003745001241964\n",
      "Epoch number:  4 \tLoss: 2.299310975089462\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.9465868443062633\n",
      "Epoch number:  4 \tLoss: 0.8227552643137731\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3003059052723023\n",
      "Epoch number:  4 \tLoss: 2.2993162780105485\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.774675174198672\n",
      "Epoch number:  4 \tLoss: 0.5123017253573804\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3002522440383926\n",
      "Epoch number:  4 \tLoss: 2.2993330915543426\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3003668849639833\n",
      "Epoch number:  4 \tLoss: 2.2983335362923647\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300462321743025\n",
      "Epoch number:  4 \tLoss: 2.2993789682239467\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2558384381349628\n",
      "Epoch number:  4 \tLoss: 0.910784605809815\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300515396156028\n",
      "Epoch number:  4 \tLoss: 2.2994484095929284\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.167004041757693\n",
      "Epoch number:  4 \tLoss: 0.5872211831117234\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300652218228056\n",
      "Epoch number:  4 \tLoss: 2.299650516154547\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2989999380877797\n",
      "Epoch number:  4 \tLoss: 1.5045137932097201\n",
      "Epoch number:  6 \tLoss: 0.7804081417982681\n",
      "Epoch number:  8 \tLoss: 0.6130935972504084\n",
      "Epoch number:  10 \tLoss: 0.4922014625821221\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3003704231503694\n",
      "Epoch number:  4 \tLoss: 2.2993108229278283\n",
      "Epoch number:  6 \tLoss: 2.299309720586628\n",
      "Epoch number:  8 \tLoss: 2.2993097036121446\n",
      "Epoch number:  10 \tLoss: 2.299309694374234\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.7690431382868494\n",
      "Epoch number:  4 \tLoss: 0.7675713680154937\n",
      "Epoch number:  6 \tLoss: 0.5257915954058795\n",
      "Epoch number:  8 \tLoss: 0.4454589005266794\n",
      "Epoch number:  10 \tLoss: 0.3923012526230023\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300320412529844\n",
      "Epoch number:  4 \tLoss: 2.299333220658568\n",
      "Epoch number:  6 \tLoss: 2.299332667107421\n",
      "Epoch number:  8 \tLoss: 2.299332633100096\n",
      "Epoch number:  10 \tLoss: 2.2993326183102463\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.7695763107697726\n",
      "Epoch number:  4 \tLoss: 0.5153511735906185\n",
      "Epoch number:  6 \tLoss: 0.4281561602167346\n",
      "Epoch number:  8 \tLoss: 0.38425948319616887\n",
      "Epoch number:  10 \tLoss: 0.34996772519163377\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3002835428404214\n",
      "Epoch number:  4 \tLoss: 2.2993117804194454\n",
      "Epoch number:  6 \tLoss: 2.2993089545278655\n",
      "Epoch number:  8 \tLoss: 2.2993070551010875\n",
      "Epoch number:  10 \tLoss: 2.2993043244665445\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.298875217565602\n",
      "Epoch number:  4 \tLoss: 1.5250919234487226\n",
      "Epoch number:  6 \tLoss: 0.7446247211942402\n",
      "Epoch number:  8 \tLoss: 0.5413009973382754\n",
      "Epoch number:  10 \tLoss: 0.4606619619871145\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3003588738913616\n",
      "Epoch number:  4 \tLoss: 2.2993076241581933\n",
      "Epoch number:  6 \tLoss: 2.2993065295519703\n",
      "Epoch number:  8 \tLoss: 2.299306522503805\n",
      "Epoch number:  10 \tLoss: 2.2993065168537243\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.8001371450373986\n",
      "Epoch number:  4 \tLoss: 0.6975333216072074\n",
      "Epoch number:  6 \tLoss: 0.5241116479850164\n",
      "Epoch number:  8 \tLoss: 0.4460325182957676\n",
      "Epoch number:  10 \tLoss: 0.39636284635354513\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3002864543161254\n",
      "Epoch number:  4 \tLoss: 2.2993154813350563\n",
      "Epoch number:  6 \tLoss: 2.299319856674894\n",
      "Epoch number:  8 \tLoss: 2.2993161802875624\n",
      "Epoch number:  10 \tLoss: 2.2993140487612598\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.7807597727883623\n",
      "Epoch number:  4 \tLoss: 0.5180098645528008\n",
      "Epoch number:  6 \tLoss: 0.431975498023018\n",
      "Epoch number:  8 \tLoss: 0.3858209577677746\n",
      "Epoch number:  10 \tLoss: 0.3568650563254662\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300214816169342\n",
      "Epoch number:  4 \tLoss: 2.2993312645997075\n",
      "Epoch number:  6 \tLoss: 2.2993272388453754\n",
      "Epoch number:  8 \tLoss: 2.2993285114253585\n",
      "Epoch number:  10 \tLoss: 2.2993259868197513\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3003650824051927\n",
      "Epoch number:  4 \tLoss: 2.2992117416594846\n",
      "Epoch number:  6 \tLoss: 2.299006332928653\n",
      "Epoch number:  8 \tLoss: 2.2661230631893554\n",
      "Epoch number:  10 \tLoss: 1.4676874851656074\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3004803912990277\n",
      "Epoch number:  4 \tLoss: 2.2994049770539733\n",
      "Epoch number:  6 \tLoss: 2.2993871298958073\n",
      "Epoch number:  8 \tLoss: 2.299373899696285\n",
      "Epoch number:  10 \tLoss: 2.299363412911007\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.258913999453171\n",
      "Epoch number:  4 \tLoss: 0.8444694498139406\n",
      "Epoch number:  6 \tLoss: 0.5684056822823773\n",
      "Epoch number:  8 \tLoss: 0.5032952447426167\n",
      "Epoch number:  10 \tLoss: 0.4447874749854585\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3005873956270606\n",
      "Epoch number:  4 \tLoss: 2.2995691943761103\n",
      "Epoch number:  6 \tLoss: 2.2995204076502866\n",
      "Epoch number:  8 \tLoss: 2.2994819018976944\n",
      "Epoch number:  10 \tLoss: 2.2994509520182316\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.9635484446218626\n",
      "Epoch number:  4 \tLoss: 0.5395948035713702\n",
      "Epoch number:  6 \tLoss: 0.45662705982880497\n",
      "Epoch number:  8 \tLoss: 0.4144104790710097\n",
      "Epoch number:  10 \tLoss: 0.38461753147567723\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.30065411806321\n",
      "Epoch number:  4 \tLoss: 2.2996296651992623\n",
      "Epoch number:  6 \tLoss: 2.2995659918487186\n",
      "Epoch number:  8 \tLoss: 2.2995159955940783\n",
      "Epoch number:  10 \tLoss: 2.299476694120174\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.330715898317033\n",
      "Epoch number:  4 \tLoss: 1.6467727451532699\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3316756124988554\n",
      "Epoch number:  4 \tLoss: 2.3002300931923885\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.8626797869071565\n",
      "Epoch number:  4 \tLoss: 0.8014450705374284\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.330244058405987\n",
      "Epoch number:  4 \tLoss: 2.3001254968741383\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.922400477610792\n",
      "Epoch number:  4 \tLoss: 0.5229058710093205\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.328976619954196\n",
      "Epoch number:  4 \tLoss: 2.299991176383921\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3319181160930817\n",
      "Epoch number:  4 \tLoss: 1.6704986065121143\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.332682581520283\n",
      "Epoch number:  4 \tLoss: 2.3003123234960525\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.684085523061581\n",
      "Epoch number:  4 \tLoss: 0.6427021029634384\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3314912569017086\n",
      "Epoch number:  4 \tLoss: 2.3002117331430405\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.7797544123153044\n",
      "Epoch number:  4 \tLoss: 0.5176011124362193\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3291232162450863\n",
      "Epoch number:  4 \tLoss: 2.300032523651208\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.331468197474981\n",
      "Epoch number:  4 \tLoss: 1.954844173551918\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3313941052372433\n",
      "Epoch number:  4 \tLoss: 2.3003131927022453\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.8781535568889212\n",
      "Epoch number:  4 \tLoss: 0.9084584627245662\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.329155541954457\n",
      "Epoch number:  4 \tLoss: 2.300335533006909\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.9376913025032457\n",
      "Epoch number:  4 \tLoss: 0.5203222812526617\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.327830837028034\n",
      "Epoch number:  4 \tLoss: 2.300345795638347\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.331901537721089\n",
      "Epoch number:  4 \tLoss: 1.672407817561477\n",
      "Epoch number:  6 \tLoss: 1.121400594346337\n",
      "Epoch number:  8 \tLoss: 0.714638637124053\n",
      "Epoch number:  10 \tLoss: 0.49227049104907034\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3316492095097954\n",
      "Epoch number:  4 \tLoss: 2.300225338574188\n",
      "Epoch number:  6 \tLoss: 2.299284791652093\n",
      "Epoch number:  8 \tLoss: 2.2992562774129404\n",
      "Epoch number:  10 \tLoss: 2.299255424845058\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.9203623838880632\n",
      "Epoch number:  4 \tLoss: 1.078707076261006\n",
      "Epoch number:  6 \tLoss: 0.5912836803747185\n",
      "Epoch number:  8 \tLoss: 0.4693146309662703\n",
      "Epoch number:  10 \tLoss: 0.3999049897444561\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3314513510514803\n",
      "Epoch number:  4 \tLoss: 2.3002011414348305\n",
      "Epoch number:  6 \tLoss: 2.2992768857380885\n",
      "Epoch number:  8 \tLoss: 2.2992483667534005\n",
      "Epoch number:  10 \tLoss: 2.299247686209874\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.9980812857861886\n",
      "Epoch number:  4 \tLoss: 0.5211305185229985\n",
      "Epoch number:  6 \tLoss: 0.4296402169932867\n",
      "Epoch number:  8 \tLoss: 0.378568846133816\n",
      "Epoch number:  10 \tLoss: 0.33910639263439907\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3298834801903556\n",
      "Epoch number:  4 \tLoss: 2.300086948635111\n",
      "Epoch number:  6 \tLoss: 2.2992810484181234\n",
      "Epoch number:  8 \tLoss: 2.299256741402803\n",
      "Epoch number:  10 \tLoss: 2.2992540724825874\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.332002214920582\n",
      "Epoch number:  4 \tLoss: 1.8404877089408187\n",
      "Epoch number:  6 \tLoss: 1.1832798244826752\n",
      "Epoch number:  8 \tLoss: 0.7245527196382013\n",
      "Epoch number:  10 \tLoss: 0.5531287623381813\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.331880642870343\n",
      "Epoch number:  4 \tLoss: 2.3002439337880403\n",
      "Epoch number:  6 \tLoss: 2.2992839546841504\n",
      "Epoch number:  8 \tLoss: 2.2992544525369767\n",
      "Epoch number:  10 \tLoss: 2.299253548144761\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.6879607070558835\n",
      "Epoch number:  4 \tLoss: 0.7662370573583409\n",
      "Epoch number:  6 \tLoss: 0.5095741693985189\n",
      "Epoch number:  8 \tLoss: 0.4398327505137132\n",
      "Epoch number:  10 \tLoss: 0.3987405869159203\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3302598031283885\n",
      "Epoch number:  4 \tLoss: 2.3001237592632098\n",
      "Epoch number:  6 \tLoss: 2.2992919420614326\n",
      "Epoch number:  8 \tLoss: 2.299268592801499\n",
      "Epoch number:  10 \tLoss: 2.2992678912932965\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.9693251622378426\n",
      "Epoch number:  4 \tLoss: 0.5327984539399844\n",
      "Epoch number:  6 \tLoss: 0.43133898089050254\n",
      "Epoch number:  8 \tLoss: 0.3761326623101436\n",
      "Epoch number:  10 \tLoss: 0.34403956434784344\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3283093302403843\n",
      "Epoch number:  4 \tLoss: 2.2999618012999075\n",
      "Epoch number:  6 \tLoss: 2.299265095169611\n",
      "Epoch number:  8 \tLoss: 2.2992450821483814\n",
      "Epoch number:  10 \tLoss: 2.2992425312301044\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.331230604146646\n",
      "Epoch number:  4 \tLoss: 1.6640019480133514\n",
      "Epoch number:  6 \tLoss: 1.1417642021814745\n",
      "Epoch number:  8 \tLoss: 0.7012316021374911\n",
      "Epoch number:  10 \tLoss: 0.5551090043083344\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.331916046133278\n",
      "Epoch number:  4 \tLoss: 2.3003481237746053\n",
      "Epoch number:  6 \tLoss: 2.2993853201889065\n",
      "Epoch number:  8 \tLoss: 2.2993499996330304\n",
      "Epoch number:  10 \tLoss: 2.2993436283711914\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.990616764619774\n",
      "Epoch number:  4 \tLoss: 0.7704913931254718\n",
      "Epoch number:  6 \tLoss: 0.549416565088197\n",
      "Epoch number:  8 \tLoss: 0.44821618662999607\n",
      "Epoch number:  10 \tLoss: 0.398928975826937\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.331573833430319\n",
      "Epoch number:  4 \tLoss: 2.300344751371379\n",
      "Epoch number:  6 \tLoss: 2.299410076067602\n",
      "Epoch number:  8 \tLoss: 2.2993748392370312\n",
      "Epoch number:  10 \tLoss: 2.2993673095548948\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.0918749909684102\n",
      "Epoch number:  4 \tLoss: 0.5238897763620426\n",
      "Epoch number:  6 \tLoss: 0.42135759019774466\n",
      "Epoch number:  8 \tLoss: 0.377954792163465\n",
      "Epoch number:  10 \tLoss: 0.353551035363104\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.329271390038863\n",
      "Epoch number:  4 \tLoss: 2.3003844632552357\n",
      "Epoch number:  6 \tLoss: 2.2996179695391\n",
      "Epoch number:  8 \tLoss: 2.29957901150926\n",
      "Epoch number:  10 \tLoss: 2.2995608259901683\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.4596655696466345\n",
      "Epoch number:  4 \tLoss: 1.7111899703576863\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.497705410266483\n",
      "Epoch number:  4 \tLoss: 2.330196736865233\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.1426760969714893\n",
      "Epoch number:  4 \tLoss: 0.6720999075889906\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.491826085017923\n",
      "Epoch number:  4 \tLoss: 2.326074799219307\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.149863961129784\n",
      "Epoch number:  4 \tLoss: 0.544902405799541\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.4719663367589293\n",
      "Epoch number:  4 \tLoss: 2.316080104959462\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.4904827062024815\n",
      "Epoch number:  4 \tLoss: 1.5898668562456677\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.498603362744846\n",
      "Epoch number:  4 \tLoss: 2.330890181662427\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.7695859912357244\n",
      "Epoch number:  4 \tLoss: 0.7646266817445952\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.4867802485918613\n",
      "Epoch number:  4 \tLoss: 2.323020581449425\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.8730687718193172\n",
      "Epoch number:  4 \tLoss: 0.5234956444027845\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.4850627840072104\n",
      "Epoch number:  4 \tLoss: 2.3220728734603933\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3915621469353225\n",
      "Epoch number:  4 \tLoss: 1.8270628251593275\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.4928527740258875\n",
      "Epoch number:  4 \tLoss: 2.3269096719683944\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.9411895998532458\n",
      "Epoch number:  4 \tLoss: 0.9543214717088606\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.4908352863363956\n",
      "Epoch number:  4 \tLoss: 2.325630633916657\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.8424831290099298\n",
      "Epoch number:  4 \tLoss: 0.5247547455935686\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.4796207651574935\n",
      "Epoch number:  4 \tLoss: 2.3196978180966905\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.49796152588785\n",
      "Epoch number:  4 \tLoss: 1.8887924545276809\n",
      "Epoch number:  6 \tLoss: 1.0856606835177762\n",
      "Epoch number:  8 \tLoss: 0.6514922885811548\n",
      "Epoch number:  10 \tLoss: 0.4952422146031792\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.4964560320238833\n",
      "Epoch number:  4 \tLoss: 2.3292665514925597\n",
      "Epoch number:  6 \tLoss: 2.3039718838455987\n",
      "Epoch number:  8 \tLoss: 2.2999908686433597\n",
      "Epoch number:  10 \tLoss: 2.299361692461362\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.04619424704491\n",
      "Epoch number:  4 \tLoss: 1.168805476294631\n",
      "Epoch number:  6 \tLoss: 0.6364812809128442\n",
      "Epoch number:  8 \tLoss: 0.4817699357507133\n",
      "Epoch number:  10 \tLoss: 0.410522642746015\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.4890063188269544\n",
      "Epoch number:  4 \tLoss: 2.3243207863515343\n",
      "Epoch number:  6 \tLoss: 2.302538660267795\n",
      "Epoch number:  8 \tLoss: 2.299676201788771\n",
      "Epoch number:  10 \tLoss: 2.2993011358383026\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.8469567272389759\n",
      "Epoch number:  4 \tLoss: 0.5156273619490648\n",
      "Epoch number:  6 \tLoss: 0.42910261117673504\n",
      "Epoch number:  8 \tLoss: 0.3835242272390391\n",
      "Epoch number:  10 \tLoss: 0.34950469122712846\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.478932044565738\n",
      "Epoch number:  4 \tLoss: 2.3190078649357155\n",
      "Epoch number:  6 \tLoss: 2.3013286932329104\n",
      "Epoch number:  8 \tLoss: 2.2994610790837253\n",
      "Epoch number:  10 \tLoss: 2.299263987237658\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.4907246796716023\n",
      "Epoch number:  4 \tLoss: 1.778283257735993\n",
      "Epoch number:  6 \tLoss: 0.8991040714126134\n",
      "Epoch number:  8 \tLoss: 0.5897369773171863\n",
      "Epoch number:  10 \tLoss: 0.5020063090097855\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.493333872540304\n",
      "Epoch number:  4 \tLoss: 2.327069622787654\n",
      "Epoch number:  6 \tLoss: 2.3032923784731953\n",
      "Epoch number:  8 \tLoss: 2.2998303079722393\n",
      "Epoch number:  10 \tLoss: 2.2993260216617064\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.8461514677016158\n",
      "Epoch number:  4 \tLoss: 0.8638861137504854\n",
      "Epoch number:  6 \tLoss: 0.5447463735509795\n",
      "Epoch number:  8 \tLoss: 0.4690424716633268\n",
      "Epoch number:  10 \tLoss: 0.42835146088130965\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.492950360658146\n",
      "Epoch number:  4 \tLoss: 2.3268061665165294\n",
      "Epoch number:  6 \tLoss: 2.3032098859643626\n",
      "Epoch number:  8 \tLoss: 2.2998063833693476\n",
      "Epoch number:  10 \tLoss: 2.29931587377709\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.8155233008805618\n",
      "Epoch number:  4 \tLoss: 0.5131627893571536\n",
      "Epoch number:  6 \tLoss: 0.43388501163558374\n",
      "Epoch number:  8 \tLoss: 0.3819293349538741\n",
      "Epoch number:  10 \tLoss: 0.3469675602476115\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.484126099108371\n",
      "Epoch number:  4 \tLoss: 2.321566320046872\n",
      "Epoch number:  6 \tLoss: 2.3018637030070512\n",
      "Epoch number:  8 \tLoss: 2.299541520551482\n",
      "Epoch number:  10 \tLoss: 2.2992680676262136\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.4926456255273592\n",
      "Epoch number:  4 \tLoss: 1.8530185012089095\n",
      "Epoch number:  6 \tLoss: 1.151336840323922\n",
      "Epoch number:  8 \tLoss: 0.7950182530487152\n",
      "Epoch number:  10 \tLoss: 0.561303392229153\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.4947837901679506\n",
      "Epoch number:  4 \tLoss: 2.3281907159990536\n",
      "Epoch number:  6 \tLoss: 2.303759334771298\n",
      "Epoch number:  8 \tLoss: 2.300067992295418\n",
      "Epoch number:  10 \tLoss: 2.299506339078415\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.028099643819101\n",
      "Epoch number:  4 \tLoss: 1.0163952577235544\n",
      "Epoch number:  6 \tLoss: 0.6614087475484419\n",
      "Epoch number:  8 \tLoss: 0.49307383832404944\n",
      "Epoch number:  10 \tLoss: 0.42009273401363084\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.490809294373739\n",
      "Epoch number:  4 \tLoss: 2.3256148484183834\n",
      "Epoch number:  6 \tLoss: 2.3030809286578555\n",
      "Epoch number:  8 \tLoss: 2.2999868407756643\n",
      "Epoch number:  10 \tLoss: 2.299559253180307\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.8856126089256557\n",
      "Epoch number:  4 \tLoss: 0.5239966029205086\n",
      "Epoch number:  6 \tLoss: 0.43010491750639035\n",
      "Epoch number:  8 \tLoss: 0.3781165965389128\n",
      "Epoch number:  10 \tLoss: 0.3446946145510333\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.4789929887544675\n",
      "Epoch number:  4 \tLoss: 2.319401414289225\n",
      "Epoch number:  6 \tLoss: 2.301776166051636\n",
      "Epoch number:  8 \tLoss: 2.2999044027687923\n",
      "Epoch number:  10 \tLoss: 2.2997021988223416\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3000407321888616\n",
      "Epoch number:  4 \tLoss: 1.8926317624400133\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3004210629706874\n",
      "Epoch number:  4 \tLoss: 2.299344379292497\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.6925305818996794\n",
      "Epoch number:  4 \tLoss: 0.8151884842783638\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300367333121054\n",
      "Epoch number:  4 \tLoss: 2.299323217478915\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.9279692931284902\n",
      "Epoch number:  4 \tLoss: 0.5250950270830754\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3001859929987907\n",
      "Epoch number:  4 \tLoss: 2.2993105512466423\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.298991807884583\n",
      "Epoch number:  4 \tLoss: 1.5799267607598293\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3003350479535367\n",
      "Epoch number:  4 \tLoss: 2.299314793430318\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.816257204640961\n",
      "Epoch number:  4 \tLoss: 0.6993361349905981\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300310566654293\n",
      "Epoch number:  4 \tLoss: 2.29929099563384\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.0712756579327123\n",
      "Epoch number:  4 \tLoss: 0.5444375720586365\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3002471316577617\n",
      "Epoch number:  4 \tLoss: 2.2993155936199368\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300412324511302\n",
      "Epoch number:  4 \tLoss: 2.299217765724564\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3004751673933885\n",
      "Epoch number:  4 \tLoss: 2.2993849265275976\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.8956821151739918\n",
      "Epoch number:  4 \tLoss: 1.1200975361075647\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3005014560602084\n",
      "Epoch number:  4 \tLoss: 2.2994313605692738\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.0114343845586302\n",
      "Epoch number:  4 \tLoss: 0.5304629020374467\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3006509504724604\n",
      "Epoch number:  4 \tLoss: 2.2996134096413616\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299289045541034\n",
      "Epoch number:  4 \tLoss: 1.3768101735724254\n",
      "Epoch number:  6 \tLoss: 0.7941400988331079\n",
      "Epoch number:  8 \tLoss: 0.5488988503889156\n",
      "Epoch number:  10 \tLoss: 0.455320845599715\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300398983642297\n",
      "Epoch number:  4 \tLoss: 2.2993235068433635\n",
      "Epoch number:  6 \tLoss: 2.2993223489710304\n",
      "Epoch number:  8 \tLoss: 2.2993223383937225\n",
      "Epoch number:  10 \tLoss: 2.299322329455017\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.584718766833996\n",
      "Epoch number:  4 \tLoss: 0.6783382052321568\n",
      "Epoch number:  6 \tLoss: 0.49987256620791076\n",
      "Epoch number:  8 \tLoss: 0.42349688420270376\n",
      "Epoch number:  10 \tLoss: 0.380063738755315\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3003946835852114\n",
      "Epoch number:  4 \tLoss: 2.299347025694108\n",
      "Epoch number:  6 \tLoss: 2.2993453409574154\n",
      "Epoch number:  8 \tLoss: 2.2993458129253552\n",
      "Epoch number:  10 \tLoss: 2.299346701017032\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.8745761495780953\n",
      "Epoch number:  4 \tLoss: 0.5321889015158614\n",
      "Epoch number:  6 \tLoss: 0.4305164058196746\n",
      "Epoch number:  8 \tLoss: 0.37642876422305543\n",
      "Epoch number:  10 \tLoss: 0.34180203887376803\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3002591565498673\n",
      "Epoch number:  4 \tLoss: 2.29934208563733\n",
      "Epoch number:  6 \tLoss: 2.2993384473202276\n",
      "Epoch number:  8 \tLoss: 2.299336810625325\n",
      "Epoch number:  10 \tLoss: 2.2993337156523888\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2998006119988954\n",
      "Epoch number:  4 \tLoss: 1.7317790915708606\n",
      "Epoch number:  6 \tLoss: 0.6789365089759856\n",
      "Epoch number:  8 \tLoss: 0.5438378554702057\n",
      "Epoch number:  10 \tLoss: 0.47347410705020804\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3003702758589055\n",
      "Epoch number:  4 \tLoss: 2.2993118168549005\n",
      "Epoch number:  6 \tLoss: 2.299310705316929\n",
      "Epoch number:  8 \tLoss: 2.299310693744388\n",
      "Epoch number:  10 \tLoss: 2.2993106832267562\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.7576278408668873\n",
      "Epoch number:  4 \tLoss: 0.70000442385624\n",
      "Epoch number:  6 \tLoss: 0.4975923317668244\n",
      "Epoch number:  8 \tLoss: 0.4210776737600911\n",
      "Epoch number:  10 \tLoss: 0.37676393740087\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3003413059517133\n",
      "Epoch number:  4 \tLoss: 2.299339093290106\n",
      "Epoch number:  6 \tLoss: 2.2993398486593843\n",
      "Epoch number:  8 \tLoss: 2.2993380561548347\n",
      "Epoch number:  10 \tLoss: 2.2993380563333408\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.832876271268098\n",
      "Epoch number:  4 \tLoss: 0.5245623535067743\n",
      "Epoch number:  6 \tLoss: 0.43264723576303377\n",
      "Epoch number:  8 \tLoss: 0.38390110914479897\n",
      "Epoch number:  10 \tLoss: 0.35123758784173703\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300265100611629\n",
      "Epoch number:  4 \tLoss: 2.2993047738350065\n",
      "Epoch number:  6 \tLoss: 2.2992999377344896\n",
      "Epoch number:  8 \tLoss: 2.2992989513204947\n",
      "Epoch number:  10 \tLoss: 2.2992956752432803\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3003498974345384\n",
      "Epoch number:  4 \tLoss: 2.2988305786210503\n",
      "Epoch number:  6 \tLoss: 1.7550777368379975\n",
      "Epoch number:  8 \tLoss: 0.8351373512090776\n",
      "Epoch number:  10 \tLoss: 0.5910952411191239\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300464723031279\n",
      "Epoch number:  4 \tLoss: 2.299368680008491\n",
      "Epoch number:  6 \tLoss: 2.299357690069006\n",
      "Epoch number:  8 \tLoss: 2.2993500520342667\n",
      "Epoch number:  10 \tLoss: 2.299344080930349\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2866819321317147\n",
      "Epoch number:  4 \tLoss: 1.0987605442295207\n",
      "Epoch number:  6 \tLoss: 0.689665331376557\n",
      "Epoch number:  8 \tLoss: 0.5495377752824823\n",
      "Epoch number:  10 \tLoss: 0.4610561237293736\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300495511557772\n",
      "Epoch number:  4 \tLoss: 2.2994159817100503\n",
      "Epoch number:  6 \tLoss: 2.299395684273833\n",
      "Epoch number:  8 \tLoss: 2.299380636154521\n",
      "Epoch number:  10 \tLoss: 2.2993687090120556\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.0496610534591677\n",
      "Epoch number:  4 \tLoss: 0.5431590732603121\n",
      "Epoch number:  6 \tLoss: 0.4544553598228139\n",
      "Epoch number:  8 \tLoss: 0.41417105309907964\n",
      "Epoch number:  10 \tLoss: 0.38881025665389723\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3006230186460823\n",
      "Epoch number:  4 \tLoss: 2.2995804518821106\n",
      "Epoch number:  6 \tLoss: 2.2995273184978244\n",
      "Epoch number:  8 \tLoss: 2.2994854590492344\n",
      "Epoch number:  10 \tLoss: 2.2994524582106712\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.331697471673324\n",
      "Epoch number:  4 \tLoss: 1.682024954361717\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.331654132932629\n",
      "Epoch number:  4 \tLoss: 2.300223203693046\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.915282791132266\n",
      "Epoch number:  4 \tLoss: 0.8321074401105629\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.331003356256491\n",
      "Epoch number:  4 \tLoss: 2.300170256279869\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.803076441501721\n",
      "Epoch number:  4 \tLoss: 0.5109736120937433\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3280168775397665\n",
      "Epoch number:  4 \tLoss: 2.2999871611930782\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3305777086257833\n",
      "Epoch number:  4 \tLoss: 1.5758395895851252\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.331837070263933\n",
      "Epoch number:  4 \tLoss: 2.30025168877013\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.6870726844314283\n",
      "Epoch number:  4 \tLoss: 0.7280153751504397\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3310189898716183\n",
      "Epoch number:  4 \tLoss: 2.3001871868098513\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.8242999942158667\n",
      "Epoch number:  4 \tLoss: 0.5060171238438024\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.329556000557334\n",
      "Epoch number:  4 \tLoss: 2.30005144790795\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3308210433460506\n",
      "Epoch number:  4 \tLoss: 1.853550789741147\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.332219005960556\n",
      "Epoch number:  4 \tLoss: 2.300351057870414\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.9169023620045706\n",
      "Epoch number:  4 \tLoss: 0.6825343327141997\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3300846246928595\n",
      "Epoch number:  4 \tLoss: 2.3003447206917063\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.0135303822065864\n",
      "Epoch number:  4 \tLoss: 0.5375573875209447\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3272982586095945\n",
      "Epoch number:  4 \tLoss: 2.3003633247224813\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3320472315627137\n",
      "Epoch number:  4 \tLoss: 1.7698773453407683\n",
      "Epoch number:  6 \tLoss: 0.9831018469756974\n",
      "Epoch number:  8 \tLoss: 0.5563991748537757\n",
      "Epoch number:  10 \tLoss: 0.46312410836293105\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3316937573578924\n",
      "Epoch number:  4 \tLoss: 2.3002479798284545\n",
      "Epoch number:  6 \tLoss: 2.299308174047165\n",
      "Epoch number:  8 \tLoss: 2.29927971392571\n",
      "Epoch number:  10 \tLoss: 2.2992788987813166\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.837603788936731\n",
      "Epoch number:  4 \tLoss: 0.6326435702902008\n",
      "Epoch number:  6 \tLoss: 0.5067272810661689\n",
      "Epoch number:  8 \tLoss: 0.42278178441997133\n",
      "Epoch number:  10 \tLoss: 0.3825927068321995\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3301935561947653\n",
      "Epoch number:  4 \tLoss: 2.3001028977031317\n",
      "Epoch number:  6 \tLoss: 2.2992780638660286\n",
      "Epoch number:  8 \tLoss: 2.2992546226360226\n",
      "Epoch number:  10 \tLoss: 2.2992543049795113\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.8130566151921239\n",
      "Epoch number:  4 \tLoss: 0.5143478805553161\n",
      "Epoch number:  6 \tLoss: 0.41454054066248136\n",
      "Epoch number:  8 \tLoss: 0.3647318816263945\n",
      "Epoch number:  10 \tLoss: 0.3328936999202285\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.327727427458467\n",
      "Epoch number:  4 \tLoss: 2.2999166531863877\n",
      "Epoch number:  6 \tLoss: 2.29925679306258\n",
      "Epoch number:  8 \tLoss: 2.2992388447729817\n",
      "Epoch number:  10 \tLoss: 2.2992366203124\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.330338101867646\n",
      "Epoch number:  4 \tLoss: 1.7115787715871231\n",
      "Epoch number:  6 \tLoss: 0.8778875866838228\n",
      "Epoch number:  8 \tLoss: 0.573186659839377\n",
      "Epoch number:  10 \tLoss: 0.4863561698059748\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3316120698550917\n",
      "Epoch number:  4 \tLoss: 2.3002046979612487\n",
      "Epoch number:  6 \tLoss: 2.299267222274186\n",
      "Epoch number:  8 \tLoss: 2.2992401951799035\n",
      "Epoch number:  10 \tLoss: 2.2992394068456794\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.0240084254713744\n",
      "Epoch number:  4 \tLoss: 0.716384742792554\n",
      "Epoch number:  6 \tLoss: 0.5091945633005266\n",
      "Epoch number:  8 \tLoss: 0.427631348424338\n",
      "Epoch number:  10 \tLoss: 0.38515261092358216\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3305062972901514\n",
      "Epoch number:  4 \tLoss: 2.300147487108602\n",
      "Epoch number:  6 \tLoss: 2.2992999839362915\n",
      "Epoch number:  8 \tLoss: 2.2992758958048825\n",
      "Epoch number:  10 \tLoss: 2.299275121223617\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.8770048640427991\n",
      "Epoch number:  4 \tLoss: 0.511303068219467\n",
      "Epoch number:  6 \tLoss: 0.4193923964677173\n",
      "Epoch number:  8 \tLoss: 0.36804415314285566\n",
      "Epoch number:  10 \tLoss: 0.3355578601698865\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.328920356336439\n",
      "Epoch number:  4 \tLoss: 2.3000038405569576\n",
      "Epoch number:  6 \tLoss: 2.2992639265596706\n",
      "Epoch number:  8 \tLoss: 2.2992434814990217\n",
      "Epoch number:  10 \tLoss: 2.2992407196631293\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.331600937932187\n",
      "Epoch number:  4 \tLoss: 1.8392165405074137\n",
      "Epoch number:  6 \tLoss: 1.5734681997991635\n",
      "Epoch number:  8 \tLoss: 0.8619110765785601\n",
      "Epoch number:  10 \tLoss: 0.5687617365118877\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3316323978332423\n",
      "Epoch number:  4 \tLoss: 2.3003337169495692\n",
      "Epoch number:  6 \tLoss: 2.299397029011978\n",
      "Epoch number:  8 \tLoss: 2.2993631109354338\n",
      "Epoch number:  10 \tLoss: 2.299356790220506\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.8022301841632191\n",
      "Epoch number:  4 \tLoss: 1.109310485625586\n",
      "Epoch number:  6 \tLoss: 0.6589676172449458\n",
      "Epoch number:  8 \tLoss: 0.4972769407095656\n",
      "Epoch number:  10 \tLoss: 0.41994255577978484\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3299832093899706\n",
      "Epoch number:  4 \tLoss: 2.300325029760818\n",
      "Epoch number:  6 \tLoss: 2.299514052787583\n",
      "Epoch number:  8 \tLoss: 2.299480978799298\n",
      "Epoch number:  10 \tLoss: 2.29946947548511\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.8950145598077528\n",
      "Epoch number:  4 \tLoss: 0.519660898593081\n",
      "Epoch number:  6 \tLoss: 0.4284206538570957\n",
      "Epoch number:  8 \tLoss: 0.3828956674337974\n",
      "Epoch number:  10 \tLoss: 0.35204442735298863\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3282426757059764\n",
      "Epoch number:  4 \tLoss: 2.3003783224739207\n",
      "Epoch number:  6 \tLoss: 2.299684478662333\n",
      "Epoch number:  8 \tLoss: 2.2996461131500605\n",
      "Epoch number:  10 \tLoss: 2.299625292165741\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.4764422775368566\n",
      "Epoch number:  4 \tLoss: 1.4914271977078917\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.4983088670458695\n",
      "Epoch number:  4 \tLoss: 2.330494496433962\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.030994598453677\n",
      "Epoch number:  4 \tLoss: 0.9696646755351482\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.4893405788693705\n",
      "Epoch number:  4 \tLoss: 2.324396631891946\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.792963610412904\n",
      "Epoch number:  4 \tLoss: 0.5149539308120054\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.470074189184373\n",
      "Epoch number:  4 \tLoss: 2.315308004166446\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.476077070749782\n",
      "Epoch number:  4 \tLoss: 1.9370529387134316\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.4938000365513835\n",
      "Epoch number:  4 \tLoss: 2.327251874039447\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.7475874451974351\n",
      "Epoch number:  4 \tLoss: 0.6704787094455223\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.486993263497719\n",
      "Epoch number:  4 \tLoss: 2.323050916741537\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.7465296657848731\n",
      "Epoch number:  4 \tLoss: 0.5016500978340831\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.4842161501413482\n",
      "Epoch number:  4 \tLoss: 2.321530909822237\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.4893910831353376\n",
      "Epoch number:  4 \tLoss: 1.4395561215453312\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.4966976818136906\n",
      "Epoch number:  4 \tLoss: 2.329381785037948\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.9791234394625474\n",
      "Epoch number:  4 \tLoss: 0.8358723176700111\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.49095739718859\n",
      "Epoch number:  4 \tLoss: 2.3255827828915683\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.0153602945807632\n",
      "Epoch number:  4 \tLoss: 0.515109754155099\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.4787761146884426\n",
      "Epoch number:  4 \tLoss: 2.3192316250253864\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.441800978541343\n",
      "Epoch number:  4 \tLoss: 1.5580233349758208\n",
      "Epoch number:  6 \tLoss: 0.8712254207009666\n",
      "Epoch number:  8 \tLoss: 0.5886741295079214\n",
      "Epoch number:  10 \tLoss: 0.49646558650130035\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.4981102725485465\n",
      "Epoch number:  4 \tLoss: 2.3303430339114986\n",
      "Epoch number:  6 \tLoss: 2.3043161534525685\n",
      "Epoch number:  8 \tLoss: 2.3000731905016853\n",
      "Epoch number:  10 \tLoss: 2.299377034128819\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.9011242386390919\n",
      "Epoch number:  4 \tLoss: 0.9376208833258073\n",
      "Epoch number:  6 \tLoss: 0.5308240226093082\n",
      "Epoch number:  8 \tLoss: 0.443269313004527\n",
      "Epoch number:  10 \tLoss: 0.3975519726331735\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.4917856709461934\n",
      "Epoch number:  4 \tLoss: 2.3259133917054355\n",
      "Epoch number:  6 \tLoss: 2.3029532663810106\n",
      "Epoch number:  8 \tLoss: 2.2997540461600257\n",
      "Epoch number:  10 \tLoss: 2.2993085878082398\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.7604504437239952\n",
      "Epoch number:  4 \tLoss: 0.5185256706105199\n",
      "Epoch number:  6 \tLoss: 0.4314820737972654\n",
      "Epoch number:  8 \tLoss: 0.37761473315838906\n",
      "Epoch number:  10 \tLoss: 0.33895688809010527\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.4798768460229907\n",
      "Epoch number:  4 \tLoss: 2.319361173766355\n",
      "Epoch number:  6 \tLoss: 2.3014072453213728\n",
      "Epoch number:  8 \tLoss: 2.2994864190450057\n",
      "Epoch number:  10 \tLoss: 2.2992801471029383\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.4953046033141324\n",
      "Epoch number:  4 \tLoss: 1.6770505610937918\n",
      "Epoch number:  6 \tLoss: 1.1139773195829068\n",
      "Epoch number:  8 \tLoss: 0.5866223887053129\n",
      "Epoch number:  10 \tLoss: 0.4938746742973087\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.496437423671167\n",
      "Epoch number:  4 \tLoss: 2.3291002777504537\n",
      "Epoch number:  6 \tLoss: 2.3039120347237003\n",
      "Epoch number:  8 \tLoss: 2.2999783042045694\n",
      "Epoch number:  10 \tLoss: 2.2993616601351725\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.8806820597624534\n",
      "Epoch number:  4 \tLoss: 0.6709798826315069\n",
      "Epoch number:  6 \tLoss: 0.4944924824077252\n",
      "Epoch number:  8 \tLoss: 0.4262336073687024\n",
      "Epoch number:  10 \tLoss: 0.3812334638413077\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.4878959198386013\n",
      "Epoch number:  4 \tLoss: 2.3235489406903\n",
      "Epoch number:  6 \tLoss: 2.3023348412721507\n",
      "Epoch number:  8 \tLoss: 2.299634750193507\n",
      "Epoch number:  10 \tLoss: 2.299292312066499\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.8522157906562683\n",
      "Epoch number:  4 \tLoss: 0.5173766283332092\n",
      "Epoch number:  6 \tLoss: 0.432819429766538\n",
      "Epoch number:  8 \tLoss: 0.38544319029830476\n",
      "Epoch number:  10 \tLoss: 0.35116394116996186\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.479765665335202\n",
      "Epoch number:  4 \tLoss: 2.319286983043993\n",
      "Epoch number:  6 \tLoss: 2.3013645614559617\n",
      "Epoch number:  8 \tLoss: 2.2994498662765634\n",
      "Epoch number:  10 \tLoss: 2.2992450563650038\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.497984508176319\n",
      "Epoch number:  4 \tLoss: 1.7315764052911473\n",
      "Epoch number:  6 \tLoss: 1.125241523060473\n",
      "Epoch number:  8 \tLoss: 0.6612536586779263\n",
      "Epoch number:  10 \tLoss: 0.5141359363217495\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.494428987723744\n",
      "Epoch number:  4 \tLoss: 2.327808115546446\n",
      "Epoch number:  6 \tLoss: 2.303639320278117\n",
      "Epoch number:  8 \tLoss: 2.3000432737736314\n",
      "Epoch number:  10 \tLoss: 2.299504788945139\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 1.7802217891208407\n",
      "Epoch number:  4 \tLoss: 0.6530096568641104\n",
      "Epoch number:  6 \tLoss: 0.5173099015621493\n",
      "Epoch number:  8 \tLoss: 0.446885918824194\n",
      "Epoch number:  10 \tLoss: 0.4050424351992916\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.4845419354030533\n",
      "Epoch number:  4 \tLoss: 2.3219833223664104\n",
      "Epoch number:  6 \tLoss: 2.3022331007034604\n",
      "Epoch number:  8 \tLoss: 2.299886676933487\n",
      "Epoch number:  10 \tLoss: 2.2996054003956226\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 0.7222860896853802\n",
      "Epoch number:  4 \tLoss: 0.49967516665657263\n",
      "Epoch number:  6 \tLoss: 0.41403846013322804\n",
      "Epoch number:  8 \tLoss: 0.3765670423102968\n",
      "Epoch number:  10 \tLoss: 0.3444709447639266\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=random, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.477438833108717\n",
      "Epoch number:  4 \tLoss: 2.318653112794988\n",
      "Epoch number:  6 \tLoss: 2.301668630963632\n",
      "Epoch number:  8 \tLoss: 2.299927436622269\n",
      "Epoch number:  10 \tLoss: 2.2997426290955203\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.007303326511128\n",
      "Epoch number:  4 \tLoss: 2.5980564348689787\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.007319837522018\n",
      "Epoch number:  4 \tLoss: 2.598072836026575\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.0073067042678012\n",
      "Epoch number:  4 \tLoss: 2.5980597638183216\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.0073210774893924\n",
      "Epoch number:  4 \tLoss: 2.598074209044145\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.007262511564866\n",
      "Epoch number:  4 \tLoss: 2.598015588776035\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.0072959629033003\n",
      "Epoch number:  4 \tLoss: 2.598049036371927\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.007280123109693\n",
      "Epoch number:  4 \tLoss: 2.5980332419389183\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.0072354119374394\n",
      "Epoch number:  4 \tLoss: 2.5979884654029948\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.0073034234792457\n",
      "Epoch number:  4 \tLoss: 2.598056541837899\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.0072742688107335\n",
      "Epoch number:  4 \tLoss: 2.5980272757692884\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.0072923755628893\n",
      "Epoch number:  4 \tLoss: 2.5980454708718894\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.007295046709533\n",
      "Epoch number:  4 \tLoss: 2.5980480844234477\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.0072966678883524\n",
      "Epoch number:  4 \tLoss: 2.598051922500711\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.0073203803591317\n",
      "Epoch number:  4 \tLoss: 2.5980749078183036\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.0073288897203136\n",
      "Epoch number:  4 \tLoss: 2.5980831079427698\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.0072962521473006\n",
      "Epoch number:  4 \tLoss: 2.5980515412495144\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.007278886518139\n",
      "Epoch number:  4 \tLoss: 2.598034689260978\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.0072954642853804\n",
      "Epoch number:  4 \tLoss: 2.598050784950719\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.007343869879795\n",
      "Epoch number:  4 \tLoss: 2.59809704986372\n",
      "Epoch number:  6 \tLoss: 2.4402462675576353\n",
      "Epoch number:  8 \tLoss: 2.3685370588839176\n",
      "Epoch number:  10 \tLoss: 2.33391388838989\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.0073018527009165\n",
      "Epoch number:  4 \tLoss: 2.598053324702876\n",
      "Epoch number:  6 \tLoss: 2.4402067371438902\n",
      "Epoch number:  8 \tLoss: 2.3685062907760535\n",
      "Epoch number:  10 \tLoss: 2.3338921445539325\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.0073081640471764\n",
      "Epoch number:  4 \tLoss: 2.598061290058993\n",
      "Epoch number:  6 \tLoss: 2.440214987536527\n",
      "Epoch number:  8 \tLoss: 2.368512895191561\n",
      "Epoch number:  10 \tLoss: 2.333896845966604\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.0072853830794646\n",
      "Epoch number:  4 \tLoss: 2.5980384802355934\n",
      "Epoch number:  6 \tLoss: 2.4401950407504045\n",
      "Epoch number:  8 \tLoss: 2.3684974944414896\n",
      "Epoch number:  10 \tLoss: 2.333885987271831\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.007304469708594\n",
      "Epoch number:  4 \tLoss: 2.5980575844228895\n",
      "Epoch number:  6 \tLoss: 2.440211745112489\n",
      "Epoch number:  8 \tLoss: 2.3685103837431716\n",
      "Epoch number:  10 \tLoss: 2.3338950622429246\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.0073040547001355\n",
      "Epoch number:  4 \tLoss: 2.5980571644898056\n",
      "Epoch number:  6 \tLoss: 2.440211376328356\n",
      "Epoch number:  8 \tLoss: 2.3685101050649746\n",
      "Epoch number:  10 \tLoss: 2.3338948763785856\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.00729567943523\n",
      "Epoch number:  4 \tLoss: 2.5980487811679764\n",
      "Epoch number:  6 \tLoss: 2.4402040395165154\n",
      "Epoch number:  8 \tLoss: 2.3685044292087505\n",
      "Epoch number:  10 \tLoss: 2.333890860092017\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.007267692243297\n",
      "Epoch number:  4 \tLoss: 2.598020803461445\n",
      "Epoch number:  6 \tLoss: 2.4401796211602313\n",
      "Epoch number:  8 \tLoss: 2.3684856265204948\n",
      "Epoch number:  10 \tLoss: 2.3338776588780052\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.007301837956036\n",
      "Epoch number:  4 \tLoss: 2.5980549102336785\n",
      "Epoch number:  6 \tLoss: 2.440209414988767\n",
      "Epoch number:  8 \tLoss: 2.3685085962988137\n",
      "Epoch number:  10 \tLoss: 2.33389381362966\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.007297870664256\n",
      "Epoch number:  4 \tLoss: 2.598050754923326\n",
      "Epoch number:  6 \tLoss: 2.440205691764067\n",
      "Epoch number:  8 \tLoss: 2.368505664988926\n",
      "Epoch number:  10 \tLoss: 2.3338917378446737\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.007278817556306\n",
      "Epoch number:  4 \tLoss: 2.598031910796724\n",
      "Epoch number:  6 \tLoss: 2.4401893143717026\n",
      "Epoch number:  8 \tLoss: 2.368493083111188\n",
      "Epoch number:  10 \tLoss: 2.3338828872142003\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.0072825229389912\n",
      "Epoch number:  4 \tLoss: 2.598035434387423\n",
      "Epoch number:  6 \tLoss: 2.4401920870851623\n",
      "Epoch number:  8 \tLoss: 2.368495152779391\n",
      "Epoch number:  10 \tLoss: 2.333884320819339\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.007272795239329\n",
      "Epoch number:  4 \tLoss: 2.598028822775968\n",
      "Epoch number:  6 \tLoss: 2.4401894630452916\n",
      "Epoch number:  8 \tLoss: 2.368495975193121\n",
      "Epoch number:  10 \tLoss: 2.3338876406290234\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.007286174445378\n",
      "Epoch number:  4 \tLoss: 2.598041787985192\n",
      "Epoch number:  6 \tLoss: 2.44020037963892\n",
      "Epoch number:  8 \tLoss: 2.3685040001226714\n",
      "Epoch number:  10 \tLoss: 2.3338929080165705\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.007296080979374\n",
      "Epoch number:  4 \tLoss: 2.598051369354756\n",
      "Epoch number:  6 \tLoss: 2.4402084353750273\n",
      "Epoch number:  8 \tLoss: 2.368509904399755\n",
      "Epoch number:  10 \tLoss: 2.3338967612055703\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.007324626934414\n",
      "Epoch number:  4 \tLoss: 2.5980790053723113\n",
      "Epoch number:  6 \tLoss: 2.440231700170604\n",
      "Epoch number:  8 \tLoss: 2.368527005188695\n",
      "Epoch number:  10 \tLoss: 2.333907982321016\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.0072844035885917\n",
      "Epoch number:  4 \tLoss: 2.598040066939088\n",
      "Epoch number:  6 \tLoss: 2.4401989522786027\n",
      "Epoch number:  8 \tLoss: 2.368502976001316\n",
      "Epoch number:  10 \tLoss: 2.333892266011059\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.0072837463310225\n",
      "Epoch number:  4 \tLoss: 2.5980394290234754\n",
      "Epoch number:  6 \tLoss: 2.4401984200515967\n",
      "Epoch number:  8 \tLoss: 2.3685026110167025\n",
      "Epoch number:  10 \tLoss: 2.33389206377093\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.5408007333495473\n",
      "Epoch number:  4 \tLoss: 3.007092285591446\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.5407728185513068\n",
      "Epoch number:  4 \tLoss: 3.00705901250695\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.5407601070100827\n",
      "Epoch number:  4 \tLoss: 3.0070434917691693\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.540824065515914\n",
      "Epoch number:  4 \tLoss: 3.0071200341736533\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.5407712860638965\n",
      "Epoch number:  4 \tLoss: 3.007057124489275\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.5408195462108436\n",
      "Epoch number:  4 \tLoss: 3.007113836266749\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.5407811043227873\n",
      "Epoch number:  4 \tLoss: 3.0070688288550724\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.540840229195396\n",
      "Epoch number:  4 \tLoss: 3.007139352654308\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.5407878894628673\n",
      "Epoch number:  4 \tLoss: 3.0070767563272987\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.5407770284580873\n",
      "Epoch number:  4 \tLoss: 3.0070635647745148\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.540757755876047\n",
      "Epoch number:  4 \tLoss: 3.007040856843062\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.5408140075806003\n",
      "Epoch number:  4 \tLoss: 3.0071081077488384\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.5407626410055144\n",
      "Epoch number:  4 \tLoss: 3.0070478316583857\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.5408079546658455\n",
      "Epoch number:  4 \tLoss: 3.0071014364154083\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.5408064374414696\n",
      "Epoch number:  4 \tLoss: 3.007099546143622\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.5407982529019573\n",
      "Epoch number:  4 \tLoss: 3.007089884621038\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.5407655805564118\n",
      "Epoch number:  4 \tLoss: 3.007051089138203\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.5408209923994876\n",
      "Epoch number:  4 \tLoss: 3.007116445974254\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.5408169659550097\n",
      "Epoch number:  4 \tLoss: 3.0071115189352025\n",
      "Epoch number:  6 \tLoss: 2.7485346219348807\n",
      "Epoch number:  8 \tLoss: 2.5979015112909347\n",
      "Epoch number:  10 \tLoss: 2.502801183199662\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.540807857042418\n",
      "Epoch number:  4 \tLoss: 3.0071007282375004\n",
      "Epoch number:  6 \tLoss: 2.7485218565016436\n",
      "Epoch number:  8 \tLoss: 2.597887258543516\n",
      "Epoch number:  10 \tLoss: 2.502786264115068\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.540789722211478\n",
      "Epoch number:  4 \tLoss: 3.0070782540447696\n",
      "Epoch number:  6 \tLoss: 2.748495153311819\n",
      "Epoch number:  8 \tLoss: 2.59785743588005\n",
      "Epoch number:  10 \tLoss: 2.5027550495497892\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.540809022175273\n",
      "Epoch number:  4 \tLoss: 3.007102159241486\n",
      "Epoch number:  6 \tLoss: 2.748523561719769\n",
      "Epoch number:  8 \tLoss: 2.597889165280721\n",
      "Epoch number:  10 \tLoss: 2.5027882601874865\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.5407939572819003\n",
      "Epoch number:  4 \tLoss: 3.0070841949201763\n",
      "Epoch number:  6 \tLoss: 2.748502323383595\n",
      "Epoch number:  8 \tLoss: 2.5978654624423183\n",
      "Epoch number:  10 \tLoss: 2.5027634514657597\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.54076794277114\n",
      "Epoch number:  4 \tLoss: 3.007052611223458\n",
      "Epoch number:  6 \tLoss: 2.748464613904278\n",
      "Epoch number:  8 \tLoss: 2.5978233028315527\n",
      "Epoch number:  10 \tLoss: 2.502719318870648\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.5408068269765747\n",
      "Epoch number:  4 \tLoss: 3.0070995322999603\n",
      "Epoch number:  6 \tLoss: 2.7485204514303097\n",
      "Epoch number:  8 \tLoss: 2.597885690186438\n",
      "Epoch number:  10 \tLoss: 2.502784619871912\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.5408482051805343\n",
      "Epoch number:  4 \tLoss: 3.0071488889440188\n",
      "Epoch number:  6 \tLoss: 2.748578836390715\n",
      "Epoch number:  8 \tLoss: 2.5979508715915354\n",
      "Epoch number:  10 \tLoss: 2.5028528576543216\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.5407591724677925\n",
      "Epoch number:  4 \tLoss: 3.00704261861847\n",
      "Epoch number:  6 \tLoss: 2.7484531406540533\n",
      "Epoch number:  8 \tLoss: 2.597810573124718\n",
      "Epoch number:  10 \tLoss: 2.502706013565013\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.5408229950908647\n",
      "Epoch number:  4 \tLoss: 3.0071188183811723\n",
      "Epoch number:  6 \tLoss: 2.7485432699232026\n",
      "Epoch number:  8 \tLoss: 2.5979111703991262\n",
      "Epoch number:  10 \tLoss: 2.5028112999044505\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.5407771290567904\n",
      "Epoch number:  4 \tLoss: 3.007064028128377\n",
      "Epoch number:  6 \tLoss: 2.7484784685616597\n",
      "Epoch number:  8 \tLoss: 2.5978388460487807\n",
      "Epoch number:  10 \tLoss: 2.502735602585447\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.540797880552294\n",
      "Epoch number:  4 \tLoss: 3.0070879037645803\n",
      "Epoch number:  6 \tLoss: 2.7485065648723954\n",
      "Epoch number:  8 \tLoss: 2.5978701602270386\n",
      "Epoch number:  10 \tLoss: 2.5027683682066395\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.5408292701370025\n",
      "Epoch number:  4 \tLoss: 3.0071266299912414\n",
      "Epoch number:  6 \tLoss: 2.7485529505903017\n",
      "Epoch number:  8 \tLoss: 2.5979224806865857\n",
      "Epoch number:  10 \tLoss: 2.502823696463727\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.5408048686812994\n",
      "Epoch number:  4 \tLoss: 3.0070978270460795\n",
      "Epoch number:  6 \tLoss: 2.748519121847662\n",
      "Epoch number:  8 \tLoss: 2.5978849972804685\n",
      "Epoch number:  10 \tLoss: 2.502784768781162\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.5407941213999194\n",
      "Epoch number:  4 \tLoss: 3.0070851026813665\n",
      "Epoch number:  6 \tLoss: 2.7485041911744656\n",
      "Epoch number:  8 \tLoss: 2.597868447020819\n",
      "Epoch number:  10 \tLoss: 2.502767571766853\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.5408166795610727\n",
      "Epoch number:  4 \tLoss: 3.0071110166541715\n",
      "Epoch number:  6 \tLoss: 2.748534469161732\n",
      "Epoch number:  8 \tLoss: 2.5979019626786\n",
      "Epoch number:  10 \tLoss: 2.5028023759283045\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.540783390761985\n",
      "Epoch number:  4 \tLoss: 3.0070723692081547\n",
      "Epoch number:  6 \tLoss: 2.748489235300741\n",
      "Epoch number:  8 \tLoss: 2.59785187703243\n",
      "Epoch number:  10 \tLoss: 2.5027503682172254\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 3.5408217897924446\n",
      "Epoch number:  4 \tLoss: 3.0071176707586704\n",
      "Epoch number:  6 \tLoss: 2.7485423644222533\n",
      "Epoch number:  8 \tLoss: 2.597910696083544\n",
      "Epoch number:  10 \tLoss: 2.5028113489884922\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 4.143382380629389\n",
      "Epoch number:  4 \tLoss: 3.540796463313107\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 4.143324744838554\n",
      "Epoch number:  4 \tLoss: 3.540728079401391\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 4.143330010530533\n",
      "Epoch number:  4 \tLoss: 3.5407334988012202\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 4.143364705085962\n",
      "Epoch number:  4 \tLoss: 3.540775467766537\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 4.1433624869028245\n",
      "Epoch number:  4 \tLoss: 3.5407709342006446\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 4.143376900213831\n",
      "Epoch number:  4 \tLoss: 3.5407899165547314\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 4.143399569429363\n",
      "Epoch number:  4 \tLoss: 3.540816628929551\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 4.143363431589141\n",
      "Epoch number:  4 \tLoss: 3.540773952990233\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 4.143370045864989\n",
      "Epoch number:  4 \tLoss: 3.540781539189561\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 4.143374983497251\n",
      "Epoch number:  4 \tLoss: 3.5407875905407793\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 4.143337335426754\n",
      "Epoch number:  4 \tLoss: 3.540742204480372\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 4.14338429534516\n",
      "Epoch number:  4 \tLoss: 3.540797401441907\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 4.143347917725893\n",
      "Epoch number:  4 \tLoss: 3.5407556065705283\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 4.1433975744699225\n",
      "Epoch number:  4 \tLoss: 3.5408145181899977\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 4.143374057128914\n",
      "Epoch number:  4 \tLoss: 3.5407865330204293\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 4.14334859773128\n",
      "Epoch number:  4 \tLoss: 3.540756387084813\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 4.143379959635887\n",
      "Epoch number:  4 \tLoss: 3.540793022441894\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 4.143369086549642\n",
      "Epoch number:  4 \tLoss: 3.540780736693857\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 4.1433761742512845\n",
      "Epoch number:  4 \tLoss: 3.5407890016627546\n",
      "Epoch number:  6 \tLoss: 3.2170216959046996\n",
      "Epoch number:  8 \tLoss: 3.007052697897916\n",
      "Epoch number:  10 \tLoss: 2.8586418189564418\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 4.143336587912853\n",
      "Epoch number:  4 \tLoss: 3.5407421248078514\n",
      "Epoch number:  6 \tLoss: 3.216963170732814\n",
      "Epoch number:  8 \tLoss: 3.0069791270168365\n",
      "Epoch number:  10 \tLoss: 2.858550869567726\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 4.143384290659684\n",
      "Epoch number:  4 \tLoss: 3.540798505682895\n",
      "Epoch number:  6 \tLoss: 3.217033535653243\n",
      "Epoch number:  8 \tLoss: 3.007067571139755\n",
      "Epoch number:  10 \tLoss: 2.85866019917904\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 4.143358369605067\n",
      "Epoch number:  4 \tLoss: 3.5407679559410172\n",
      "Epoch number:  6 \tLoss: 3.216995421048651\n",
      "Epoch number:  8 \tLoss: 3.007019660277324\n",
      "Epoch number:  10 \tLoss: 2.858600966532533\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 4.143337218241753\n",
      "Epoch number:  4 \tLoss: 3.5407415876665342\n",
      "Epoch number:  6 \tLoss: 3.2169622149859864\n",
      "Epoch number:  8 \tLoss: 3.0069778812902612\n",
      "Epoch number:  10 \tLoss: 2.8585493479552944\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 4.143337218972071\n",
      "Epoch number:  4 \tLoss: 3.5407428023051444\n",
      "Epoch number:  6 \tLoss: 3.2169640019078733\n",
      "Epoch number:  8 \tLoss: 3.0069801769715814\n",
      "Epoch number:  10 \tLoss: 2.8585521769207856\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 4.143370092049835\n",
      "Epoch number:  4 \tLoss: 3.5407817409138467\n",
      "Epoch number:  6 \tLoss: 3.2170126167541535\n",
      "Epoch number:  8 \tLoss: 3.0070412791042127\n",
      "Epoch number:  10 \tLoss: 2.8586276985018784\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 4.143243169028404\n",
      "Epoch number:  4 \tLoss: 3.540631434339056\n",
      "Epoch number:  6 \tLoss: 3.216825053820645\n",
      "Epoch number:  8 \tLoss: 3.0068056155406166\n",
      "Epoch number:  10 \tLoss: 2.8583365011142248\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 4.143373616887845\n",
      "Epoch number:  4 \tLoss: 3.540785649234837\n",
      "Epoch number:  6 \tLoss: 3.2170174573883963\n",
      "Epoch number:  8 \tLoss: 3.007047365399068\n",
      "Epoch number:  10 \tLoss: 2.858635232219015\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 4.143375717296031\n",
      "Epoch number:  4 \tLoss: 3.5407885132840233\n",
      "Epoch number:  6 \tLoss: 3.217020765069413\n",
      "Epoch number:  8 \tLoss: 3.007051404622159\n",
      "Epoch number:  10 \tLoss: 2.8586401672357415\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 4.143370545258522\n",
      "Epoch number:  4 \tLoss: 3.5407821546713087\n",
      "Epoch number:  6 \tLoss: 3.217013110225932\n",
      "Epoch number:  8 \tLoss: 3.0070418927751814\n",
      "Epoch number:  10 \tLoss: 2.85862845330542\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 4.143338033871776\n",
      "Epoch number:  4 \tLoss: 3.54074372755186\n",
      "Epoch number:  6 \tLoss: 3.216965056989691\n",
      "Epoch number:  8 \tLoss: 3.006981478250046\n",
      "Epoch number:  10 \tLoss: 2.858553783361443\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 4.143321826576356\n",
      "Epoch number:  4 \tLoss: 3.5407248616601215\n",
      "Epoch number:  6 \tLoss: 3.2169419141363167\n",
      "Epoch number:  8 \tLoss: 3.0069527624204686\n",
      "Epoch number:  10 \tLoss: 2.8585187067845204\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 4.143321923838761\n",
      "Epoch number:  4 \tLoss: 3.540724986248764\n",
      "Epoch number:  6 \tLoss: 3.216942076584775\n",
      "Epoch number:  8 \tLoss: 3.0069529761988605\n",
      "Epoch number:  10 \tLoss: 2.8585189818520966\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 4.143386891426815\n",
      "Epoch number:  4 \tLoss: 3.5408015895917093\n",
      "Epoch number:  6 \tLoss: 3.2170374591542177\n",
      "Epoch number:  8 \tLoss: 3.0070726144029005\n",
      "Epoch number:  10 \tLoss: 2.8586665851734288\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 4.143384848150306\n",
      "Epoch number:  4 \tLoss: 3.5407994458620045\n",
      "Epoch number:  6 \tLoss: 3.217034835502871\n",
      "Epoch number:  8 \tLoss: 3.0070693248851303\n",
      "Epoch number:  10 \tLoss: 2.858662517007919\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 4.143346934161053\n",
      "Epoch number:  4 \tLoss: 3.540754094228845\n",
      "Epoch number:  6 \tLoss: 3.216978237613702\n",
      "Epoch number:  8 \tLoss: 3.0069983067213752\n",
      "Epoch number:  10 \tLoss: 2.8585748962604463\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 4.143369299108342\n",
      "Epoch number:  4 \tLoss: 3.5407809404436277\n",
      "Epoch number:  6 \tLoss: 3.2170117161816907\n",
      "Epoch number:  8 \tLoss: 3.0070402694450085\n",
      "Epoch number:  10 \tLoss: 2.858626625147274\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300423354363569\n",
      "Epoch number:  4 \tLoss: 2.2993223235281377\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3004232638846256\n",
      "Epoch number:  4 \tLoss: 2.299321812764791\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300423662167955\n",
      "Epoch number:  4 \tLoss: 2.2993226725985623\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3004207435400224\n",
      "Epoch number:  4 \tLoss: 2.2993220322769057\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3004220324017925\n",
      "Epoch number:  4 \tLoss: 2.29932202251951\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3004225212226324\n",
      "Epoch number:  4 \tLoss: 2.2993218055556426\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3004189869886393\n",
      "Epoch number:  4 \tLoss: 2.299321668795048\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3004222623030413\n",
      "Epoch number:  4 \tLoss: 2.2993219493315324\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.30042264094099\n",
      "Epoch number:  4 \tLoss: 2.2993219657048285\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3004227570449602\n",
      "Epoch number:  4 \tLoss: 2.2993219949108252\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3004209488008525\n",
      "Epoch number:  4 \tLoss: 2.2993216935066334\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300421545212809\n",
      "Epoch number:  4 \tLoss: 2.2993221494091243\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3004259985886493\n",
      "Epoch number:  4 \tLoss: 2.299324358770963\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3004261018071164\n",
      "Epoch number:  4 \tLoss: 2.299324325832827\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3004275283664137\n",
      "Epoch number:  4 \tLoss: 2.2993269610536844\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300426542190437\n",
      "Epoch number:  4 \tLoss: 2.2993250292211487\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3004274038641754\n",
      "Epoch number:  4 \tLoss: 2.2993268725688067\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300427532635474\n",
      "Epoch number:  4 \tLoss: 2.299326955524624\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3004214660685354\n",
      "Epoch number:  4 \tLoss: 2.2993215435893934\n",
      "Epoch number:  6 \tLoss: 2.2993203330923664\n",
      "Epoch number:  8 \tLoss: 2.2993203329658587\n",
      "Epoch number:  10 \tLoss: 2.2993203235473865\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3004239673201106\n",
      "Epoch number:  4 \tLoss: 2.2993218501656947\n",
      "Epoch number:  6 \tLoss: 2.2993206430541884\n",
      "Epoch number:  8 \tLoss: 2.299320642001206\n",
      "Epoch number:  10 \tLoss: 2.2993206422703962\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300420090152842\n",
      "Epoch number:  4 \tLoss: 2.299320686275995\n",
      "Epoch number:  6 \tLoss: 2.29931944857335\n",
      "Epoch number:  8 \tLoss: 2.2993194422756424\n",
      "Epoch number:  10 \tLoss: 2.299319421719842\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300422170276902\n",
      "Epoch number:  4 \tLoss: 2.2993217927122904\n",
      "Epoch number:  6 \tLoss: 2.2993205893841036\n",
      "Epoch number:  8 \tLoss: 2.2993205877877148\n",
      "Epoch number:  10 \tLoss: 2.299320587506346\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3004220112248515\n",
      "Epoch number:  4 \tLoss: 2.2993220797299103\n",
      "Epoch number:  6 \tLoss: 2.2993208729440724\n",
      "Epoch number:  8 \tLoss: 2.2993208779545626\n",
      "Epoch number:  10 \tLoss: 2.2993208420351534\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300421228229506\n",
      "Epoch number:  4 \tLoss: 2.2993220212388485\n",
      "Epoch number:  6 \tLoss: 2.299320820769994\n",
      "Epoch number:  8 \tLoss: 2.29932081911348\n",
      "Epoch number:  10 \tLoss: 2.2993208187763012\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3004235645876494\n",
      "Epoch number:  4 \tLoss: 2.2993221715065895\n",
      "Epoch number:  6 \tLoss: 2.2993209596556814\n",
      "Epoch number:  8 \tLoss: 2.2993209150908833\n",
      "Epoch number:  10 \tLoss: 2.2993208960508342\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3004220804820403\n",
      "Epoch number:  4 \tLoss: 2.2993215117143917\n",
      "Epoch number:  6 \tLoss: 2.299320308212459\n",
      "Epoch number:  8 \tLoss: 2.2993203069177506\n",
      "Epoch number:  10 \tLoss: 2.2993203069398067\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300423282403134\n",
      "Epoch number:  4 \tLoss: 2.299322032727688\n",
      "Epoch number:  6 \tLoss: 2.2993208533042284\n",
      "Epoch number:  8 \tLoss: 2.299320821739936\n",
      "Epoch number:  10 \tLoss: 2.2993208151387527\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300421733483646\n",
      "Epoch number:  4 \tLoss: 2.2993218324523035\n",
      "Epoch number:  6 \tLoss: 2.2993206300760787\n",
      "Epoch number:  8 \tLoss: 2.2993206282518903\n",
      "Epoch number:  10 \tLoss: 2.2993206277392346\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300421829836271\n",
      "Epoch number:  4 \tLoss: 2.2993220756475403\n",
      "Epoch number:  6 \tLoss: 2.299320861163336\n",
      "Epoch number:  8 \tLoss: 2.299320841864486\n",
      "Epoch number:  10 \tLoss: 2.2993208466643336\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3004216393087566\n",
      "Epoch number:  4 \tLoss: 2.2993219778957457\n",
      "Epoch number:  6 \tLoss: 2.2993207761848886\n",
      "Epoch number:  8 \tLoss: 2.299320774432937\n",
      "Epoch number:  10 \tLoss: 2.2993207739909898\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3004271332861244\n",
      "Epoch number:  4 \tLoss: 2.2993263012857623\n",
      "Epoch number:  6 \tLoss: 2.2993243026417614\n",
      "Epoch number:  8 \tLoss: 2.2993236519916467\n",
      "Epoch number:  10 \tLoss: 2.2993231235207343\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300425487913912\n",
      "Epoch number:  4 \tLoss: 2.299323314651085\n",
      "Epoch number:  6 \tLoss: 2.299321856492536\n",
      "Epoch number:  8 \tLoss: 2.299321654991582\n",
      "Epoch number:  10 \tLoss: 2.299321495114236\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3004263155773286\n",
      "Epoch number:  4 \tLoss: 2.299324822751321\n",
      "Epoch number:  6 \tLoss: 2.2993230982324633\n",
      "Epoch number:  8 \tLoss: 2.29932266940029\n",
      "Epoch number:  10 \tLoss: 2.299322322641076\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3004278577677204\n",
      "Epoch number:  4 \tLoss: 2.2993272358521906\n",
      "Epoch number:  6 \tLoss: 2.2993250257976663\n",
      "Epoch number:  8 \tLoss: 2.2993242148677595\n",
      "Epoch number:  10 \tLoss: 2.2993235643700074\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3004283829140193\n",
      "Epoch number:  4 \tLoss: 2.299327913448067\n",
      "Epoch number:  6 \tLoss: 2.2993255728979785\n",
      "Epoch number:  8 \tLoss: 2.2993246512626975\n",
      "Epoch number:  10 \tLoss: 2.299323913311509\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3004266702424268\n",
      "Epoch number:  4 \tLoss: 2.299325559928298\n",
      "Epoch number:  6 \tLoss: 2.29932369922621\n",
      "Epoch number:  8 \tLoss: 2.2993231616744016\n",
      "Epoch number:  10 \tLoss: 2.299322725079372\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.333060676588773\n",
      "Epoch number:  4 \tLoss: 2.3003517366909567\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.333051790781148\n",
      "Epoch number:  4 \tLoss: 2.3003510270235243\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3330353253136615\n",
      "Epoch number:  4 \tLoss: 2.3003496326273027\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3329672017047\n",
      "Epoch number:  4 \tLoss: 2.300342014465104\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3330222931434714\n",
      "Epoch number:  4 \tLoss: 2.3003478551033822\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.333011370363846\n",
      "Epoch number:  4 \tLoss: 2.3003467090952703\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3330312444228505\n",
      "Epoch number:  4 \tLoss: 2.300349022484695\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3330123570448627\n",
      "Epoch number:  4 \tLoss: 2.3003472600018218\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3329905248434613\n",
      "Epoch number:  4 \tLoss: 2.300345093720534\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.33303672332451\n",
      "Epoch number:  4 \tLoss: 2.3003493670839568\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3330106998216245\n",
      "Epoch number:  4 \tLoss: 2.3003467731984437\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3330206350474008\n",
      "Epoch number:  4 \tLoss: 2.300348215597861\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3330799290697204\n",
      "Epoch number:  4 \tLoss: 2.3003551884611095\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.333086534917886\n",
      "Epoch number:  4 \tLoss: 2.3003554028195725\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3330308173553074\n",
      "Epoch number:  4 \tLoss: 2.300353616452021\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.333035765495144\n",
      "Epoch number:  4 \tLoss: 2.300354212206216\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3330159606655347\n",
      "Epoch number:  4 \tLoss: 2.3003539022691006\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.33299750928436\n",
      "Epoch number:  4 \tLoss: 2.3003535875687082\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3330378109665517\n",
      "Epoch number:  4 \tLoss: 2.300348910379451\n",
      "Epoch number:  6 \tLoss: 2.299289162310707\n",
      "Epoch number:  8 \tLoss: 2.2992542167812493\n",
      "Epoch number:  10 \tLoss: 2.2992530510879843\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.332963029711831\n",
      "Epoch number:  4 \tLoss: 2.3003428174087\n",
      "Epoch number:  6 \tLoss: 2.2992898739125613\n",
      "Epoch number:  8 \tLoss: 2.299255339728014\n",
      "Epoch number:  10 \tLoss: 2.2992542080125107\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3330327531556554\n",
      "Epoch number:  4 \tLoss: 2.3003492606206972\n",
      "Epoch number:  6 \tLoss: 2.2992900301548835\n",
      "Epoch number:  8 \tLoss: 2.2992551376738155\n",
      "Epoch number:  10 \tLoss: 2.299253995876631\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3330006182154595\n",
      "Epoch number:  4 \tLoss: 2.300346293808545\n",
      "Epoch number:  6 \tLoss: 2.299289986719829\n",
      "Epoch number:  8 \tLoss: 2.2992552643092354\n",
      "Epoch number:  10 \tLoss: 2.2992541233080037\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3330284667541483\n",
      "Epoch number:  4 \tLoss: 2.3003485507242054\n",
      "Epoch number:  6 \tLoss: 2.2992896896491706\n",
      "Epoch number:  8 \tLoss: 2.2992548070942744\n",
      "Epoch number:  10 \tLoss: 2.299253637786788\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3330160404136095\n",
      "Epoch number:  4 \tLoss: 2.300347296904541\n",
      "Epoch number:  6 \tLoss: 2.299289567717351\n",
      "Epoch number:  8 \tLoss: 2.299254765799924\n",
      "Epoch number:  10 \tLoss: 2.2992536210521197\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3330345525041847\n",
      "Epoch number:  4 \tLoss: 2.30034952388233\n",
      "Epoch number:  6 \tLoss: 2.299290151452\n",
      "Epoch number:  8 \tLoss: 2.2992553157609494\n",
      "Epoch number:  10 \tLoss: 2.299254102220851\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.332998652214341\n",
      "Epoch number:  4 \tLoss: 2.300346358915143\n",
      "Epoch number:  6 \tLoss: 2.2992902511662496\n",
      "Epoch number:  8 \tLoss: 2.299255539991793\n",
      "Epoch number:  10 \tLoss: 2.2992543996450543\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3330190624287095\n",
      "Epoch number:  4 \tLoss: 2.3003476543367363\n",
      "Epoch number:  6 \tLoss: 2.2992896062365755\n",
      "Epoch number:  8 \tLoss: 2.2992547717370564\n",
      "Epoch number:  10 \tLoss: 2.2992536195541304\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.333039260694482\n",
      "Epoch number:  4 \tLoss: 2.300349668523759\n",
      "Epoch number:  6 \tLoss: 2.2992898758333755\n",
      "Epoch number:  8 \tLoss: 2.299254958168414\n",
      "Epoch number:  10 \tLoss: 2.2992538077811346\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3330113235962315\n",
      "Epoch number:  4 \tLoss: 2.3003469687310663\n",
      "Epoch number:  6 \tLoss: 2.299289642419625\n",
      "Epoch number:  8 \tLoss: 2.2992548506509545\n",
      "Epoch number:  10 \tLoss: 2.299253695777525\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.333045040604231\n",
      "Epoch number:  4 \tLoss: 2.3003498998181775\n",
      "Epoch number:  6 \tLoss: 2.2992895594575344\n",
      "Epoch number:  8 \tLoss: 2.2992546112288914\n",
      "Epoch number:  10 \tLoss: 2.299253459514767\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3330101634916236\n",
      "Epoch number:  4 \tLoss: 2.3003522998850956\n",
      "Epoch number:  6 \tLoss: 2.2992952329088596\n",
      "Epoch number:  8 \tLoss: 2.299260102026398\n",
      "Epoch number:  10 \tLoss: 2.299258765540498\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3330683063822697\n",
      "Epoch number:  4 \tLoss: 2.3003551014005916\n",
      "Epoch number:  6 \tLoss: 2.299292689517563\n",
      "Epoch number:  8 \tLoss: 2.299257459533782\n",
      "Epoch number:  10 \tLoss: 2.2992561423703473\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.333036315432161\n",
      "Epoch number:  4 \tLoss: 2.3003539298709796\n",
      "Epoch number:  6 \tLoss: 2.299294369111834\n",
      "Epoch number:  8 \tLoss: 2.2992592229539737\n",
      "Epoch number:  10 \tLoss: 2.2992578346374266\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3330230567425456\n",
      "Epoch number:  4 \tLoss: 2.3003538623929978\n",
      "Epoch number:  6 \tLoss: 2.2992955176538867\n",
      "Epoch number:  8 \tLoss: 2.299260368598258\n",
      "Epoch number:  10 \tLoss: 2.2992589193271065\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3330283641945386\n",
      "Epoch number:  4 \tLoss: 2.300354105811588\n",
      "Epoch number:  6 \tLoss: 2.2992952441751364\n",
      "Epoch number:  8 \tLoss: 2.299260081622928\n",
      "Epoch number:  10 \tLoss: 2.299258633079333\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.333020805194654\n",
      "Epoch number:  4 \tLoss: 2.3003536392028936\n",
      "Epoch number:  6 \tLoss: 2.2992954974909963\n",
      "Epoch number:  8 \tLoss: 2.2992603671023946\n",
      "Epoch number:  10 \tLoss: 2.29925892523959\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.5008990532424216\n",
      "Epoch number:  4 \tLoss: 2.3327376869827114\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.501080098035976\n",
      "Epoch number:  4 \tLoss: 2.332888377270467\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.5010179431813047\n",
      "Epoch number:  4 \tLoss: 2.3328368791212726\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.501012594318159\n",
      "Epoch number:  4 \tLoss: 2.3328323543492115\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.5009470991302707\n",
      "Epoch number:  4 \tLoss: 2.33277744573221\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.501132930601858\n",
      "Epoch number:  4 \tLoss: 2.332933151210996\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.500962227170521\n",
      "Epoch number:  4 \tLoss: 2.3327903281500437\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.5010469227203473\n",
      "Epoch number:  4 \tLoss: 2.3328609660908377\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.5009294060912954\n",
      "Epoch number:  4 \tLoss: 2.332763028017543\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.5009404541060922\n",
      "Epoch number:  4 \tLoss: 2.332772291008572\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.50100627929422\n",
      "Epoch number:  4 \tLoss: 2.3328269229273038\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.5009990370572925\n",
      "Epoch number:  4 \tLoss: 2.3328207134443173\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.5011337637400675\n",
      "Epoch number:  4 \tLoss: 2.3329375335462417\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.5008121946198605\n",
      "Epoch number:  4 \tLoss: 2.3326751516199815\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.501029753094624\n",
      "Epoch number:  4 \tLoss: 2.3328522789891935\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.501025492570479\n",
      "Epoch number:  4 \tLoss: 2.332848676795952\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.500974987277423\n",
      "Epoch number:  4 \tLoss: 2.332807602225104\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.5010091560080943\n",
      "Epoch number:  4 \tLoss: 2.3328355008307904\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.5009132266966874\n",
      "Epoch number:  4 \tLoss: 2.332749210556535\n",
      "Epoch number:  6 \tLoss: 2.3051809079181935\n",
      "Epoch number:  8 \tLoss: 2.300300944187426\n",
      "Epoch number:  10 \tLoss: 2.299426311367892\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.501048939974478\n",
      "Epoch number:  4 \tLoss: 2.332862551057004\n",
      "Epoch number:  6 \tLoss: 2.3052235777998304\n",
      "Epoch number:  8 \tLoss: 2.300312964857458\n",
      "Epoch number:  10 \tLoss: 2.2994292849293485\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.5010517309304467\n",
      "Epoch number:  4 \tLoss: 2.3328649864539797\n",
      "Epoch number:  6 \tLoss: 2.305224616827005\n",
      "Epoch number:  8 \tLoss: 2.3003133540686935\n",
      "Epoch number:  10 \tLoss: 2.299429470968858\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.501040727202598\n",
      "Epoch number:  4 \tLoss: 2.33285576237574\n",
      "Epoch number:  6 \tLoss: 2.305221109705207\n",
      "Epoch number:  8 \tLoss: 2.3003123732822197\n",
      "Epoch number:  10 \tLoss: 2.2994292481309793\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.5008939887946187\n",
      "Epoch number:  4 \tLoss: 2.3327333454025228\n",
      "Epoch number:  6 \tLoss: 2.3051752066097264\n",
      "Epoch number:  8 \tLoss: 2.3002996396926174\n",
      "Epoch number:  10 \tLoss: 2.2994262815613467\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.5010076292310286\n",
      "Epoch number:  4 \tLoss: 2.332828246291561\n",
      "Epoch number:  6 \tLoss: 2.3052110066142215\n",
      "Epoch number:  8 \tLoss: 2.300309834509945\n",
      "Epoch number:  10 \tLoss: 2.299428944149357\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.5010902294193076\n",
      "Epoch number:  4 \tLoss: 2.332897299083933\n",
      "Epoch number:  6 \tLoss: 2.3052368487013757\n",
      "Epoch number:  8 \tLoss: 2.3003168678481867\n",
      "Epoch number:  10 \tLoss: 2.299430399821602\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.5012191864744127\n",
      "Epoch number:  4 \tLoss: 2.333005411599277\n",
      "Epoch number:  6 \tLoss: 2.3052776302550275\n",
      "Epoch number:  8 \tLoss: 2.300328210872826\n",
      "Epoch number:  10 \tLoss: 2.2994330148410698\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.5010129282156233\n",
      "Epoch number:  4 \tLoss: 2.3328327417191033\n",
      "Epoch number:  6 \tLoss: 2.305212749217134\n",
      "Epoch number:  8 \tLoss: 2.3003103381033605\n",
      "Epoch number:  10 \tLoss: 2.2994290630686027\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.5010693914870865\n",
      "Epoch number:  4 \tLoss: 2.332879796591989\n",
      "Epoch number:  6 \tLoss: 2.305230255582011\n",
      "Epoch number:  8 \tLoss: 2.3003150333419837\n",
      "Epoch number:  10 \tLoss: 2.299429988583731\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.5008952931065553\n",
      "Epoch number:  4 \tLoss: 2.332734419592646\n",
      "Epoch number:  6 \tLoss: 2.3051756010502813\n",
      "Epoch number:  8 \tLoss: 2.300299755419505\n",
      "Epoch number:  10 \tLoss: 2.299426318235611\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.5009655270568927\n",
      "Epoch number:  4 \tLoss: 2.332792973418985\n",
      "Epoch number:  6 \tLoss: 2.3051975650257397\n",
      "Epoch number:  8 \tLoss: 2.300305874608691\n",
      "Epoch number:  10 \tLoss: 2.299427782758876\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.5008977919448663\n",
      "Epoch number:  4 \tLoss: 2.3327451138586692\n",
      "Epoch number:  6 \tLoss: 2.305188203850232\n",
      "Epoch number:  8 \tLoss: 2.3003118853795446\n",
      "Epoch number:  10 \tLoss: 2.2994379606045787\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.5011575014238248\n",
      "Epoch number:  4 \tLoss: 2.332956830763734\n",
      "Epoch number:  6 \tLoss: 2.305262475645332\n",
      "Epoch number:  8 \tLoss: 2.30032720642015\n",
      "Epoch number:  10 \tLoss: 2.299436082671791\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.50098899082593\n",
      "Epoch number:  4 \tLoss: 2.3328191976307306\n",
      "Epoch number:  6 \tLoss: 2.3052139590257386\n",
      "Epoch number:  8 \tLoss: 2.3003169873272915\n",
      "Epoch number:  10 \tLoss: 2.299437046328166\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.5010877318480667\n",
      "Epoch number:  4 \tLoss: 2.3328996959205135\n",
      "Epoch number:  6 \tLoss: 2.3052422240501214\n",
      "Epoch number:  8 \tLoss: 2.300322849236303\n",
      "Epoch number:  10 \tLoss: 2.299436383724338\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.5009609561737602\n",
      "Epoch number:  4 \tLoss: 2.3327962924303423\n",
      "Epoch number:  6 \tLoss: 2.305205863892124\n",
      "Epoch number:  8 \tLoss: 2.3003152455973077\n",
      "Epoch number:  10 \tLoss: 2.2994371643478106\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.5010165451164292\n",
      "Epoch number:  4 \tLoss: 2.3328417281681366\n",
      "Epoch number:  6 \tLoss: 2.3052220980008875\n",
      "Epoch number:  8 \tLoss: 2.300318964989746\n",
      "Epoch number:  10 \tLoss: 2.2994372365174507\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3004298318322616\n",
      "Epoch number:  4 \tLoss: 2.299322738340922\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300429153239088\n",
      "Epoch number:  4 \tLoss: 2.2993219787471046\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.30042785997292\n",
      "Epoch number:  4 \tLoss: 2.2993221624951072\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.30042908646643\n",
      "Epoch number:  4 \tLoss: 2.299321545240282\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3004284934104815\n",
      "Epoch number:  4 \tLoss: 2.2993216626409083\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300429254994646\n",
      "Epoch number:  4 \tLoss: 2.299321731368624\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3004270170507146\n",
      "Epoch number:  4 \tLoss: 2.299320997838446\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300429061952708\n",
      "Epoch number:  4 \tLoss: 2.299321976331793\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3004288726911377\n",
      "Epoch number:  4 \tLoss: 2.2993222865684873\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3004283116601383\n",
      "Epoch number:  4 \tLoss: 2.299321796669298\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.30042878642446\n",
      "Epoch number:  4 \tLoss: 2.299321842834838\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300428690522656\n",
      "Epoch number:  4 \tLoss: 2.2993226797956563\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3004330263057917\n",
      "Epoch number:  4 \tLoss: 2.2993246208539695\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300434330979378\n",
      "Epoch number:  4 \tLoss: 2.2993268457901146\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300434521043002\n",
      "Epoch number:  4 \tLoss: 2.299327174363219\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300433466251497\n",
      "Epoch number:  4 \tLoss: 2.2993253788447845\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3004348982206415\n",
      "Epoch number:  4 \tLoss: 2.2993280564322145\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300433249144912\n",
      "Epoch number:  4 \tLoss: 2.299324833862461\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3004269936203525\n",
      "Epoch number:  4 \tLoss: 2.299322528882009\n",
      "Epoch number:  6 \tLoss: 2.299321158150838\n",
      "Epoch number:  8 \tLoss: 2.2993211518456182\n",
      "Epoch number:  10 \tLoss: 2.299321290597964\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3004297788142547\n",
      "Epoch number:  4 \tLoss: 2.2993220156889054\n",
      "Epoch number:  6 \tLoss: 2.299320796139417\n",
      "Epoch number:  8 \tLoss: 2.2993207946740815\n",
      "Epoch number:  10 \tLoss: 2.299320794550315\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3004279250938886\n",
      "Epoch number:  4 \tLoss: 2.2993220755539907\n",
      "Epoch number:  6 \tLoss: 2.29932085206897\n",
      "Epoch number:  8 \tLoss: 2.299320849537772\n",
      "Epoch number:  10 \tLoss: 2.2993208303525856\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3004293903112707\n",
      "Epoch number:  4 \tLoss: 2.2993219212354017\n",
      "Epoch number:  6 \tLoss: 2.299320702429451\n",
      "Epoch number:  8 \tLoss: 2.2993207009686603\n",
      "Epoch number:  10 \tLoss: 2.2993207008493655\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3004284619429476\n",
      "Epoch number:  4 \tLoss: 2.2993222837187006\n",
      "Epoch number:  6 \tLoss: 2.2993210452984\n",
      "Epoch number:  8 \tLoss: 2.2993210500569514\n",
      "Epoch number:  10 \tLoss: 2.2993210152132417\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3004285441658885\n",
      "Epoch number:  4 \tLoss: 2.2993221650466964\n",
      "Epoch number:  6 \tLoss: 2.2993209488191573\n",
      "Epoch number:  8 \tLoss: 2.2993209472129794\n",
      "Epoch number:  10 \tLoss: 2.299320946945791\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300430279633488\n",
      "Epoch number:  4 \tLoss: 2.299322123133152\n",
      "Epoch number:  6 \tLoss: 2.299320935299805\n",
      "Epoch number:  8 \tLoss: 2.299320945960464\n",
      "Epoch number:  10 \tLoss: 2.2993208377893066\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3004308190390623\n",
      "Epoch number:  4 \tLoss: 2.299321826844601\n",
      "Epoch number:  6 \tLoss: 2.2993206042341767\n",
      "Epoch number:  8 \tLoss: 2.299320602785213\n",
      "Epoch number:  10 \tLoss: 2.299320602683743\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3004275576463127\n",
      "Epoch number:  4 \tLoss: 2.299321437524324\n",
      "Epoch number:  6 \tLoss: 2.299320213855885\n",
      "Epoch number:  8 \tLoss: 2.299320213327634\n",
      "Epoch number:  10 \tLoss: 2.2993202106076724\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3004285458714073\n",
      "Epoch number:  4 \tLoss: 2.299322258159268\n",
      "Epoch number:  6 \tLoss: 2.2993210415651366\n",
      "Epoch number:  8 \tLoss: 2.2993210393679995\n",
      "Epoch number:  10 \tLoss: 2.299321038508793\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3004286176556423\n",
      "Epoch number:  4 \tLoss: 2.2993223411874104\n",
      "Epoch number:  6 \tLoss: 2.299321102439241\n",
      "Epoch number:  8 \tLoss: 2.299321098610698\n",
      "Epoch number:  10 \tLoss: 2.2993210937099917\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3004292693515787\n",
      "Epoch number:  4 \tLoss: 2.2993217915742763\n",
      "Epoch number:  6 \tLoss: 2.299320572803558\n",
      "Epoch number:  8 \tLoss: 2.299320571403923\n",
      "Epoch number:  10 \tLoss: 2.2993205713458114\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300433859907311\n",
      "Epoch number:  4 \tLoss: 2.2993257216506082\n",
      "Epoch number:  6 \tLoss: 2.2993237680390113\n",
      "Epoch number:  8 \tLoss: 2.2993231674026813\n",
      "Epoch number:  10 \tLoss: 2.2993226878367614\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300432731196509\n",
      "Epoch number:  4 \tLoss: 2.299324321893451\n",
      "Epoch number:  6 \tLoss: 2.2993226645000924\n",
      "Epoch number:  8 \tLoss: 2.299322302301574\n",
      "Epoch number:  10 \tLoss: 2.2993220058338686\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300434175058827\n",
      "Epoch number:  4 \tLoss: 2.299326766221632\n",
      "Epoch number:  6 \tLoss: 2.299324659708892\n",
      "Epoch number:  8 \tLoss: 2.299323919410826\n",
      "Epoch number:  10 \tLoss: 2.2993233168906575\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3004329742093477\n",
      "Epoch number:  4 \tLoss: 2.299324630890931\n",
      "Epoch number:  6 \tLoss: 2.2993229043704764\n",
      "Epoch number:  8 \tLoss: 2.2993224896017415\n",
      "Epoch number:  10 \tLoss: 2.299322152998092\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300433623544114\n",
      "Epoch number:  4 \tLoss: 2.2993258011871447\n",
      "Epoch number:  6 \tLoss: 2.2993238649928625\n",
      "Epoch number:  8 \tLoss: 2.299323271783855\n",
      "Epoch number:  10 \tLoss: 2.2993227903329174\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3004337196760862\n",
      "Epoch number:  4 \tLoss: 2.299325342287987\n",
      "Epoch number:  6 \tLoss: 2.2993234278781123\n",
      "Epoch number:  8 \tLoss: 2.2993228788149773\n",
      "Epoch number:  10 \tLoss: 2.2993224454748025\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.333149629267891\n",
      "Epoch number:  4 \tLoss: 2.300357064261145\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3330779062316793\n",
      "Epoch number:  4 \tLoss: 2.3003501766708547\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.333149339559332\n",
      "Epoch number:  4 \tLoss: 2.3003565044225205\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3331406246757966\n",
      "Epoch number:  4 \tLoss: 2.3003561834242716\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3331241525876107\n",
      "Epoch number:  4 \tLoss: 2.30035410735241\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.333098025429532\n",
      "Epoch number:  4 \tLoss: 2.3003522187999907\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3331571699462463\n",
      "Epoch number:  4 \tLoss: 2.300356964751504\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3331574062409035\n",
      "Epoch number:  4 \tLoss: 2.300357611781217\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3331445986515873\n",
      "Epoch number:  4 \tLoss: 2.3003561941769775\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.333144868982012\n",
      "Epoch number:  4 \tLoss: 2.30035637316901\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.333104473427889\n",
      "Epoch number:  4 \tLoss: 2.300352301550987\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.333136295988964\n",
      "Epoch number:  4 \tLoss: 2.3003555582879343\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.333162618207435\n",
      "Epoch number:  4 \tLoss: 2.3003610039247517\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3331539291235375\n",
      "Epoch number:  4 \tLoss: 2.300361211963781\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.333089835690391\n",
      "Epoch number:  4 \tLoss: 2.300360053412413\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.333156462780321\n",
      "Epoch number:  4 \tLoss: 2.3003612200176464\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3331161449629914\n",
      "Epoch number:  4 \tLoss: 2.300360297144075\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3331108809416246\n",
      "Epoch number:  4 \tLoss: 2.300360318220967\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3331431989707685\n",
      "Epoch number:  4 \tLoss: 2.3003562348431457\n",
      "Epoch number:  6 \tLoss: 2.29929017944475\n",
      "Epoch number:  8 \tLoss: 2.2992549455365703\n",
      "Epoch number:  10 \tLoss: 2.299253776573081\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3331068493625367\n",
      "Epoch number:  4 \tLoss: 2.3003533160177967\n",
      "Epoch number:  6 \tLoss: 2.29929059473272\n",
      "Epoch number:  8 \tLoss: 2.299255554292432\n",
      "Epoch number:  10 \tLoss: 2.2992543994340524\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3331365235477444\n",
      "Epoch number:  4 \tLoss: 2.300355367871086\n",
      "Epoch number:  6 \tLoss: 2.299289864390792\n",
      "Epoch number:  8 \tLoss: 2.2992546625959527\n",
      "Epoch number:  10 \tLoss: 2.299253490601074\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3331477041031223\n",
      "Epoch number:  4 \tLoss: 2.300356813783954\n",
      "Epoch number:  6 \tLoss: 2.2992903972170637\n",
      "Epoch number:  8 \tLoss: 2.299255148999252\n",
      "Epoch number:  10 \tLoss: 2.2992539839163872\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.333129805330252\n",
      "Epoch number:  4 \tLoss: 2.300354858214959\n",
      "Epoch number:  6 \tLoss: 2.299289978241743\n",
      "Epoch number:  8 \tLoss: 2.2992548258326737\n",
      "Epoch number:  10 \tLoss: 2.299253639791958\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3331024742964908\n",
      "Epoch number:  4 \tLoss: 2.3003523363472134\n",
      "Epoch number:  6 \tLoss: 2.2992899552784545\n",
      "Epoch number:  8 \tLoss: 2.299254934081198\n",
      "Epoch number:  10 \tLoss: 2.2992537802489124\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3331421618639694\n",
      "Epoch number:  4 \tLoss: 2.3003561999099222\n",
      "Epoch number:  6 \tLoss: 2.299290256420633\n",
      "Epoch number:  8 \tLoss: 2.2992549694992013\n",
      "Epoch number:  10 \tLoss: 2.299253784624091\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.333178637781483\n",
      "Epoch number:  4 \tLoss: 2.300359499827428\n",
      "Epoch number:  6 \tLoss: 2.2992902650150895\n",
      "Epoch number:  8 \tLoss: 2.2992548578062566\n",
      "Epoch number:  10 \tLoss: 2.2992536848656346\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.333140935833246\n",
      "Epoch number:  4 \tLoss: 2.3003555103175475\n",
      "Epoch number:  6 \tLoss: 2.2992895704662457\n",
      "Epoch number:  8 \tLoss: 2.299254341896873\n",
      "Epoch number:  10 \tLoss: 2.299253169411089\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.333137749014648\n",
      "Epoch number:  4 \tLoss: 2.3003556999952184\n",
      "Epoch number:  6 \tLoss: 2.2992901538531845\n",
      "Epoch number:  8 \tLoss: 2.299254954777997\n",
      "Epoch number:  10 \tLoss: 2.299253792335486\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.333136693813002\n",
      "Epoch number:  4 \tLoss: 2.3003558419046364\n",
      "Epoch number:  6 \tLoss: 2.2992903124346675\n",
      "Epoch number:  8 \tLoss: 2.2992551040205527\n",
      "Epoch number:  10 \tLoss: 2.2992539271700254\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.333151057981451\n",
      "Epoch number:  4 \tLoss: 2.3003566395347073\n",
      "Epoch number:  6 \tLoss: 2.299289865828521\n",
      "Epoch number:  8 \tLoss: 2.2992545976140932\n",
      "Epoch number:  10 \tLoss: 2.299253431730527\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3331824360575713\n",
      "Epoch number:  4 \tLoss: 2.3003617243593824\n",
      "Epoch number:  6 \tLoss: 2.2992920800468144\n",
      "Epoch number:  8 \tLoss: 2.299256572104356\n",
      "Epoch number:  10 \tLoss: 2.2992552822464254\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3331148994218136\n",
      "Epoch number:  4 \tLoss: 2.3003602738317195\n",
      "Epoch number:  6 \tLoss: 2.299296776412055\n",
      "Epoch number:  8 \tLoss: 2.2992613296675337\n",
      "Epoch number:  10 \tLoss: 2.299259821166077\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.333123145699857\n",
      "Epoch number:  4 \tLoss: 2.300360644118062\n",
      "Epoch number:  6 \tLoss: 2.299296397521063\n",
      "Epoch number:  8 \tLoss: 2.2992608895419866\n",
      "Epoch number:  10 \tLoss: 2.2992593939844825\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3331606289099898\n",
      "Epoch number:  4 \tLoss: 2.3003613339010323\n",
      "Epoch number:  6 \tLoss: 2.2992937186339075\n",
      "Epoch number:  8 \tLoss: 2.299258207205112\n",
      "Epoch number:  10 \tLoss: 2.2992568494225427\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3331384462541482\n",
      "Epoch number:  4 \tLoss: 2.300361162193065\n",
      "Epoch number:  6 \tLoss: 2.299295512648587\n",
      "Epoch number:  8 \tLoss: 2.2992600029836487\n",
      "Epoch number:  10 \tLoss: 2.299258543498267\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.33312397726893\n",
      "Epoch number:  4 \tLoss: 2.300360253702354\n",
      "Epoch number:  6 \tLoss: 2.299295942144257\n",
      "Epoch number:  8 \tLoss: 2.299260503199347\n",
      "Epoch number:  10 \tLoss: 2.2992590441462393\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.501506531268227\n",
      "Epoch number:  4 \tLoss: 2.333062748151916\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.5013332035404705\n",
      "Epoch number:  4 \tLoss: 2.3329176114104113\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.501319377834935\n",
      "Epoch number:  4 \tLoss: 2.332905878167012\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.5014394127650696\n",
      "Epoch number:  4 \tLoss: 2.3330064427186876\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.501331185243695\n",
      "Epoch number:  4 \tLoss: 2.3329155941805073\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.501334308680757\n",
      "Epoch number:  4 \tLoss: 2.3329184288710225\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.5015741360445154\n",
      "Epoch number:  4 \tLoss: 2.333119614529065\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.5013059913673326\n",
      "Epoch number:  4 \tLoss: 2.3328948166332286\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.501389443713078\n",
      "Epoch number:  4 \tLoss: 2.33296452859292\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.5013254297065113\n",
      "Epoch number:  4 \tLoss: 2.332911118436484\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.501260793493094\n",
      "Epoch number:  4 \tLoss: 2.332857173249172\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.5012723204372382\n",
      "Epoch number:  4 \tLoss: 2.3328666742867816\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.501295223663412\n",
      "Epoch number:  4 \tLoss: 2.332892980607551\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.501434414268962\n",
      "Epoch number:  4 \tLoss: 2.333006695571675\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.5014876242970376\n",
      "Epoch number:  4 \tLoss: 2.3330503414154258\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.501259746292926\n",
      "Epoch number:  4 \tLoss: 2.332864325135312\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.5012500555807313\n",
      "Epoch number:  4 \tLoss: 2.332856343216052\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.5013561744443162\n",
      "Epoch number:  4 \tLoss: 2.332942710097289\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.501099782999124\n",
      "Epoch number:  4 \tLoss: 2.3327226588812326\n",
      "Epoch number:  6 \tLoss: 2.305159133882188\n",
      "Epoch number:  8 \tLoss: 2.300293820405027\n",
      "Epoch number:  10 \tLoss: 2.299424550928247\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.5014191982219165\n",
      "Epoch number:  4 \tLoss: 2.332989429836415\n",
      "Epoch number:  6 \tLoss: 2.3052594835074713\n",
      "Epoch number:  8 \tLoss: 2.3003220020622366\n",
      "Epoch number:  10 \tLoss: 2.299431473763277\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.501367943806229\n",
      "Epoch number:  4 \tLoss: 2.3329465758645735\n",
      "Epoch number:  6 \tLoss: 2.3052434917015874\n",
      "Epoch number:  8 \tLoss: 2.3003177092692653\n",
      "Epoch number:  10 \tLoss: 2.299430639109546\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.5013841887106567\n",
      "Epoch number:  4 \tLoss: 2.332960194105629\n",
      "Epoch number:  6 \tLoss: 2.3052486647468395\n",
      "Epoch number:  8 \tLoss: 2.300319217096578\n",
      "Epoch number:  10 \tLoss: 2.299431070228112\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.501302261257574\n",
      "Epoch number:  4 \tLoss: 2.3328916754281614\n",
      "Epoch number:  6 \tLoss: 2.3052228325073756\n",
      "Epoch number:  8 \tLoss: 2.3003119540282677\n",
      "Epoch number:  10 \tLoss: 2.2994292904231863\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.5014519477050063\n",
      "Epoch number:  4 \tLoss: 2.333017067526071\n",
      "Epoch number:  6 \tLoss: 2.3052701669950477\n",
      "Epoch number:  8 \tLoss: 2.30032527006932\n",
      "Epoch number:  10 \tLoss: 2.2994325406614453\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.501318289936075\n",
      "Epoch number:  4 \tLoss: 2.332904948775752\n",
      "Epoch number:  6 \tLoss: 2.3052277214889845\n",
      "Epoch number:  8 \tLoss: 2.3003131650901807\n",
      "Epoch number:  10 \tLoss: 2.2994294094555863\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.501129747793612\n",
      "Epoch number:  4 \tLoss: 2.332747795538185\n",
      "Epoch number:  6 \tLoss: 2.3051689295132745\n",
      "Epoch number:  8 \tLoss: 2.3002970107831797\n",
      "Epoch number:  10 \tLoss: 2.2994258254629942\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.501311040959987\n",
      "Epoch number:  4 \tLoss: 2.332898844396131\n",
      "Epoch number:  6 \tLoss: 2.3052252868802436\n",
      "Epoch number:  8 \tLoss: 2.300312390239701\n",
      "Epoch number:  10 \tLoss: 2.299429104625968\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.501401084220803\n",
      "Epoch number:  4 \tLoss: 2.332974453009202\n",
      "Epoch number:  6 \tLoss: 2.3052541714573778\n",
      "Epoch number:  8 \tLoss: 2.300320900241925\n",
      "Epoch number:  10 \tLoss: 2.299431619712001\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.5012305397575023\n",
      "Epoch number:  4 \tLoss: 2.3328316454777807\n",
      "Epoch number:  6 \tLoss: 2.305200133844056\n",
      "Epoch number:  8 \tLoss: 2.3003054503288674\n",
      "Epoch number:  10 \tLoss: 2.2994275653981067\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.5011989262415892\n",
      "Epoch number:  4 \tLoss: 2.332805374076923\n",
      "Epoch number:  6 \tLoss: 2.3051904527567526\n",
      "Epoch number:  8 \tLoss: 2.300302962377195\n",
      "Epoch number:  10 \tLoss: 2.2994272018983106\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.5014752892703465\n",
      "Epoch number:  4 \tLoss: 2.3330401466292665\n",
      "Epoch number:  6 \tLoss: 2.305282147988787\n",
      "Epoch number:  8 \tLoss: 2.300331827466001\n",
      "Epoch number:  10 \tLoss: 2.299437333112335\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.501570873455477\n",
      "Epoch number:  4 \tLoss: 2.333118832594743\n",
      "Epoch number:  6 \tLoss: 2.3053104283394332\n",
      "Epoch number:  8 \tLoss: 2.3003382867437168\n",
      "Epoch number:  10 \tLoss: 2.2994373680066196\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.5015077383647206\n",
      "Epoch number:  4 \tLoss: 2.3330667822017777\n",
      "Epoch number:  6 \tLoss: 2.3052916700356114\n",
      "Epoch number:  8 \tLoss: 2.3003339512939647\n",
      "Epoch number:  10 \tLoss: 2.2994372985916027\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.501301417064933\n",
      "Epoch number:  4 \tLoss: 2.332898082204255\n",
      "Epoch number:  6 \tLoss: 2.305232161566982\n",
      "Epoch number:  8 \tLoss: 2.3003214653420447\n",
      "Epoch number:  10 \tLoss: 2.2994385653930465\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.501262846790611\n",
      "Epoch number:  4 \tLoss: 2.332866579780939\n",
      "Epoch number:  6 \tLoss: 2.305221138056881\n",
      "Epoch number:  8 \tLoss: 2.3003192099681065\n",
      "Epoch number:  10 \tLoss: 2.299438853441124\n",
      "Training with params: learning_rate=0.0001, activation=relu, initialization=xavier, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.501363516252667\n",
      "Epoch number:  4 \tLoss: 2.3329487346369615\n",
      "Epoch number:  6 \tLoss: 2.3052501430211394\n",
      "Epoch number:  8 \tLoss: 2.3003254248681864\n",
      "Epoch number:  10 \tLoss: 2.299438426442765\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2993004839962743\n",
      "Epoch number:  4 \tLoss: 2.2993070923027665\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299299999087621\n",
      "Epoch number:  4 \tLoss: 2.2993068763178495\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299459987140025\n",
      "Epoch number:  4 \tLoss: 2.299459585856497\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2994604454954315\n",
      "Epoch number:  4 \tLoss: 2.299460107823661\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299805913496299\n",
      "Epoch number:  4 \tLoss: 2.299803925922329\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2997984268830285\n",
      "Epoch number:  4 \tLoss: 2.2997969516166212\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299297063989233\n",
      "Epoch number:  4 \tLoss: 2.2993039790695913\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299297995666542\n",
      "Epoch number:  4 \tLoss: 2.2993045902318494\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2994573905351934\n",
      "Epoch number:  4 \tLoss: 2.299457012781712\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299463914978791\n",
      "Epoch number:  4 \tLoss: 2.2994635723851213\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2998003062749386\n",
      "Epoch number:  4 \tLoss: 2.299798315891117\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299805480659012\n",
      "Epoch number:  4 \tLoss: 2.2998040030122926\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.295610947075919\n",
      "Epoch number:  4 \tLoss: 2.2956206760318247\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.295610331744357\n",
      "Epoch number:  4 \tLoss: 2.2956200006647474\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2973514583161467\n",
      "Epoch number:  4 \tLoss: 2.297352139343269\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.297364498559124\n",
      "Epoch number:  4 \tLoss: 2.297364841562693\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2986899398715193\n",
      "Epoch number:  4 \tLoss: 2.2986866367288683\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2987015076370887\n",
      "Epoch number:  4 \tLoss: 2.2986980977630087\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2993042644824704\n",
      "Epoch number:  4 \tLoss: 2.2993103193908584\n",
      "Epoch number:  6 \tLoss: 2.2993102225767656\n",
      "Epoch number:  8 \tLoss: 2.2993101258291246\n",
      "Epoch number:  10 \tLoss: 2.2993100291712802\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299301553561653\n",
      "Epoch number:  4 \tLoss: 2.299308106792588\n",
      "Epoch number:  6 \tLoss: 2.2993080249871363\n",
      "Epoch number:  8 \tLoss: 2.2993079432039156\n",
      "Epoch number:  10 \tLoss: 2.299307861471723\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299461383193384\n",
      "Epoch number:  4 \tLoss: 2.2994609901982472\n",
      "Epoch number:  6 \tLoss: 2.2994605962595225\n",
      "Epoch number:  8 \tLoss: 2.299460202790343\n",
      "Epoch number:  10 \tLoss: 2.299459809774124\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299459269776321\n",
      "Epoch number:  4 \tLoss: 2.2994589329164605\n",
      "Epoch number:  6 \tLoss: 2.2994585950298716\n",
      "Epoch number:  8 \tLoss: 2.299458257539299\n",
      "Epoch number:  10 \tLoss: 2.299457920444252\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2998025522894263\n",
      "Epoch number:  4 \tLoss: 2.2998006575144756\n",
      "Epoch number:  6 \tLoss: 2.299798768274789\n",
      "Epoch number:  8 \tLoss: 2.2997968839555374\n",
      "Epoch number:  10 \tLoss: 2.2997950039480237\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2998021905598547\n",
      "Epoch number:  4 \tLoss: 2.299800723014901\n",
      "Epoch number:  6 \tLoss: 2.2997992587000153\n",
      "Epoch number:  8 \tLoss: 2.29979779761065\n",
      "Epoch number:  10 \tLoss: 2.2997963397422456\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2993002527258803\n",
      "Epoch number:  4 \tLoss: 2.2993066333863363\n",
      "Epoch number:  6 \tLoss: 2.299306534906195\n",
      "Epoch number:  8 \tLoss: 2.299306436575771\n",
      "Epoch number:  10 \tLoss: 2.2993063384207826\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2992981102896035\n",
      "Epoch number:  4 \tLoss: 2.2993044472259196\n",
      "Epoch number:  6 \tLoss: 2.2993043646672637\n",
      "Epoch number:  8 \tLoss: 2.2993042821358376\n",
      "Epoch number:  10 \tLoss: 2.2993041996586525\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2994581232598454\n",
      "Epoch number:  4 \tLoss: 2.299457730681626\n",
      "Epoch number:  6 \tLoss: 2.2994573370048914\n",
      "Epoch number:  8 \tLoss: 2.299456943608429\n",
      "Epoch number:  10 \tLoss: 2.299456550471356\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2994600278101847\n",
      "Epoch number:  4 \tLoss: 2.299459685248639\n",
      "Epoch number:  6 \tLoss: 2.299459341857627\n",
      "Epoch number:  8 \tLoss: 2.2994589988774705\n",
      "Epoch number:  10 \tLoss: 2.299458656307649\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.29980321333971\n",
      "Epoch number:  4 \tLoss: 2.299801326709201\n",
      "Epoch number:  6 \tLoss: 2.299799439845226\n",
      "Epoch number:  8 \tLoss: 2.2997975521345513\n",
      "Epoch number:  10 \tLoss: 2.2997956629502694\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2997935195621015\n",
      "Epoch number:  4 \tLoss: 2.299792070474609\n",
      "Epoch number:  6 \tLoss: 2.2997906246142845\n",
      "Epoch number:  8 \tLoss: 2.299789181976423\n",
      "Epoch number:  10 \tLoss: 2.2997877425563074\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.295611058566734\n",
      "Epoch number:  4 \tLoss: 2.295620733558566\n",
      "Epoch number:  6 \tLoss: 2.2956240593969164\n",
      "Epoch number:  8 \tLoss: 2.2956273386089188\n",
      "Epoch number:  10 \tLoss: 2.2956305704370323\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2956108861256284\n",
      "Epoch number:  4 \tLoss: 2.295620569038038\n",
      "Epoch number:  6 \tLoss: 2.2956238827678725\n",
      "Epoch number:  8 \tLoss: 2.29562715185252\n",
      "Epoch number:  10 \tLoss: 2.2956303754471628\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.297354306590945\n",
      "Epoch number:  4 \tLoss: 2.297354961689927\n",
      "Epoch number:  6 \tLoss: 2.297355630630675\n",
      "Epoch number:  8 \tLoss: 2.2973563122346676\n",
      "Epoch number:  10 \tLoss: 2.2973570043109754\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2973593510421706\n",
      "Epoch number:  4 \tLoss: 2.2973598660546237\n",
      "Epoch number:  6 \tLoss: 2.2973603932600017\n",
      "Epoch number:  8 \tLoss: 2.2973609325010207\n",
      "Epoch number:  10 \tLoss: 2.2973614824640927\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2986909215932765\n",
      "Epoch number:  4 \tLoss: 2.2986876151697593\n",
      "Epoch number:  6 \tLoss: 2.2986845015470045\n",
      "Epoch number:  8 \tLoss: 2.2986815644636933\n",
      "Epoch number:  10 \tLoss: 2.298678789489141\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.298703502946717\n",
      "Epoch number:  4 \tLoss: 2.2987000233222084\n",
      "Epoch number:  6 \tLoss: 2.2986966841866283\n",
      "Epoch number:  8 \tLoss: 2.298693479839458\n",
      "Epoch number:  10 \tLoss: 2.298690404813599\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2992985721304717\n",
      "Epoch number:  4 \tLoss: 2.2993054607806704\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2992993908494066\n",
      "Epoch number:  4 \tLoss: 2.2993063511677647\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2994552574457674\n",
      "Epoch number:  4 \tLoss: 2.29945485742486\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2994597818595626\n",
      "Epoch number:  4 \tLoss: 2.299459437950464\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2998012612672336\n",
      "Epoch number:  4 \tLoss: 2.2997992837693784\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2997985211358403\n",
      "Epoch number:  4 \tLoss: 2.2997970493858886\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2992980835320505\n",
      "Epoch number:  4 \tLoss: 2.2993048432795726\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2992997568873643\n",
      "Epoch number:  4 \tLoss: 2.299306247833874\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2994625820858894\n",
      "Epoch number:  4 \tLoss: 2.299462171458297\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299459528949869\n",
      "Epoch number:  4 \tLoss: 2.299459184095247\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.29979789837136\n",
      "Epoch number:  4 \tLoss: 2.2997960112145877\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299799189819432\n",
      "Epoch number:  4 \tLoss: 2.2997977086603907\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2983890560259623\n",
      "Epoch number:  4 \tLoss: 2.298396014236702\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2983920881257376\n",
      "Epoch number:  4 \tLoss: 2.2983989044906905\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2989571110951394\n",
      "Epoch number:  4 \tLoss: 2.298956608085157\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2989584995531005\n",
      "Epoch number:  4 \tLoss: 2.2989580418141986\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2995640904010073\n",
      "Epoch number:  4 \tLoss: 2.2995617410039344\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2995840558416716\n",
      "Epoch number:  4 \tLoss: 2.299581912908402\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2993016217738096\n",
      "Epoch number:  4 \tLoss: 2.2993078060112424\n",
      "Epoch number:  6 \tLoss: 2.299307703400386\n",
      "Epoch number:  8 \tLoss: 2.2993076008451627\n",
      "Epoch number:  10 \tLoss: 2.299307498370315\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2993014969413097\n",
      "Epoch number:  4 \tLoss: 2.2993080146589273\n",
      "Epoch number:  6 \tLoss: 2.2993079252860475\n",
      "Epoch number:  8 \tLoss: 2.299307835928988\n",
      "Epoch number:  10 \tLoss: 2.2993077466163925\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299460132262055\n",
      "Epoch number:  4 \tLoss: 2.299459714154847\n",
      "Epoch number:  6 \tLoss: 2.299459295111092\n",
      "Epoch number:  8 \tLoss: 2.2994588764247865\n",
      "Epoch number:  10 \tLoss: 2.299458458069066\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299459343549167\n",
      "Epoch number:  4 \tLoss: 2.2994590034779905\n",
      "Epoch number:  6 \tLoss: 2.29945866231666\n",
      "Epoch number:  8 \tLoss: 2.2994583215268705\n",
      "Epoch number:  10 \tLoss: 2.2994579811080995\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2998027986622\n",
      "Epoch number:  4 \tLoss: 2.29980084350938\n",
      "Epoch number:  6 \tLoss: 2.2997988947582426\n",
      "Epoch number:  8 \tLoss: 2.299796951790188\n",
      "Epoch number:  10 \tLoss: 2.299795013997542\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299795700252727\n",
      "Epoch number:  4 \tLoss: 2.299794232277327\n",
      "Epoch number:  6 \tLoss: 2.299792767536533\n",
      "Epoch number:  8 \tLoss: 2.29979130602543\n",
      "Epoch number:  10 \tLoss: 2.299789847739086\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.29929960002364\n",
      "Epoch number:  4 \tLoss: 2.299306186295664\n",
      "Epoch number:  6 \tLoss: 2.2993060862418315\n",
      "Epoch number:  8 \tLoss: 2.2993059862032452\n",
      "Epoch number:  10 \tLoss: 2.299305886208092\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2992993204186956\n",
      "Epoch number:  4 \tLoss: 2.2993062016294825\n",
      "Epoch number:  6 \tLoss: 2.2993061131669923\n",
      "Epoch number:  8 \tLoss: 2.2993060247172545\n",
      "Epoch number:  10 \tLoss: 2.299305936311843\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2994600994528698\n",
      "Epoch number:  4 \tLoss: 2.29945969065674\n",
      "Epoch number:  6 \tLoss: 2.2994592810992582\n",
      "Epoch number:  8 \tLoss: 2.2994588720420706\n",
      "Epoch number:  10 \tLoss: 2.2994584634625066\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2994594118734555\n",
      "Epoch number:  4 \tLoss: 2.2994590668297517\n",
      "Epoch number:  6 \tLoss: 2.299458720848655\n",
      "Epoch number:  8 \tLoss: 2.2994583752525166\n",
      "Epoch number:  10 \tLoss: 2.299458030040798\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2998044179819375\n",
      "Epoch number:  4 \tLoss: 2.2998024772551227\n",
      "Epoch number:  6 \tLoss: 2.299800536343494\n",
      "Epoch number:  8 \tLoss: 2.2997985946040966\n",
      "Epoch number:  10 \tLoss: 2.299796651375845\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299803864923877\n",
      "Epoch number:  4 \tLoss: 2.2998023821343105\n",
      "Epoch number:  6 \tLoss: 2.299800902588059\n",
      "Epoch number:  8 \tLoss: 2.299799426280279\n",
      "Epoch number:  10 \tLoss: 2.29979795320612\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2983922919926996\n",
      "Epoch number:  4 \tLoss: 2.298398937279618\n",
      "Epoch number:  6 \tLoss: 2.2983989988675906\n",
      "Epoch number:  8 \tLoss: 2.2983990611273852\n",
      "Epoch number:  10 \tLoss: 2.2983991240609276\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.29839344351345\n",
      "Epoch number:  4 \tLoss: 2.298400171078889\n",
      "Epoch number:  6 \tLoss: 2.2984002434661566\n",
      "Epoch number:  8 \tLoss: 2.2984003161317488\n",
      "Epoch number:  10 \tLoss: 2.2984003890924174\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.298956299817202\n",
      "Epoch number:  4 \tLoss: 2.2989558201662383\n",
      "Epoch number:  6 \tLoss: 2.2989553456038574\n",
      "Epoch number:  8 \tLoss: 2.2989548773425192\n",
      "Epoch number:  10 \tLoss: 2.2989544152464356\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2989612315262593\n",
      "Epoch number:  4 \tLoss: 2.298960752636652\n",
      "Epoch number:  6 \tLoss: 2.2989602770595847\n",
      "Epoch number:  8 \tLoss: 2.2989598059971836\n",
      "Epoch number:  10 \tLoss: 2.2989593394048162\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.29956807417056\n",
      "Epoch number:  4 \tLoss: 2.2995656036816365\n",
      "Epoch number:  6 \tLoss: 2.299563178004243\n",
      "Epoch number:  8 \tLoss: 2.2995607952738113\n",
      "Epoch number:  10 \tLoss: 2.299558453755914\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2995888329664975\n",
      "Epoch number:  4 \tLoss: 2.2995866565047813\n",
      "Epoch number:  6 \tLoss: 2.299584499714454\n",
      "Epoch number:  8 \tLoss: 2.2995823624116167\n",
      "Epoch number:  10 \tLoss: 2.2995802444140834\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299299888553728\n",
      "Epoch number:  4 \tLoss: 2.299306387677429\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2992997040645204\n",
      "Epoch number:  4 \tLoss: 2.2993062506001296\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299460391732236\n",
      "Epoch number:  4 \tLoss: 2.2994599749513576\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299458821665965\n",
      "Epoch number:  4 \tLoss: 2.2994584776715588\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2997974120676066\n",
      "Epoch number:  4 \tLoss: 2.299795435664246\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299808608914849\n",
      "Epoch number:  4 \tLoss: 2.2998070841480316\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299299711521981\n",
      "Epoch number:  4 \tLoss: 2.2993063535673133\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2992994387893826\n",
      "Epoch number:  4 \tLoss: 2.2993058401264337\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299458187957078\n",
      "Epoch number:  4 \tLoss: 2.2994577902088227\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2994571708343434\n",
      "Epoch number:  4 \tLoss: 2.2994568278603222\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2998064160711915\n",
      "Epoch number:  4 \tLoss: 2.2998044122949763\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299799163678406\n",
      "Epoch number:  4 \tLoss: 2.2997976468061334\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299098040399815\n",
      "Epoch number:  4 \tLoss: 2.2991047878459856\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2990992506798964\n",
      "Epoch number:  4 \tLoss: 2.2991058366659987\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2993627572826436\n",
      "Epoch number:  4 \tLoss: 2.299362297772993\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299367109240314\n",
      "Epoch number:  4 \tLoss: 2.2993667115328815\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2997877432661067\n",
      "Epoch number:  4 \tLoss: 2.2997856804874934\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299813163111411\n",
      "Epoch number:  4 \tLoss: 2.2998114779301893\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2992966276630735\n",
      "Epoch number:  4 \tLoss: 2.2993034853991268\n",
      "Epoch number:  6 \tLoss: 2.2993033911392473\n",
      "Epoch number:  8 \tLoss: 2.2993032969434157\n",
      "Epoch number:  10 \tLoss: 2.2993032028420632\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.29929718879786\n",
      "Epoch number:  4 \tLoss: 2.2993041858752767\n",
      "Epoch number:  6 \tLoss: 2.299304101948247\n",
      "Epoch number:  8 \tLoss: 2.299304018041022\n",
      "Epoch number:  10 \tLoss: 2.299303934186115\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299458932812757\n",
      "Epoch number:  4 \tLoss: 2.2994585285398177\n",
      "Epoch number:  6 \tLoss: 2.299458123206259\n",
      "Epoch number:  8 \tLoss: 2.2994577180633646\n",
      "Epoch number:  10 \tLoss: 2.299457313093422\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299458120440874\n",
      "Epoch number:  4 \tLoss: 2.299457772671514\n",
      "Epoch number:  6 \tLoss: 2.2994574240138643\n",
      "Epoch number:  8 \tLoss: 2.299457075771203\n",
      "Epoch number:  10 \tLoss: 2.299456727942985\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2997965342514872\n",
      "Epoch number:  4 \tLoss: 2.2997945558356383\n",
      "Epoch number:  6 \tLoss: 2.2997925798743406\n",
      "Epoch number:  8 \tLoss: 2.2997906057294695\n",
      "Epoch number:  10 \tLoss: 2.299788632755977\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2998039522520863\n",
      "Epoch number:  4 \tLoss: 2.2998024437843707\n",
      "Epoch number:  6 \tLoss: 2.299800938712165\n",
      "Epoch number:  8 \tLoss: 2.29979943703034\n",
      "Epoch number:  10 \tLoss: 2.2997979387337564\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2992996922238578\n",
      "Epoch number:  4 \tLoss: 2.2993063698232787\n",
      "Epoch number:  6 \tLoss: 2.299306273618852\n",
      "Epoch number:  8 \tLoss: 2.2993061774810717\n",
      "Epoch number:  10 \tLoss: 2.299306081438858\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299299525261852\n",
      "Epoch number:  4 \tLoss: 2.299306295091327\n",
      "Epoch number:  6 \tLoss: 2.299306211388857\n",
      "Epoch number:  8 \tLoss: 2.299306127707673\n",
      "Epoch number:  10 \tLoss: 2.299306044078308\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2994576170756247\n",
      "Epoch number:  4 \tLoss: 2.2994572147389643\n",
      "Epoch number:  6 \tLoss: 2.2994568112483886\n",
      "Epoch number:  8 \tLoss: 2.2994564079505766\n",
      "Epoch number:  10 \tLoss: 2.2994560048247585\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2994619844447395\n",
      "Epoch number:  4 \tLoss: 2.2994616354338366\n",
      "Epoch number:  6 \tLoss: 2.2994612856651946\n",
      "Epoch number:  8 \tLoss: 2.2994609363114824\n",
      "Epoch number:  10 \tLoss: 2.2994605873721445\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299802128048462\n",
      "Epoch number:  4 \tLoss: 2.299800111499367\n",
      "Epoch number:  6 \tLoss: 2.2997980958658117\n",
      "Epoch number:  8 \tLoss: 2.2997960803572295\n",
      "Epoch number:  10 \tLoss: 2.2997940641668384\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2998019057035575\n",
      "Epoch number:  4 \tLoss: 2.2998003647146072\n",
      "Epoch number:  6 \tLoss: 2.2997988272310312\n",
      "Epoch number:  8 \tLoss: 2.299797293247657\n",
      "Epoch number:  10 \tLoss: 2.299795762759299\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2991013629597896\n",
      "Epoch number:  4 \tLoss: 2.299107691374853\n",
      "Epoch number:  6 \tLoss: 2.2991075876077165\n",
      "Epoch number:  8 \tLoss: 2.2991074842829997\n",
      "Epoch number:  10 \tLoss: 2.299107381422288\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2991003971810784\n",
      "Epoch number:  4 \tLoss: 2.299106828314253\n",
      "Epoch number:  6 \tLoss: 2.299106745205373\n",
      "Epoch number:  8 \tLoss: 2.2991066622738905\n",
      "Epoch number:  10 \tLoss: 2.2991065795468413\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299362166214165\n",
      "Epoch number:  4 \tLoss: 2.2993617401816295\n",
      "Epoch number:  6 \tLoss: 2.2993613146756693\n",
      "Epoch number:  8 \tLoss: 2.29936089099997\n",
      "Epoch number:  10 \tLoss: 2.2993604691277536\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299363583560632\n",
      "Epoch number:  4 \tLoss: 2.299363199311035\n",
      "Epoch number:  6 \tLoss: 2.2993628146995486\n",
      "Epoch number:  8 \tLoss: 2.2993624311693917\n",
      "Epoch number:  10 \tLoss: 2.2993620487173847\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299798979391799\n",
      "Epoch number:  4 \tLoss: 2.2997968907783415\n",
      "Epoch number:  6 \tLoss: 2.299794812429798\n",
      "Epoch number:  8 \tLoss: 2.299792743738999\n",
      "Epoch number:  10 \tLoss: 2.2997906841078035\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=sgd, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2998146676760207\n",
      "Epoch number:  4 \tLoss: 2.299812962854978\n",
      "Epoch number:  6 \tLoss: 2.299811264368191\n",
      "Epoch number:  8 \tLoss: 2.2998095721933485\n",
      "Epoch number:  10 \tLoss: 2.2998078863081934\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3006287858445047\n",
      "Epoch number:  4 \tLoss: 2.3005667880300966\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3006321300516452\n",
      "Epoch number:  4 \tLoss: 2.300570450130319\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301564019061562\n",
      "Epoch number:  4 \tLoss: 2.3013544184843724\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3015567788842426\n",
      "Epoch number:  4 \tLoss: 2.3013473469946217\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3031100433441893\n",
      "Epoch number:  4 \tLoss: 2.302402783505046\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.303108472207181\n",
      "Epoch number:  4 \tLoss: 2.30240866974281\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300611567746582\n",
      "Epoch number:  4 \tLoss: 2.300550859328807\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300613107858844\n",
      "Epoch number:  4 \tLoss: 2.3005528980885477\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3015651777871735\n",
      "Epoch number:  4 \tLoss: 2.3013511475621664\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301556483666067\n",
      "Epoch number:  4 \tLoss: 2.301346849302805\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3030740991793026\n",
      "Epoch number:  4 \tLoss: 2.3023833214067437\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3031538514091876\n",
      "Epoch number:  4 \tLoss: 2.302454237911597\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.296972211706857\n",
      "Epoch number:  4 \tLoss: 2.296985495232533\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2969618251374606\n",
      "Epoch number:  4 \tLoss: 2.296977572882522\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2995053930378964\n",
      "Epoch number:  4 \tLoss: 2.299402243842572\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2995001348819684\n",
      "Epoch number:  4 \tLoss: 2.2993981515416673\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.302119230522323\n",
      "Epoch number:  4 \tLoss: 2.301638542466232\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3021073806489647\n",
      "Epoch number:  4 \tLoss: 2.3016358465138578\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300628349300481\n",
      "Epoch number:  4 \tLoss: 2.300566628877198\n",
      "Epoch number:  6 \tLoss: 2.300508001552647\n",
      "Epoch number:  8 \tLoss: 2.3004523329246767\n",
      "Epoch number:  10 \tLoss: 2.300399496551704\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3006280560087973\n",
      "Epoch number:  4 \tLoss: 2.300566805597974\n",
      "Epoch number:  6 \tLoss: 2.3005086964163755\n",
      "Epoch number:  8 \tLoss: 2.300453589040149\n",
      "Epoch number:  10 \tLoss: 2.3004013574908573\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3015646981028954\n",
      "Epoch number:  4 \tLoss: 2.30135337744962\n",
      "Epoch number:  6 \tLoss: 2.301162786429714\n",
      "Epoch number:  8 \tLoss: 2.300991838232644\n",
      "Epoch number:  10 \tLoss: 2.300839143048778\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301559276848014\n",
      "Epoch number:  4 \tLoss: 2.3013500798579285\n",
      "Epoch number:  6 \tLoss: 2.3011618278282113\n",
      "Epoch number:  8 \tLoss: 2.300993592531075\n",
      "Epoch number:  10 \tLoss: 2.300844240521187\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3031080092673024\n",
      "Epoch number:  4 \tLoss: 2.3023995240927158\n",
      "Epoch number:  6 \tLoss: 2.301851244102433\n",
      "Epoch number:  8 \tLoss: 2.301411992849968\n",
      "Epoch number:  10 \tLoss: 2.301057802267623\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3031236365688783\n",
      "Epoch number:  4 \tLoss: 2.3024295275342355\n",
      "Epoch number:  6 \tLoss: 2.3018931219051146\n",
      "Epoch number:  8 \tLoss: 2.3014659914104283\n",
      "Epoch number:  10 \tLoss: 2.301129322614208\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3006215189639985\n",
      "Epoch number:  4 \tLoss: 2.300561289821486\n",
      "Epoch number:  6 \tLoss: 2.3005040536271797\n",
      "Epoch number:  8 \tLoss: 2.300449708280689\n",
      "Epoch number:  10 \tLoss: 2.300398157106753\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3006355343923817\n",
      "Epoch number:  4 \tLoss: 2.3005737850511845\n",
      "Epoch number:  6 \tLoss: 2.300515217855521\n",
      "Epoch number:  8 \tLoss: 2.3004596892416127\n",
      "Epoch number:  10 \tLoss: 2.3004070694496828\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3015568503982755\n",
      "Epoch number:  4 \tLoss: 2.301347599253799\n",
      "Epoch number:  6 \tLoss: 2.301159668609395\n",
      "Epoch number:  8 \tLoss: 2.3009920747877497\n",
      "Epoch number:  10 \tLoss: 2.3008435959667035\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301575057288555\n",
      "Epoch number:  4 \tLoss: 2.301365255890269\n",
      "Epoch number:  6 \tLoss: 2.3011763049080898\n",
      "Epoch number:  8 \tLoss: 2.3010072797146717\n",
      "Epoch number:  10 \tLoss: 2.3008570690746732\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3031406882075913\n",
      "Epoch number:  4 \tLoss: 2.302440804797458\n",
      "Epoch number:  6 \tLoss: 2.3018977325922854\n",
      "Epoch number:  8 \tLoss: 2.3014611157455747\n",
      "Epoch number:  10 \tLoss: 2.301109149670733\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3031493805022816\n",
      "Epoch number:  4 \tLoss: 2.3024466263129493\n",
      "Epoch number:  6 \tLoss: 2.3019054118584963\n",
      "Epoch number:  8 \tLoss: 2.301475214841761\n",
      "Epoch number:  10 \tLoss: 2.301136448512484\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2969649990488477\n",
      "Epoch number:  4 \tLoss: 2.296979663419499\n",
      "Epoch number:  6 \tLoss: 2.2969916929091267\n",
      "Epoch number:  8 \tLoss: 2.2970016044784405\n",
      "Epoch number:  10 \tLoss: 2.2970098416570557\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.296980245296982\n",
      "Epoch number:  4 \tLoss: 2.2969918740453528\n",
      "Epoch number:  6 \tLoss: 2.2970014353418713\n",
      "Epoch number:  8 \tLoss: 2.297009358849251\n",
      "Epoch number:  10 \tLoss: 2.2970160032858535\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299503838587003\n",
      "Epoch number:  4 \tLoss: 2.2994003845212356\n",
      "Epoch number:  6 \tLoss: 2.2993282411575318\n",
      "Epoch number:  8 \tLoss: 2.2992776073161187\n",
      "Epoch number:  10 \tLoss: 2.299241891993445\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2995087002629417\n",
      "Epoch number:  4 \tLoss: 2.2994040579030406\n",
      "Epoch number:  6 \tLoss: 2.299330907918083\n",
      "Epoch number:  8 \tLoss: 2.2992795168292366\n",
      "Epoch number:  10 \tLoss: 2.299243256115776\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.302091081384943\n",
      "Epoch number:  4 \tLoss: 2.3016198698039885\n",
      "Epoch number:  6 \tLoss: 2.301328877102293\n",
      "Epoch number:  8 \tLoss: 2.3011423372445057\n",
      "Epoch number:  10 \tLoss: 2.3010200295428653\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3021231051812405\n",
      "Epoch number:  4 \tLoss: 2.3016427442542033\n",
      "Epoch number:  6 \tLoss: 2.301345687665436\n",
      "Epoch number:  8 \tLoss: 2.301155087349554\n",
      "Epoch number:  10 \tLoss: 2.301030017852798\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3005552159140588\n",
      "Epoch number:  4 \tLoss: 2.300502089447667\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3005460646574245\n",
      "Epoch number:  4 \tLoss: 2.3004936530707103\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3013317104540296\n",
      "Epoch number:  4 \tLoss: 2.301156400196712\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3013374404571643\n",
      "Epoch number:  4 \tLoss: 2.301163282082344\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3025171815222363\n",
      "Epoch number:  4 \tLoss: 2.302097368155861\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3024840550981422\n",
      "Epoch number:  4 \tLoss: 2.3020621981004368\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3005370896415407\n",
      "Epoch number:  4 \tLoss: 2.3004850367074736\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3005432840281155\n",
      "Epoch number:  4 \tLoss: 2.30049151462487\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3013292475386575\n",
      "Epoch number:  4 \tLoss: 2.301156634445762\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3013341361354875\n",
      "Epoch number:  4 \tLoss: 2.3011581182884773\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.302475126147194\n",
      "Epoch number:  4 \tLoss: 2.3020467695626583\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3024944958685176\n",
      "Epoch number:  4 \tLoss: 2.3020753389821262\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299647177298358\n",
      "Epoch number:  4 \tLoss: 2.299615047226981\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2996298542647526\n",
      "Epoch number:  4 \tLoss: 2.2995996626684585\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300825208496717\n",
      "Epoch number:  4 \tLoss: 2.300684854368781\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3008100589516745\n",
      "Epoch number:  4 \tLoss: 2.3006738535372437\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3022684607417943\n",
      "Epoch number:  4 \tLoss: 2.3019138266417767\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.302263991771747\n",
      "Epoch number:  4 \tLoss: 2.301910539459356\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300553380114275\n",
      "Epoch number:  4 \tLoss: 2.3005006005695283\n",
      "Epoch number:  6 \tLoss: 2.3004498033706584\n",
      "Epoch number:  8 \tLoss: 2.3004009112867183\n",
      "Epoch number:  10 \tLoss: 2.3003538646925685\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3005389954404\n",
      "Epoch number:  4 \tLoss: 2.30048757889556\n",
      "Epoch number:  6 \tLoss: 2.3004381458431915\n",
      "Epoch number:  8 \tLoss: 2.300390646579591\n",
      "Epoch number:  10 \tLoss: 2.3003450489029924\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301318693920353\n",
      "Epoch number:  4 \tLoss: 2.301147177484382\n",
      "Epoch number:  6 \tLoss: 2.3009923571834237\n",
      "Epoch number:  8 \tLoss: 2.3008535667227488\n",
      "Epoch number:  10 \tLoss: 2.3007295343914524\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3013314348939033\n",
      "Epoch number:  4 \tLoss: 2.30115941631419\n",
      "Epoch number:  6 \tLoss: 2.301003981213974\n",
      "Epoch number:  8 \tLoss: 2.3008646350273416\n",
      "Epoch number:  10 \tLoss: 2.3007403176614303\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.302463625731824\n",
      "Epoch number:  4 \tLoss: 2.302033919787828\n",
      "Epoch number:  6 \tLoss: 2.3016157723996673\n",
      "Epoch number:  8 \tLoss: 2.3012395818736144\n",
      "Epoch number:  10 \tLoss: 2.3009085446435265\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.30248872615721\n",
      "Epoch number:  4 \tLoss: 2.3020673255892374\n",
      "Epoch number:  6 \tLoss: 2.3016603189064773\n",
      "Epoch number:  8 \tLoss: 2.3013048584006683\n",
      "Epoch number:  10 \tLoss: 2.3010141464882987\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3005510348213436\n",
      "Epoch number:  4 \tLoss: 2.3004986298018237\n",
      "Epoch number:  6 \tLoss: 2.300448278274267\n",
      "Epoch number:  8 \tLoss: 2.3003999073148607\n",
      "Epoch number:  10 \tLoss: 2.300353463928422\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300547424440036\n",
      "Epoch number:  4 \tLoss: 2.3004956625911333\n",
      "Epoch number:  6 \tLoss: 2.300445903877\n",
      "Epoch number:  8 \tLoss: 2.30039809330065\n",
      "Epoch number:  10 \tLoss: 2.300352194524062\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3013199003246783\n",
      "Epoch number:  4 \tLoss: 2.301147197938828\n",
      "Epoch number:  6 \tLoss: 2.3009914380979075\n",
      "Epoch number:  8 \tLoss: 2.3008520135125754\n",
      "Epoch number:  10 \tLoss: 2.300727636345596\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3013049591192694\n",
      "Epoch number:  4 \tLoss: 2.30113180045235\n",
      "Epoch number:  6 \tLoss: 2.300976014520835\n",
      "Epoch number:  8 \tLoss: 2.300836972137997\n",
      "Epoch number:  10 \tLoss: 2.3007134612729394\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.302494254903189\n",
      "Epoch number:  4 \tLoss: 2.302068221561989\n",
      "Epoch number:  6 \tLoss: 2.301652215854904\n",
      "Epoch number:  8 \tLoss: 2.301283222218196\n",
      "Epoch number:  10 \tLoss: 2.300972829249989\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3024943111330414\n",
      "Epoch number:  4 \tLoss: 2.3020739628283993\n",
      "Epoch number:  6 \tLoss: 2.301667090274893\n",
      "Epoch number:  8 \tLoss: 2.3013112426590525\n",
      "Epoch number:  10 \tLoss: 2.3010200114248325\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2996363388536065\n",
      "Epoch number:  4 \tLoss: 2.29960508115426\n",
      "Epoch number:  6 \tLoss: 2.2995773629327982\n",
      "Epoch number:  8 \tLoss: 2.299552753413743\n",
      "Epoch number:  10 \tLoss: 2.2995308764921605\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2996354801860392\n",
      "Epoch number:  4 \tLoss: 2.299604531196708\n",
      "Epoch number:  6 \tLoss: 2.29957701842757\n",
      "Epoch number:  8 \tLoss: 2.299552539037704\n",
      "Epoch number:  10 \tLoss: 2.2995307379720136\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300828920644061\n",
      "Epoch number:  4 \tLoss: 2.3006881298299278\n",
      "Epoch number:  6 \tLoss: 2.300569354545089\n",
      "Epoch number:  8 \tLoss: 2.300469603785369\n",
      "Epoch number:  10 \tLoss: 2.3003859854209767\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300821024397698\n",
      "Epoch number:  4 \tLoss: 2.300682122751003\n",
      "Epoch number:  6 \tLoss: 2.3005649659864456\n",
      "Epoch number:  8 \tLoss: 2.300466502146543\n",
      "Epoch number:  10 \tLoss: 2.300383847715242\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3022483882653897\n",
      "Epoch number:  4 \tLoss: 2.3018901670938146\n",
      "Epoch number:  6 \tLoss: 2.3015689618332367\n",
      "Epoch number:  8 \tLoss: 2.301300571200682\n",
      "Epoch number:  10 \tLoss: 2.3010835019038915\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.30227076186144\n",
      "Epoch number:  4 \tLoss: 2.3019155448909423\n",
      "Epoch number:  6 \tLoss: 2.301596153793533\n",
      "Epoch number:  8 \tLoss: 2.301329791057924\n",
      "Epoch number:  10 \tLoss: 2.301116718837657\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300532138129347\n",
      "Epoch number:  4 \tLoss: 2.300476594816597\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3005362216438687\n",
      "Epoch number:  4 \tLoss: 2.3004797999104696\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301082686963311\n",
      "Epoch number:  4 \tLoss: 2.300950193355526\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.30107435466026\n",
      "Epoch number:  4 \tLoss: 2.300945602983387\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3023816648128794\n",
      "Epoch number:  4 \tLoss: 2.301781901518257\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3023777629715063\n",
      "Epoch number:  4 \tLoss: 2.301792058706217\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3005265857208963\n",
      "Epoch number:  4 \tLoss: 2.3004706968162982\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3005347729849497\n",
      "Epoch number:  4 \tLoss: 2.300478770265778\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301076268969432\n",
      "Epoch number:  4 \tLoss: 2.3009482663506966\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301066174157687\n",
      "Epoch number:  4 \tLoss: 2.30094056554119\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.302399884877923\n",
      "Epoch number:  4 \tLoss: 2.3018218413732985\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.302379704371625\n",
      "Epoch number:  4 \tLoss: 2.3017990038025986\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3003313545614237\n",
      "Epoch number:  4 \tLoss: 2.3002855967346902\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300332418093843\n",
      "Epoch number:  4 \tLoss: 2.300286710782907\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3009814832032305\n",
      "Epoch number:  4 \tLoss: 2.3008628001575855\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3009835937931316\n",
      "Epoch number:  4 \tLoss: 2.3008668530279524\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3023693693872493\n",
      "Epoch number:  4 \tLoss: 2.3018193421649\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.302370450772073\n",
      "Epoch number:  4 \tLoss: 2.3018206437304425\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300536765612202\n",
      "Epoch number:  4 \tLoss: 2.3004806197390404\n",
      "Epoch number:  6 \tLoss: 2.3004260897668365\n",
      "Epoch number:  8 \tLoss: 2.3003733866267733\n",
      "Epoch number:  10 \tLoss: 2.300322681659642\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300543151525865\n",
      "Epoch number:  4 \tLoss: 2.300487143647242\n",
      "Epoch number:  6 \tLoss: 2.300432724366088\n",
      "Epoch number:  8 \tLoss: 2.3003801331422498\n",
      "Epoch number:  10 \tLoss: 2.3003295678520583\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301073140556719\n",
      "Epoch number:  4 \tLoss: 2.3009452098082175\n",
      "Epoch number:  6 \tLoss: 2.3008304588339565\n",
      "Epoch number:  8 \tLoss: 2.300724581541767\n",
      "Epoch number:  10 \tLoss: 2.3006248923086203\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010913987181274\n",
      "Epoch number:  4 \tLoss: 2.30096309929018\n",
      "Epoch number:  6 \tLoss: 2.300848835898038\n",
      "Epoch number:  8 \tLoss: 2.3007442077916456\n",
      "Epoch number:  10 \tLoss: 2.300646615779022\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.302388609075707\n",
      "Epoch number:  4 \tLoss: 2.301778763896184\n",
      "Epoch number:  6 \tLoss: 2.3012525489146074\n",
      "Epoch number:  8 \tLoss: 2.300839929117067\n",
      "Epoch number:  10 \tLoss: 2.300514179951012\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3024099805389353\n",
      "Epoch number:  4 \tLoss: 2.3018194171397246\n",
      "Epoch number:  6 \tLoss: 2.3013239528976492\n",
      "Epoch number:  8 \tLoss: 2.300957130591661\n",
      "Epoch number:  10 \tLoss: 2.3006990118551087\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300533996383642\n",
      "Epoch number:  4 \tLoss: 2.3004784989245377\n",
      "Epoch number:  6 \tLoss: 2.300424624543675\n",
      "Epoch number:  8 \tLoss: 2.3003726014844044\n",
      "Epoch number:  10 \tLoss: 2.3003226164964063\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3005356117460827\n",
      "Epoch number:  4 \tLoss: 2.3004793457033292\n",
      "Epoch number:  6 \tLoss: 2.3004247350172435\n",
      "Epoch number:  8 \tLoss: 2.3003720177336024\n",
      "Epoch number:  10 \tLoss: 2.3003213883932143\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301064725735567\n",
      "Epoch number:  4 \tLoss: 2.3009362728762897\n",
      "Epoch number:  6 \tLoss: 2.3008214958994153\n",
      "Epoch number:  8 \tLoss: 2.30071584644659\n",
      "Epoch number:  10 \tLoss: 2.3006165220550807\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301069310904024\n",
      "Epoch number:  4 \tLoss: 2.300941040085505\n",
      "Epoch number:  6 \tLoss: 2.300826575388522\n",
      "Epoch number:  8 \tLoss: 2.3007216561515165\n",
      "Epoch number:  10 \tLoss: 2.300623827532511\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3023842666452126\n",
      "Epoch number:  4 \tLoss: 2.3017893311206223\n",
      "Epoch number:  6 \tLoss: 2.3012871088211675\n",
      "Epoch number:  8 \tLoss: 2.3009057227156786\n",
      "Epoch number:  10 \tLoss: 2.300620256542162\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3024082873701026\n",
      "Epoch number:  4 \tLoss: 2.3018235680905024\n",
      "Epoch number:  6 \tLoss: 2.301332248187047\n",
      "Epoch number:  8 \tLoss: 2.3009677237314374\n",
      "Epoch number:  10 \tLoss: 2.300710566529009\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3003389550657487\n",
      "Epoch number:  4 \tLoss: 2.3002936532751135\n",
      "Epoch number:  6 \tLoss: 2.300250669519501\n",
      "Epoch number:  8 \tLoss: 2.3002100210603578\n",
      "Epoch number:  10 \tLoss: 2.3001716962519803\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3003363080531827\n",
      "Epoch number:  4 \tLoss: 2.3002901326907823\n",
      "Epoch number:  6 \tLoss: 2.3002463737099945\n",
      "Epoch number:  8 \tLoss: 2.3002050549896964\n",
      "Epoch number:  10 \tLoss: 2.300166167510287\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3009757271070237\n",
      "Epoch number:  4 \tLoss: 2.3008594502363695\n",
      "Epoch number:  6 \tLoss: 2.300757745274283\n",
      "Epoch number:  8 \tLoss: 2.3006666604316828\n",
      "Epoch number:  10 \tLoss: 2.3005835721484127\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3009918812059986\n",
      "Epoch number:  4 \tLoss: 2.300874325705353\n",
      "Epoch number:  6 \tLoss: 2.3007723016818966\n",
      "Epoch number:  8 \tLoss: 2.3006816968280486\n",
      "Epoch number:  10 \tLoss: 2.30059986042374\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.302400844108463\n",
      "Epoch number:  4 \tLoss: 2.301839612350738\n",
      "Epoch number:  6 \tLoss: 2.3013674266751982\n",
      "Epoch number:  8 \tLoss: 2.301008870163489\n",
      "Epoch number:  10 \tLoss: 2.300744012713678\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=momentum, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.302440955182749\n",
      "Epoch number:  4 \tLoss: 2.3018858834962104\n",
      "Epoch number:  6 \tLoss: 2.301419425057785\n",
      "Epoch number:  8 \tLoss: 2.30106996227644\n",
      "Epoch number:  10 \tLoss: 2.300821955262388\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3006224451169333\n",
      "Epoch number:  4 \tLoss: 2.3005610333088433\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3006371610870535\n",
      "Epoch number:  4 \tLoss: 2.3005757362089287\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301580943123121\n",
      "Epoch number:  4 \tLoss: 2.301370674393752\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301590403128913\n",
      "Epoch number:  4 \tLoss: 2.301380121616109\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.303249548582948\n",
      "Epoch number:  4 \tLoss: 2.302523144149988\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.303205497138972\n",
      "Epoch number:  4 \tLoss: 2.302496849513957\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300631933908667\n",
      "Epoch number:  4 \tLoss: 2.3005707769960004\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3006264381526775\n",
      "Epoch number:  4 \tLoss: 2.300565337292673\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3015759906912874\n",
      "Epoch number:  4 \tLoss: 2.301366759892601\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3015732167464873\n",
      "Epoch number:  4 \tLoss: 2.3013644784623932\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3031786587806216\n",
      "Epoch number:  4 \tLoss: 2.302464630451211\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.303211988644044\n",
      "Epoch number:  4 \tLoss: 2.3024956362193176\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.296975756171464\n",
      "Epoch number:  4 \tLoss: 2.2969877735482402\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.296973536991738\n",
      "Epoch number:  4 \tLoss: 2.2969860274943223\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2995330700972043\n",
      "Epoch number:  4 \tLoss: 2.299424844593303\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299519549627692\n",
      "Epoch number:  4 \tLoss: 2.2994152587583923\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3022203040040665\n",
      "Epoch number:  4 \tLoss: 2.3017244223804316\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.302246562707207\n",
      "Epoch number:  4 \tLoss: 2.3017424701105273\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300629284616836\n",
      "Epoch number:  4 \tLoss: 2.300568093897625\n",
      "Epoch number:  6 \tLoss: 2.300510164264807\n",
      "Epoch number:  8 \tLoss: 2.300455285980045\n",
      "Epoch number:  10 \tLoss: 2.3004032716183054\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300643887821045\n",
      "Epoch number:  4 \tLoss: 2.30058181825179\n",
      "Epoch number:  6 \tLoss: 2.300522893533445\n",
      "Epoch number:  8 \tLoss: 2.3004669780756664\n",
      "Epoch number:  10 \tLoss: 2.3004139487279707\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3016060596237375\n",
      "Epoch number:  4 \tLoss: 2.30139197010502\n",
      "Epoch number:  6 \tLoss: 2.301198666223878\n",
      "Epoch number:  8 \tLoss: 2.3010251107241473\n",
      "Epoch number:  10 \tLoss: 2.300870107883271\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301572512576206\n",
      "Epoch number:  4 \tLoss: 2.301365179715249\n",
      "Epoch number:  6 \tLoss: 2.3011780434672855\n",
      "Epoch number:  8 \tLoss: 2.3010101359710347\n",
      "Epoch number:  10 \tLoss: 2.3008603838581148\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.303165848724881\n",
      "Epoch number:  4 \tLoss: 2.3024438907018117\n",
      "Epoch number:  6 \tLoss: 2.3018788434546154\n",
      "Epoch number:  8 \tLoss: 2.301419283352153\n",
      "Epoch number:  10 \tLoss: 2.3010370464755736\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.303242930852317\n",
      "Epoch number:  4 \tLoss: 2.3025200423691357\n",
      "Epoch number:  6 \tLoss: 2.301961808225067\n",
      "Epoch number:  8 \tLoss: 2.301518532152785\n",
      "Epoch number:  10 \tLoss: 2.3011687889848047\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3006290376929566\n",
      "Epoch number:  4 \tLoss: 2.300566771334644\n",
      "Epoch number:  6 \tLoss: 2.300507889695318\n",
      "Epoch number:  8 \tLoss: 2.300452191687924\n",
      "Epoch number:  10 \tLoss: 2.300399499100619\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300626200062856\n",
      "Epoch number:  4 \tLoss: 2.300565492484755\n",
      "Epoch number:  6 \tLoss: 2.3005078346492174\n",
      "Epoch number:  8 \tLoss: 2.3004530945666257\n",
      "Epoch number:  10 \tLoss: 2.3004011530099246\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.30159489507818\n",
      "Epoch number:  4 \tLoss: 2.301383145120089\n",
      "Epoch number:  6 \tLoss: 2.3011919026108694\n",
      "Epoch number:  8 \tLoss: 2.301019854626793\n",
      "Epoch number:  10 \tLoss: 2.3008656389442863\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301605588726218\n",
      "Epoch number:  4 \tLoss: 2.3013935935570298\n",
      "Epoch number:  6 \tLoss: 2.301202464685091\n",
      "Epoch number:  8 \tLoss: 2.301031189482373\n",
      "Epoch number:  10 \tLoss: 2.3008786458069\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3032111919513705\n",
      "Epoch number:  4 \tLoss: 2.302486389016433\n",
      "Epoch number:  6 \tLoss: 2.301924440316745\n",
      "Epoch number:  8 \tLoss: 2.3014726433685126\n",
      "Epoch number:  10 \tLoss: 2.3011034887779855\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.303225049838384\n",
      "Epoch number:  4 \tLoss: 2.3025113782221776\n",
      "Epoch number:  6 \tLoss: 2.301958158336216\n",
      "Epoch number:  8 \tLoss: 2.301517536549365\n",
      "Epoch number:  10 \tLoss: 2.3011688406385327\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2969802836165707\n",
      "Epoch number:  4 \tLoss: 2.2969913526456027\n",
      "Epoch number:  6 \tLoss: 2.2970006204981765\n",
      "Epoch number:  8 \tLoss: 2.2970084219706948\n",
      "Epoch number:  10 \tLoss: 2.297015051565765\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.29697188694825\n",
      "Epoch number:  4 \tLoss: 2.296985246236047\n",
      "Epoch number:  6 \tLoss: 2.2969962182540264\n",
      "Epoch number:  8 \tLoss: 2.2970052778236076\n",
      "Epoch number:  10 \tLoss: 2.297012829976176\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299511231063969\n",
      "Epoch number:  4 \tLoss: 2.2994097019937043\n",
      "Epoch number:  6 \tLoss: 2.2993383403300065\n",
      "Epoch number:  8 \tLoss: 2.2992879271741744\n",
      "Epoch number:  10 \tLoss: 2.299252175479665\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2995168376288144\n",
      "Epoch number:  4 \tLoss: 2.2994133299847976\n",
      "Epoch number:  6 \tLoss: 2.2993406760921573\n",
      "Epoch number:  8 \tLoss: 2.2992894491244944\n",
      "Epoch number:  10 \tLoss: 2.2992531875684614\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3022381490190793\n",
      "Epoch number:  4 \tLoss: 2.3017354344858307\n",
      "Epoch number:  6 \tLoss: 2.301421912573956\n",
      "Epoch number:  8 \tLoss: 2.3012194772878587\n",
      "Epoch number:  10 \tLoss: 2.3010858510994914\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=16, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.302238032046074\n",
      "Epoch number:  4 \tLoss: 2.301738325033748\n",
      "Epoch number:  6 \tLoss: 2.3014253847847\n",
      "Epoch number:  8 \tLoss: 2.301222892758409\n",
      "Epoch number:  10 \tLoss: 2.3010890776114086\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3005361031690947\n",
      "Epoch number:  4 \tLoss: 2.3004850178182865\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3005312250706487\n",
      "Epoch number:  4 \tLoss: 2.3004796673806256\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3013023801010153\n",
      "Epoch number:  4 \tLoss: 2.3011359148566726\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3012897423682994\n",
      "Epoch number:  4 \tLoss: 2.301123016659464\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.302485288135303\n",
      "Epoch number:  4 \tLoss: 2.3020688562993206\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3024736901035494\n",
      "Epoch number:  4 \tLoss: 2.302061808022595\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3005358125168165\n",
      "Epoch number:  4 \tLoss: 2.3004840832395757\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3005280613518244\n",
      "Epoch number:  4 \tLoss: 2.300477182401485\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301317758445528\n",
      "Epoch number:  4 \tLoss: 2.3011500751683465\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3013159243052588\n",
      "Epoch number:  4 \tLoss: 2.3011476935913993\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3024967097110385\n",
      "Epoch number:  4 \tLoss: 2.3020794414953683\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.302496329080205\n",
      "Epoch number:  4 \tLoss: 2.3020890439442994\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2996294795957253\n",
      "Epoch number:  4 \tLoss: 2.2995973814119735\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2996341320406017\n",
      "Epoch number:  4 \tLoss: 2.299601443941885\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300813140377996\n",
      "Epoch number:  4 \tLoss: 2.3006769493720762\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3008254294290347\n",
      "Epoch number:  4 \tLoss: 2.300688659650215\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3022483815871504\n",
      "Epoch number:  4 \tLoss: 2.301895667602668\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=32, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3022517626343952\n",
      "Epoch number:  4 \tLoss: 2.3018994474986556\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300540783629877\n",
      "Epoch number:  4 \tLoss: 2.3004891418270423\n",
      "Epoch number:  6 \tLoss: 2.3004396205515087\n",
      "Epoch number:  8 \tLoss: 2.3003921486745753\n",
      "Epoch number:  10 \tLoss: 2.300346672855194\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3005389158657192\n",
      "Epoch number:  4 \tLoss: 2.3004874363849135\n",
      "Epoch number:  6 \tLoss: 2.3004378983688785\n",
      "Epoch number:  8 \tLoss: 2.3003902535796885\n",
      "Epoch number:  10 \tLoss: 2.3003444692329102\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301307230838457\n",
      "Epoch number:  4 \tLoss: 2.301140127482385\n",
      "Epoch number:  6 \tLoss: 2.300987372934276\n",
      "Epoch number:  8 \tLoss: 2.300848489056094\n",
      "Epoch number:  10 \tLoss: 2.3007224538432136\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3013222482513287\n",
      "Epoch number:  4 \tLoss: 2.301155279907603\n",
      "Epoch number:  6 \tLoss: 2.3010030121286644\n",
      "Epoch number:  8 \tLoss: 2.30086524470415\n",
      "Epoch number:  10 \tLoss: 2.3007412864204904\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.302510649454801\n",
      "Epoch number:  4 \tLoss: 2.3020945481692183\n",
      "Epoch number:  6 \tLoss: 2.301683301491084\n",
      "Epoch number:  8 \tLoss: 2.3013038660781064\n",
      "Epoch number:  10 \tLoss: 2.3009662610599344\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3024936500947435\n",
      "Epoch number:  4 \tLoss: 2.302084266569409\n",
      "Epoch number:  6 \tLoss: 2.3016886165696087\n",
      "Epoch number:  8 \tLoss: 2.3013362979650047\n",
      "Epoch number:  10 \tLoss: 2.3010414420806278\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300532020569827\n",
      "Epoch number:  4 \tLoss: 2.300481026733067\n",
      "Epoch number:  6 \tLoss: 2.300432030347412\n",
      "Epoch number:  8 \tLoss: 2.300384946566237\n",
      "Epoch number:  10 \tLoss: 2.300339709096306\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300539021963452\n",
      "Epoch number:  4 \tLoss: 2.3004876825970633\n",
      "Epoch number:  6 \tLoss: 2.3004382820637836\n",
      "Epoch number:  8 \tLoss: 2.3003907712174265\n",
      "Epoch number:  10 \tLoss: 2.3003451166054636\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3012955911228805\n",
      "Epoch number:  4 \tLoss: 2.30112899712168\n",
      "Epoch number:  6 \tLoss: 2.300977417777188\n",
      "Epoch number:  8 \tLoss: 2.3008405684315245\n",
      "Epoch number:  10 \tLoss: 2.3007175924571124\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301315271346347\n",
      "Epoch number:  4 \tLoss: 2.301147849058999\n",
      "Epoch number:  6 \tLoss: 2.3009953671325025\n",
      "Epoch number:  8 \tLoss: 2.3008575917807152\n",
      "Epoch number:  10 \tLoss: 2.3007337927296265\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.302485937566419\n",
      "Epoch number:  4 \tLoss: 2.302076213359167\n",
      "Epoch number:  6 \tLoss: 2.301679410133242\n",
      "Epoch number:  8 \tLoss: 2.3013225966474264\n",
      "Epoch number:  10 \tLoss: 2.3010147554321296\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3025001446557254\n",
      "Epoch number:  4 \tLoss: 2.302094686093067\n",
      "Epoch number:  6 \tLoss: 2.301701853018359\n",
      "Epoch number:  8 \tLoss: 2.301350868676102\n",
      "Epoch number:  10 \tLoss: 2.301056057355121\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2996290338136114\n",
      "Epoch number:  4 \tLoss: 2.2995964880598576\n",
      "Epoch number:  6 \tLoss: 2.2995676386554087\n",
      "Epoch number:  8 \tLoss: 2.29954202931692\n",
      "Epoch number:  10 \tLoss: 2.2995192641440587\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2996266412180373\n",
      "Epoch number:  4 \tLoss: 2.2995947955128058\n",
      "Epoch number:  6 \tLoss: 2.2995664483625444\n",
      "Epoch number:  8 \tLoss: 2.2995411968716084\n",
      "Epoch number:  10 \tLoss: 2.299518684393168\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3008254937207817\n",
      "Epoch number:  4 \tLoss: 2.300688063219233\n",
      "Epoch number:  6 \tLoss: 2.300571222064044\n",
      "Epoch number:  8 \tLoss: 2.30047236374071\n",
      "Epoch number:  10 \tLoss: 2.3003889224495957\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300825280364882\n",
      "Epoch number:  4 \tLoss: 2.3006870082240276\n",
      "Epoch number:  6 \tLoss: 2.300569310035987\n",
      "Epoch number:  8 \tLoss: 2.3004695255819003\n",
      "Epoch number:  10 \tLoss: 2.3003850875364225\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3022684391997053\n",
      "Epoch number:  4 \tLoss: 2.301918709675745\n",
      "Epoch number:  6 \tLoss: 2.301607740136683\n",
      "Epoch number:  8 \tLoss: 2.3013469339272006\n",
      "Epoch number:  10 \tLoss: 2.30113559555324\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=32, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3022777027867196\n",
      "Epoch number:  4 \tLoss: 2.301922723534372\n",
      "Epoch number:  6 \tLoss: 2.301605243247381\n",
      "Epoch number:  8 \tLoss: 2.301337684420188\n",
      "Epoch number:  10 \tLoss: 2.3011207984800977\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3005265286346432\n",
      "Epoch number:  4 \tLoss: 2.3004736224769142\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3005032025828824\n",
      "Epoch number:  4 \tLoss: 2.30045077306306\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301078522416785\n",
      "Epoch number:  4 \tLoss: 2.300951575730887\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.30108902747618\n",
      "Epoch number:  4 \tLoss: 2.3009612213306028\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3023047714663614\n",
      "Epoch number:  4 \tLoss: 2.3018433835805174\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3023324747286367\n",
      "Epoch number:  4 \tLoss: 2.3018821779173613\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300500129902255\n",
      "Epoch number:  4 \tLoss: 2.3004479698375793\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3005059075700043\n",
      "Epoch number:  4 \tLoss: 2.300453967597884\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010804678305385\n",
      "Epoch number:  4 \tLoss: 2.3009502337572307\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3010694211703906\n",
      "Epoch number:  4 \tLoss: 2.3009423531234248\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3023017414340816\n",
      "Epoch number:  4 \tLoss: 2.3018438642964796\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3023213795353916\n",
      "Epoch number:  4 \tLoss: 2.3018681984878517\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300311136935915\n",
      "Epoch number:  4 \tLoss: 2.3002674624035997\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300315128663283\n",
      "Epoch number:  4 \tLoss: 2.300271347507557\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300999078479002\n",
      "Epoch number:  4 \tLoss: 2.3008800739618263\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3009911833769174\n",
      "Epoch number:  4 \tLoss: 2.3008746219000695\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.302329600005693\n",
      "Epoch number:  4 \tLoss: 2.301892472251793\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=64, epochs=5, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3023276814615876\n",
      "Epoch number:  4 \tLoss: 2.301898991333404\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3005076926489703\n",
      "Epoch number:  4 \tLoss: 2.3004551597881995\n",
      "Epoch number:  6 \tLoss: 2.300403934612987\n",
      "Epoch number:  8 \tLoss: 2.3003541799029263\n",
      "Epoch number:  10 \tLoss: 2.3003060399040347\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3005172423923126\n",
      "Epoch number:  4 \tLoss: 2.3004644777822216\n",
      "Epoch number:  6 \tLoss: 2.3004130251342496\n",
      "Epoch number:  8 \tLoss: 2.3003630826268986\n",
      "Epoch number:  10 \tLoss: 2.3003148255338868\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301081559075402\n",
      "Epoch number:  4 \tLoss: 2.3009544035901293\n",
      "Epoch number:  6 \tLoss: 2.3008399441392577\n",
      "Epoch number:  8 \tLoss: 2.300735178125283\n",
      "Epoch number:  10 \tLoss: 2.3006378666409093\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301088989810509\n",
      "Epoch number:  4 \tLoss: 2.3009598748133833\n",
      "Epoch number:  6 \tLoss: 2.3008435805608456\n",
      "Epoch number:  8 \tLoss: 2.300737181464954\n",
      "Epoch number:  10 \tLoss: 2.3006385751463623\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.302346066692964\n",
      "Epoch number:  4 \tLoss: 2.3018823253379046\n",
      "Epoch number:  6 \tLoss: 2.301440989010534\n",
      "Epoch number:  8 \tLoss: 2.3010615711157687\n",
      "Epoch number:  10 \tLoss: 2.300750327509118\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.302346645475121\n",
      "Epoch number:  4 \tLoss: 2.301894399248881\n",
      "Epoch number:  6 \tLoss: 2.30146928626519\n",
      "Epoch number:  8 \tLoss: 2.3011122539115454\n",
      "Epoch number:  10 \tLoss: 2.3008335312905146\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3005075299616125\n",
      "Epoch number:  4 \tLoss: 2.3004558162808078\n",
      "Epoch number:  6 \tLoss: 2.3004053697306124\n",
      "Epoch number:  8 \tLoss: 2.3003563369436466\n",
      "Epoch number:  10 \tLoss: 2.3003088500051887\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300510513154823\n",
      "Epoch number:  4 \tLoss: 2.300458285993461\n",
      "Epoch number:  6 \tLoss: 2.3004073677600845\n",
      "Epoch number:  8 \tLoss: 2.300357951117135\n",
      "Epoch number:  10 \tLoss: 2.3003102059612934\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301081323554953\n",
      "Epoch number:  4 \tLoss: 2.3009545333182038\n",
      "Epoch number:  6 \tLoss: 2.300840563599916\n",
      "Epoch number:  8 \tLoss: 2.3007364339110428\n",
      "Epoch number:  10 \tLoss: 2.300639973607599\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.301075058068387\n",
      "Epoch number:  4 \tLoss: 2.3009480721922864\n",
      "Epoch number:  6 \tLoss: 2.3008336546755404\n",
      "Epoch number:  8 \tLoss: 2.3007288993168924\n",
      "Epoch number:  10 \tLoss: 2.300631757854583\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.302278456992701\n",
      "Epoch number:  4 \tLoss: 2.3018130552924343\n",
      "Epoch number:  6 \tLoss: 2.301377757599252\n",
      "Epoch number:  8 \tLoss: 2.3010092848144104\n",
      "Epoch number:  10 \tLoss: 2.3007117190168453\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3023176615286562\n",
      "Epoch number:  4 \tLoss: 2.3018627038238297\n",
      "Epoch number:  6 \tLoss: 2.301437964449694\n",
      "Epoch number:  8 \tLoss: 2.3010830245673213\n",
      "Epoch number:  10 \tLoss: 2.3008066665845033\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3003105880352193\n",
      "Epoch number:  4 \tLoss: 2.300267028410257\n",
      "Epoch number:  6 \tLoss: 2.300225626259064\n",
      "Epoch number:  8 \tLoss: 2.30018638842625\n",
      "Epoch number:  10 \tLoss: 2.3001493015921226\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3003138450769285\n",
      "Epoch number:  4 \tLoss: 2.3002701761913715\n",
      "Epoch number:  6 \tLoss: 2.3002285976822736\n",
      "Epoch number:  8 \tLoss: 2.3001891341940897\n",
      "Epoch number:  10 \tLoss: 2.300151788348821\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3009911033654284\n",
      "Epoch number:  4 \tLoss: 2.3008729490865965\n",
      "Epoch number:  6 \tLoss: 2.300768837872152\n",
      "Epoch number:  8 \tLoss: 2.300675806093475\n",
      "Epoch number:  10 \tLoss: 2.3005916065151153\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.300985223757614\n",
      "Epoch number:  4 \tLoss: 2.300869127847343\n",
      "Epoch number:  6 \tLoss: 2.3007667605996884\n",
      "Epoch number:  8 \tLoss: 2.3006753266477453\n",
      "Epoch number:  10 \tLoss: 2.300592752398858\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.30229603902133\n",
      "Epoch number:  4 \tLoss: 2.3018610758307103\n",
      "Epoch number:  6 \tLoss: 2.3014595913327236\n",
      "Epoch number:  8 \tLoss: 2.301120234814288\n",
      "Epoch number:  10 \tLoss: 2.3008449710743455\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=random, optimizer=nestrov, batch_size=64, epochs=10, weight_decay=0.5, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.3023518601338364\n",
      "Epoch number:  4 \tLoss: 2.3019187077474905\n",
      "Epoch number:  6 \tLoss: 2.301518362874983\n",
      "Epoch number:  8 \tLoss: 2.3011844391655853\n",
      "Epoch number:  10 \tLoss: 2.300924166066679\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2993012296372455\n",
      "Epoch number:  4 \tLoss: 2.299307823201517\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2993010761834176\n",
      "Epoch number:  4 \tLoss: 2.2993077191404185\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2994602843783514\n",
      "Epoch number:  4 \tLoss: 2.299459948226227\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2994602543866582\n",
      "Epoch number:  4 \tLoss: 2.2994599180431776\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2998018496167223\n",
      "Epoch number:  4 \tLoss: 2.2998003845425345\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2998012679738626\n",
      "Epoch number:  4 \tLoss: 2.2997998036844383\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299297467238137\n",
      "Epoch number:  4 \tLoss: 2.299304063774712\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.29929741569402\n",
      "Epoch number:  4 \tLoss: 2.2993040370516398\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299458162635338\n",
      "Epoch number:  4 \tLoss: 2.299457826900416\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299458276390654\n",
      "Epoch number:  4 \tLoss: 2.299457940793228\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299800713138064\n",
      "Epoch number:  4 \tLoss: 2.2997992476306384\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.0005, num_neurons=128, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.299800540905489\n",
      "Epoch number:  4 \tLoss: 2.299799075763287\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.295598419901603\n",
      "Epoch number:  4 \tLoss: 2.295608640832926\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=32, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.295598465168278\n",
      "Epoch number:  4 \tLoss: 2.2956086930682913\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=3, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.2973263548272898\n",
      "Epoch number:  4 \tLoss: 2.297328182911081\n",
      "Training with params: learning_rate=0.0001, activation=sigmoid, initialization=xavier, optimizer=sgd, batch_size=16, epochs=5, weight_decay=0.5, num_neurons=64, num_hidden=5, beta=0.9\n",
      "Epoch number:  2 \tLoss: 2.297326819840811\n"
     ]
    }
   ],
   "source": [
    "def grid_search(X_train, y_train_one_hot, X_val, y_val, num_features, num_classes, learning_rates, activations, initializations, optimizers, batch_sizes, epochs_list, weight_decays, num_neurons_list, num_hidden_list, betas):\n",
    "    top_5_accuracies = [0] * 5\n",
    "    top_5_params = [None] * 5\n",
    "    \n",
    "    for (learning_rate, activation, initialization, optimizer, batch_size, epochs, weight_decay, num_neurons, num_hidden, beta) in product(learning_rates, activations, initializations, optimizers, batch_sizes, epochs_list, weight_decays, num_neurons_list, num_hidden_list, betas):\n",
    "        print(f\"Training with params: learning_rate={learning_rate}, activation={activation}, initialization={initialization}, optimizer={optimizer}, batch_size={batch_size}, epochs={epochs}, weight_decay={weight_decay}, num_neurons={num_neurons}, num_hidden={num_hidden}, beta={beta}\")\n",
    "        \n",
    "        parameters = fit(X_train, y_train_one_hot, learning_rate, activation, initialization, optimizer, batch_size, epochs, weight_decay, num_neurons, num_hidden, beta)\n",
    "        _, val_accuracy = accuracy(X_train, y_train, X_val, y_val, parameters, activation)\n",
    "        \n",
    "        for i, (acc, params) in enumerate(zip(top_5_accuracies, top_5_params)):\n",
    "            if val_accuracy > acc:\n",
    "                top_5_accuracies.insert(i, val_accuracy)\n",
    "                top_5_params.insert(i, {\n",
    "                    \"learning_rate\": learning_rate,\n",
    "                    \"activation\": activation,\n",
    "                    \"initialization\": initialization,\n",
    "                    \"optimizer\": optimizer,\n",
    "                    \"batch_size\": batch_size,\n",
    "                    \"epochs\": epochs,\n",
    "                    \"weight_decay\": weight_decay,\n",
    "                    \"num_neurons\": num_neurons,\n",
    "                    \"num_hidden\": num_hidden,\n",
    "                    \"beta\": beta\n",
    "                })\n",
    "                top_5_accuracies = top_5_accuracies[:5]\n",
    "                top_5_params = top_5_params[:5]\n",
    "                break\n",
    "    \n",
    "    return top_5_params, top_5_accuracies\n",
    "\n",
    "epochs_list = [5, 10]\n",
    "num_hidden_list = [3, 5]\n",
    "num_neurons_list = [32, 64, 128]\n",
    "weight_decays = [0, 0.0005, 0.5]\n",
    "learning_rates = [1e-3, 1e-4]\n",
    "optimizers = ['sgd', 'momentum', 'nestrov']\n",
    "batch_sizes = [16, 32, 64]\n",
    "initializations = ['random', 'xavier']\n",
    "activations = ['relu', 'sigmoid', 'tanh']\n",
    "betas = [0.9]\n",
    "\n",
    "top_5_params, top_5_accuracies = grid_search(X_train, y_train_one_hot, \n",
    "                                             X_val, y_val, \n",
    "                                             num_features, \n",
    "                                             num_classes, \n",
    "                                             learning_rates, \n",
    "                                             activations, \n",
    "                                             initializations, \n",
    "                                             optimizers, \n",
    "                                             batch_sizes, \n",
    "                                             epochs_list,\n",
    "                                             weight_decays, \n",
    "                                             num_neurons_list, \n",
    "                                             num_hidden_list, \n",
    "                                             betas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = top_5_params[0]\n",
    "class_names = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "best_parameters = fit(X_train, y_train_one_hot, **best_params)\n",
    "test_predictions = predict(X_test, best_parameters, best_params['activation'])\n",
    "train_accuracy, test_accuracy = accuracy(X_train, y_train, X_test, y_test, best_parameters, best_params['activation'])\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, test_predictions)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
